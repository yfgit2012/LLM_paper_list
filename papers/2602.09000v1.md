The document presents **iGRPO**, an enhanced version of the GRPO (Gradient-based Reinforcement Policy Optimization) method, designed to improve mathematical reasoning performance through a **two-stage self-feedback mechanism**. Below is a structured summary of the key points and findings:

---

### **Core Concepts of iGRPO**
1. **Two-Stage Process**:
   - **Stage 1**: Generate an initial solution using a base model.
   - **Stage 2**: Refine the solution iteratively using self-feedback, leveraging the output of Stage 1 to improve accuracy without additional training data.

2. **Self-Feedback Mechanism**:
   - Introduces minimal overhead in memory and computation, validated by negligible differences in peak memory usage (54.93 GB vs. 54.92 GB for GRPO) and a minor throughput reduction (0.34 vs. 0.41 samples/s).

3. **Mathematical Reasoning Focus**:
   - Tailored for tasks requiring complex logical or mathematical reasoning, such as solving multi-step problems.

---

### **Key Findings and Comparisons**
#### **1. Resource Efficiency**
- **Memory Usage**: 
  - iGRPO's peak memory is **~0.0063 GB higher** than GRPO, which is practically negligible.
  - Both methods operate within **80 GB A100 GPU limits**, ensuring compatibility with constrained budgets.
- **Throughput**: 
  - iGRPO processes **0.34 samples/s** vs. GRPO's **0.41 samples/s**, a **~17% slowdown**, but not an order-of-magnitude difference.
- **Training Time**: 
  - iGRPO requires **~13% more GPU hours** (94.1 vs. 83.3) due to sequential Stage 1 + Stage 2 decoding. However, this is offset by **substantial gains in reasoning accuracy**.

#### **2. Performance Metrics**
- **Reward Trajectory**: 
  - iGRPO maintains **higher average rewards** throughout training compared to GRPO, indicating superior policy optimization.
- **Response Length**: 
  - Both methods produce **similar output lengths**, with GRPO generating slightly longer responses on average. iGRPO's refinement occurs within a **similar token budget**, emphasizing quality over verbosity.

#### **3. Ablation Studies**
- **KL Divergence Coefficient (β)**:
  - **β = 0.0001** yields the best score (70.23%), but **β = 0** (no regularization) is sufficient for marginal gains, simplifying training.
- **Number of Completions**:
  - Increasing completions from **4 to 8** improves performance significantly, but gains beyond 8 are minimal, suggesting a balance between computational cost and accuracy.

---

### **Practical Implications**
- **Trade-off Justification**: 
  - The **13% increase in training time** for iGRPO is deemed **favorable** due to **substantial gains in reasoning accuracy** (e.g., AIME24/AIME25 performance improvements).
- **Scalability**: 
  - iGRPO is **practical for real-world deployment** as it maintains resource efficiency while enhancing model capabilities.
- **Use Cases**: 
  - Ideal for applications requiring **complex mathematical reasoning**, such as automated theorem proving, problem-solving in STEM, or logical deduction tasks.

---

### **Conclusion**
iGRPO demonstrates that **iterative refinement via self-feedback** can significantly boost mathematical reasoning performance with **minimal computational overhead**. The method's **practical efficiency** and **performance gains** make it a viable strategy for deploying advanced reasoning models in resource-constrained environments. The trade-off between increased training time and improved accuracy is justified, positioning iGRPO as a **practical and effective enhancement** over traditional GRPO.

===============

## 中文翻译

文档介绍了**iGRPO**，这是基于梯度的强化策略优化（GRPO）方法的增强版本，旨在通过**两阶段自我反馈机制**提升数学推理性能。以下是关键点和发现的结构化总结：

---

### **iGRPO 的核心概念**
1. **两阶段流程**：
   - **阶段1**：使用基础模型生成初始解。
   - **阶段2**：通过自我反馈机制迭代优化解，利用阶段1的输出提升准确性，而无需额外训练数据。

2. **自我反馈机制**：
   - 在内存和计算上引入的开销极小，通过峰值内存使用量（iGRPO 为 54.93 GB，GRPO 为 54.92 GB）和吞吐量（iGRPO 为 0.34，GRPO 为 0.41 个样本/秒）的微小差异可验证。

3. **数学推理聚焦**：
   - 针对需要复杂逻辑或数学推理的任务，如解决多步骤问题。

---

### **关键发现与对比**
#### **1. 资源效率**
- **内存使用**：
  - iGRPO 的峰值内存比 GRPO 高约 **0.0063 GB**，实际可忽略。
  - 两种方法均在 **80 GB A100 GPU** 的限制内运行，确保与受限预算的兼容性。
- **吞吐量**：
  - iGRPO 处理 **0.34 个样本/秒**，比 GRPO 的 **0.41 个样本/秒** 慢约 **17%**，但并非数量级差异。
- **训练时间**：
  - iGRPO 因需依次执行阶段1和阶段2解码，需 **约 13% 更多的 GPU 小时**（94.1 vs. 83.3）。但通过 **显著提升推理准确性** 可弥补这一差距。

#### **2. 性能指标**
- **奖励轨迹**：
  - iGRPO 在训练过程中保持 **更高的平均奖励**，表明其策略优化更优。
- **响应长度**：
  - 两种方法生成的输出长度 **相似**，GRPO 平均生成略长的响应。iGRPO 的优化在 **相近的 token 预算** 内进行，强调质量而非冗长。

#### **3. 消融实验**
- **KL 散度系数（β）**：
  - **β = 0.0001** 得到最佳分数（70.23%），但 **β = 0**（无正则化）已足以实现边际提升，简化训练。
- **生成次数**：
  - 生成次数从 **4 增加到 8** 显著提升性能，但超过 8 后收益有限，表明需在计算成本与准确性之间取得平衡。

---

### **实际应用意义**
- **权衡合理性**：
  - iGRPO 的 **13% 训练时间增加** 被认为 **有利**，因其带来 **显著的推理准确性提升**（如 AIME24/AIME25 性能改善）。
- **可扩展性**：
  - iGRPO **适用于实际部署**，因其在保持资源效率的同时增强模型能力。
- **应用场景**：
  - 适用于需要 **复杂数学推理** 的任务，如自动定理证明、STEM 领域问题解决或逻辑推导任务。

---

### **结论**
iGRPO 展示了通过**自我反馈的迭代优化**，可在**极低计算开销**下显著提升数学推理性能。该方法在**实际效率**和**性能提升**方面的优势，使其成为在资源受限环境中部署先进推理模型的可行策略。训练时间增加与准确性提升之间的权衡已被证明是合理的，使 iGRPO 成为传统 GRPO 的**实用且有效的增强方案**。

#### Reference: 

Source file: 2602.09000v1.pdf

---

**Title**: Ali Hatamizadeh** [1] **, Shrimai Prabhumoye, Igor Gitman, Ximing Lu, Seungju Han, Wei Ping,
