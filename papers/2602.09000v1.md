## Summary
- **Objective**: To analyze and compare **iGRPO** (Iterative Gradient Refinement with Self-Feedback) and **GRPO** (Gradient Refinement with Self-Feedback) for training large language models (LLMs) in mathematical reasoning tasks, focusing on efficiency, performance, and practical deployment.  
- **Methodologies**: The study employs a **two-stage process** (initial generation and iterative refinement using self-feedback) with a reward function combining **accuracy** and **format**. Key parameters include the **KL divergence coefficient (β)** and the number of completions. Experiments compare resource usage, training dynamics, and performance metrics.  
- **Results**:  
  - **Resource Efficiency**: iGRPO and GRPO show nearly identical memory usage (≈54.9 GB), with iGRPO’s overhead negligible. Throughput is slightly lower (0.34 vs. 0.41 samples/s).  
  - **Training Performance**: iGRPO achieves **13% higher GPU hours** than GRPO but significantly improves reasoning accuracy (e.g., AIME24/AIME25 scores). It outperforms GRPO in average training rewards.  
  - **Ablation Studies**: β = 0 is optimal for balancing performance and overhead; 4–8 completions yield the best trade-off between accuracy and computational cost.  
- **Key Contributions**:  
  - Introduces **iGRPO**, an iterative refinement method for enhancing mathematical reasoning accuracy.  
  - Demonstrates the effectiveness of **self-feedback mechanisms** and **two-stage refinement** with minimal resource overhead.  
  - Establishes optimal settings (β = 0, 4–8 completions) for practical deployment.  
  - Validates scalability for resource-constrained environments.  
- **Conclusions**: iGRPO achieves state-of-the-art performance in mathematical reasoning with a manageable trade-off between training time and accuracy. Its efficiency and practicality make it a viable strategy for real-world applications, leveraging self-feedback without compromising computational feasibility.  

## Title and Authors (Required)  
The paper title, main authors, and affiliations are not provided in the extracted content.

===============

## 中文翻译

## 摘要
- **目标**：分析并比较**iGRPO**（带自反馈的迭代梯度精炼）和**GRPO**（带自反馈的梯度精炼）在数学推理任务中训练大语言模型（LLMs）的效率、性能及实际部署效果。  
- **方法**：研究采用**两阶段流程**（初始生成与自反馈迭代精炼），结合**准确率**与**格式**的奖励函数。关键参数包括**KL散度系数（β）**和完成次数。实验对比资源消耗、训练动态及性能指标。  
- **结果**：  
  - **资源效率**：iGRPO与GRPO内存使用量几乎相同（≈54.9 GB），iGRPO的开销可忽略。吞吐量略低（0.34 vs. 0.41样本/秒）。  
  - **训练性能**：iGRPO的GPU小时数比GRPO高出13%，但显著提升推理准确率（如AIME24/AIME25分数）。其平均训练奖励优于GRPO。  
  - **消融实验**：β=0在性能与开销之间达到最佳平衡；4–8次完成次数在准确率与计算成本之间取得最佳权衡。  
- **关键贡献**：  
  - 引入**iGRPO**，一种提升数学推理准确率的迭代精炼方法。  
  - 验证**自反馈机制**和**两阶段精炼**在资源开销最小化下的有效性。  
  - 确定实际部署的最优设置（β=0，4–8次完成）。  
  - 验证其在资源受限环境中的可扩展性。  
- **结论**：iGRPO在数学推理任务中实现最先进的性能，且训练时间与准确率之间存在可控的权衡。其高效性与实用性使其成为现实应用的可行策略，利用自反馈机制而不影响计算可行性。  

## 标题与作者（必填）  
提取内容中未提供论文标题、主要作者及所属机构信息。

#### Reference: 

Source file: 2602.09000v1.pdf

---

**Title**: Ali Hatamizadeh** [1] **, Shrimai Prabhumoye, Igor Gitman, Ximing Lu, Seungju Han, Wei Ping,
