The supplementary material and associated algorithms, tables, and figures provide critical context for evaluating whether generative video models understand physical principles. Here's a structured breakdown of the key components and their relevance:

---

### **1. Algorithms for Video Processing**
#### **Algorithm 1: Change Video FPS with Linear Interpolation**
- **Purpose**: Adjusts the frame rate (FPS) of a video to match the requirements of different models (e.g., Physics-IQ benchmarks).
- **Key Steps**:
  - **Linear interpolation** between original frames ensures smooth temporal transitions.
  - **Optional resizing** allows adaptation to different resolutions.
  - **Temporal consistency** is maintained, which is crucial for simulating physical motion (e.g., object trajectories).
- **Relevance**: Ensures compatibility with models that require specific FPS, enabling fair comparisons across different video generation systems.

#### **Algorithm 2: Generate Binary Mask Video for Moving Objects**
- **Purpose**: Creates masks to highlight moving objects in videos, useful for analyzing motion dynamics.
- **Key Steps**:
  - **Background subtraction** with adaptive updates (using a sliding window and thresholding) isolates moving objects.
  - **Morphological operations** (opening/closing) clean up masks to remove noise.
- **Relevance**: Masks are used in Physics-IQ evaluations to segment motion, enabling analysis of how models predict physical interactions (e.g., collisions, gravity).

---

### **2. Table of Evaluated Video Models**
| Model                     | Text Condition | Multi-frame Condition | Single-frame Condition | FPS | Resolution     |
|--------------------------|----------------|------------------------|-------------------------|-----|----------------|
| **VideoPoet (i2v)**      | ✓              | ✗                     | ✓                       | 8   | 128×224        |
| **VideoPoet (multiframe)**| ✓              | ✓                     | ✗                       | 8   | 128×224        |
| **Lumiere (i2v)**        | ✓              | ✗                     | ✓                       | 16  | 128×128        |
| **Lumiere (multiframe)**| ✓              | ✓                     | ✗                       | 16  | 128×128        |
| **Stable Video Diffusion (i2v)** | ✗ | ✗ | ✓ | 8 | 1024×576 |
| **Runway Gen 3 (i2v)**   | ✓              | ✗                     | ✓                       | 24  | 1280×768      |
| **Pika 1.0 (i2v)**       | ✓              | ✗                     | ✓                       | 24  | 1280×720      |
| **Sora (i2v)**           | ✓              | ✗                     | ✓                       | 30  | 854×480       |

- **Key Insight**: Models vary in their conditioning inputs (text, multi-frame, single-frame) and output resolutions. This allows testing how different architectures handle physical reasoning tasks (e.g., predicting motion from sparse inputs).

---

### **3. Figures and Visualizations**
#### **Fig. 8: Recording Setup and Perspectives**
- **Purpose**: Illustrates the experimental setup for capturing physics-based scenarios (e.g., falling objects, collisions).
- **Relevance**: Grounds the Physics-IQ dataset in real-world physics, ensuring models are evaluated against physically plausible scenarios.

#### **Fig. 9: Switch Frames in Physics-IQ Dataset**
- **Purpose**: Shows the last conditioning frame (before the model predicts future frames) for all 66 scenarios.
- **Key Insight**: Models are tested on their ability to predict future motion based on initial conditions, which requires understanding physical laws (e.g., Newtonian mechanics).

#### **Fig. 10: MSE vs. Distortion**
- **Purpose**: Visualizes how Mean Squared Error (MSE) affects video quality, linking metric values to perceptible distortions.
- **Relevance**: MSE is used to quantify the discrepancy between generated and real videos, helping assess how well models preserve physical consistency.

---

### **4. MLLM Evaluation Prompt**
- **Task**: Discriminate between real and generated videos in a two-alternative forced-choice paradigm.
- **Key Question**: Does the model recognize physical inconsistencies (e.g., unrealistic motion, energy violations) that distinguish real-world videos from synthetic ones?
- **Implication**: If models fail to distinguish real vs. generated videos, it suggests they lack an understanding of physical principles.

---

### **5. Core Question: Do Generative Video Models Understand Physical Principles?**
- **Methodology**: The paper evaluates models using:
  1. **Physics-IQ benchmarks**: Tasks requiring prediction of future motion (e.g., object trajectories, collisions).
  2. **Binary masks**: Analyze motion segmentation and physical interactions.
  3. **MSE metrics**: Quantify discrepancies in generated vs. real videos.
- **Key Findings**: 
  - Models often fail to predict physically accurate motion (e.g., objects falling at incorrect speeds, violating conservation laws).
  - Performance varies with input conditioning (e.g., multi-frame inputs improve accuracy).
  - Text-conditioned models struggle with abstract physical reasoning (e.g., "a ball rolling down a hill" vs. realistic dynamics).

---

### **6. Implications**
- **Limitations**: Current models lack explicit understanding of physics; they rely on pattern recognition rather than causal reasoning.
- **Future Directions**: Incorporating physics priors (e.g., energy conservation, Newtonian dynamics) into training could improve performance on physics-based tasks.

This analysis highlights the gap between current generative models and human-level understanding of physics, emphasizing the need for more sophisticated architectural and training methodologies.

===============

## 中文翻译

补充材料及相关的算法、表格和图表为评估生成式视频模型是否理解物理原理提供了关键背景。以下是关键组成部分及其相关性的结构化分析：

---

### **1. 视频处理算法**
#### **算法1：使用线性插值调整视频帧率**
- **目的**：调整视频的帧率（FPS），以适配不同模型的需求（如物理智能基准测试）。
- **关键步骤**：
  - **线性插值**在原始帧之间，确保时间上的平滑过渡。
  - **可选的缩放**允许适配不同分辨率。
  - **保持时间一致性**，这对于模拟物理运动（如物体轨迹）至关重要。
- **相关性**：确保与需要特定帧率的模型兼容性，实现不同视频生成系统间的公平比较。

#### **算法2：为移动物体生成二值掩码视频**
- **目的**：创建掩码以突出视频中的移动物体，用于分析运动动态。
- **关键步骤**：
  - **背景减除**结合自适应更新（使用滑动窗口和阈值处理）隔离移动物体。
  - **形态学操作**（开运算/闭运算）清理掩码以去除噪声。
- **相关性**：掩码用于物理智能评估中对运动的分割，使模型能够分析其预测物理交互（如碰撞、重力）的能力。

---

### **2. 评估视频模型的表格**
| 模型                     | 文本条件 | 多帧条件 | 单帧条件 | FPS | 分辨率     |
|--------------------------|----------|----------|----------|-----|------------|
| **VideoPoet (i2v)**      | ✓        | ✗        | ✓        | 8   | 128×224    |
| **VideoPoet (multiframe)**| ✓        | ✓        | ✗        | 8   | 128×224    |
| **Lumiere (i2v)**        | ✓        | ✗        | ✓        | 16  | 128×128    |
| **Lumiere (multiframe)**| ✓        | ✓        | ✗        | 16  | 128×128    |
| **Stable Video Diffusion (i2v)** | ✗ | ✗ | ✓ | 8 | 1024×576 |
| **Runway Gen 3 (i2v)**   | ✓        | ✗        | ✓        | 24  | 1280×768  |
| **Pika 1.0 (i2v)**       | ✓        | ✗        | ✓        | 24  | 1280×720  |
| **Sora (i2v)**           | ✓        | ✗        | ✓        | 30  | 854×480   |

- **关键洞察**：模型在输入条件（文本、多帧、单帧）和输出分辨率上存在差异。这允许测试不同架构在物理推理任务（如从稀疏输入预测运动）中的表现。

---

### **3. 图表和可视化**
#### **图8：记录设置和视角**
- **目的**：说明捕捉基于物理场景（如物体下落、碰撞）的实验设置。
- **相关性**：将物理智能数据集扎根于现实物理，确保模型在物理上合理的场景中进行评估。

#### **图9：物理智能数据集中的切换帧**
- **目的**：展示所有66个场景中模型预测未来帧前的最后一个条件帧。
- **关键洞察**：模型基于初始条件预测未来运动的能力被测试，这需要理解物理定律（如牛顿力学）。

#### **图10：均方误差（MSE）与失真**
- **目的**：可视化均方误差（MSE）如何影响视频质量，将指标值与可感知失真联系起来。
- **相关性**：MSE用于量化生成视频与真实视频之间的差异，有助于评估模型在保持物理一致性方面的表现。

---

### **4. 多模态大语言模型（MLLM）评估提示**
- **任务**：在二选一强制选择范式中区分真实视频和生成视频。
- **关键问题**：模型是否能识别区分真实世界视频和合成视频的物理不一致性（如不现实的运动、能量违反）？
- **含义**：如果模型无法区分真实视频和生成视频，表明其缺乏对物理原理的理解。

---

### **5. 核心问题：生成式视频模型是否理解物理原理？**
- **方法论**：论文通过以下方法评估模型：
  1. **物理智能基准测试**：需要预测未来运动的任务（如物体轨迹、碰撞）。
  2. **二值掩码**：分析运动分割和物理交互。
  3. **MSE指标**：量化生成视频与真实视频之间的差异。
- **关键发现**：
  - 模型通常无法预测物理准确的运动（如物体以错误速度下落，违反守恒定律）。
  - 性能随输入条件变化（如多帧输入可提高准确性）。
  - 文本条件模型在抽象物理推理上表现不佳（如“球从山坡滚下”与真实动态的差异）。

---

### **6. 启示**
- **局限性**：当前模型缺乏对物理的显式理解，依赖模式识别而非因果推理。
- **未来方向**：在训练中引入物理先验知识（如能量守恒、牛顿动力学）可提高模型在基于物理任务中的表现。

该分析突显了当前生成模型与人类物理理解之间的差距，强调了需要更复杂架构和训练方法的重要性。

#### Reference: 

Source file: 2501.09038v3.pdf

---

**Title**: Do generative video models understand physical principles?

**Authors & Affiliations**: aINSAIT, Sofia University; work done while at Google DeepMind; bGoogle DeepMind; _†_ Joint last authors.
