The provided text contains a series of tables comparing the performance of different machine learning models and training methods, particularly focusing on **Self-Improving Pretraining**, **DPO (Direct Preference Optimization)**, and **pivot-based comparisons**. Here's a structured summary of the key findings and insights:

---

### **1. Model Performance Comparisons**
- **Self-Improving Pretraining** outperforms standard pretraining in multiple metrics:
  - **Generation Quality**: 86.3 (vs. 50.0 for Llama Base).
  - **Factuality**: 57.6 (vs. 42.3 for Llama Base).
  - **Safety**: 91.1 (vs. 76.9 for Llama Base).
  - **Coherence**: 87.9 (vs. 50.1 for Llama Base).
  - **Standard Evaluations**: 84.9 (vs. 47.6 for Llama Base).
- **Llama-3.1 8B Base** shows improved metrics compared to Llama Base (e.g., 66.1 Gen. Quality vs. 50.0), but **Self-Improving Pretraining** surpasses it in most areas.

---

### **2. Impact of Rollouts and Training Strategies**
- **Increasing Rollouts** improves performance on **factuality benchmarks**:
  - **FActScore (pairwise)**: Scores rise from 53.9 (2 rollouts) to 69.3 (16 rollouts).
  - **HaluEval (summarization)**: Scores jump from 64.2 (8 rollouts) to 87.6 (16 rollouts).
- **Pivot-Based Comparisons** (using reference texts as pivots) reduce computational overhead while maintaining performance:
  - **Coherence Eval**: 84.3 (full comparisons) vs. 72.1 (suffix as pivot).
  - **Factuality Benchmarks**: Slight trade-offs (e.g., 61.1 vs. 60.0 for Slimpajama test set), but overall performance remains strong.

---

### **3. DPO and Judge Performance**
- **Online DPO Training** using **GPT-OSS-120B** as a judge slightly outperforms a fine-tuned Llama3 judge:
  - **boolq**: 70.9 (GPT-OSS) vs. 67.5 (Llama3).
  - **piqa**: 75.6 (GPT-OSS) vs. 76.1 (Llama3).
  - **mmlu**: 28.3 (GPT-OSS) vs. 26.9 (Llama3).
- This suggests that **larger, more capable judges** may yield better results in preference-based training.

---

### **4. Dataset-Specific Results**
- **Slimpajama Test Set**:
  - **Pointwise Factuality**: 61.1 (suffix as pivot) vs. 60.0 (full comparisons).
  - **Dialogue Tasks**: 56.1 (suffix as pivot) vs. 57.2 (full comparisons).
- **TruthfulQA (MC1/MC2)**:
  - **MC1**: 25.3 (suffix as pivot) vs. 24.7 (full comparisons).
  - **MC2**: 38.9 (suffix as pivot) vs. 38.0 (full comparisons).
- **HaluEval (QA)**: 59.9 (suffix as pivot) vs. 59.0 (full comparisons).

---

### **5. Safety and Factuality Focus**
- **Safety Metrics** (e.g., TruthfulQA, HaluEval) show significant improvements with **Self-Improving Pretraining**:
  - **Safety**: 91.1 (vs. 76.9 for Llama Base).
  - **HaluEval (dialogue)**: 54.6 (8 rollouts) vs. 50.8 (suffix as pivot).
- **Factuality Benchmarks** (e.g., FActScore, Slimpajama) benefit from **increased rollouts** and **pivot-based comparisons**, indicating that these strategies help models better align with factual knowledge.

---

### **Key Takeaways**
1. **Self-Improving Pretraining** is highly effective for improving generation quality, factuality, and safety.
2. **More rollouts** and **pivot-based comparisons** enhance performance on factuality benchmarks while reducing computational costs.
3. **GPT-OSS-120B** as a judge in DPO training slightly outperforms fine-tuned Llama3 models.
4. **Safety and factuality** are critical areas where advanced training methods yield significant gains.

This analysis highlights the importance of iterative training strategies, efficient comparison methods, and robust evaluation benchmarks in developing high-performing language models.

===============

## 中文翻译

### **1. 模型性能对比**
- **自我改进预训练**在多个指标上优于标准预训练：
  - **生成质量**：86.3（与Llama Base的50.0相比）。
  - **事实性**：57.6（与Llama Base的42.3相比）。
  - **安全性**：91.1（与Llama Base的76.9相比）。
  - **连贯性**：87.9（与Llama Base的50.1相比）。
  - **标准评估**：84.9（与Llama Base的47.6相比）。
- **Llama-3.1 8B Base**相比Llama Base（如生成质量66.1 vs. 50.0）表现有所提升，但**自我改进预训练**在大多数方面仍优于它。

---

### **2. 滚动次数和训练策略的影响**
- **增加滚动次数**可提升**事实性基准**的性能：
  - **FActScore（成对）**：得分从53.9（2次滚动）提升至69.3（16次滚动）。
  - **HaluEval（摘要）**：得分从64.2（8次滚动）跃升至87.6（16次滚动）。
- **基于参考文本的比较**（使用参考文本作为基准）在保持性能的同时降低计算开销：
  - **连贯性评估**：84.3（完整比较） vs. 72.1（后缀作为基准）。
  - **事实性基准**：存在轻微权衡（如Slimpajama测试集的61.1 vs. 60.0），但整体性能仍保持强劲。

---

### **3. DPO与裁判性能**
- 使用**GPT-OSS-120B**作为裁判的**在线DPO训练**略优于微调后的Llama3裁判：
  - **boolq**：70.9（GPT-OSS） vs. 67.5（Llama3）。
  - **piqa**：75.6（GPT-OSS） vs. 76.1（Llama3）。
  - **mmlu**：28.3（GPT-OSS） vs. 26.9（Llama3）。
- 这表明**更大、更强大的裁判**可能在基于偏好的训练中获得更优结果。

---

### **4. 数据集特定结果**
- **Slimpajama测试集**：
  - **点对点事实性**：61.1（后缀作为基准） vs. 60.0（完整比较）。
  - **对话任务**：56.1（后缀作为基准） vs. 57.2（完整比较）。
- **TruthfulQA（MC1/MC2）**：
  - **MC1**：25.3（后缀作为基准） vs. 24.7（完整比较）。
  - **MC2**：38.9（后缀作为基准） vs. 38.0（完整比较）。
- **HaluEval（问答）**：59.9（后缀作为基准） vs. 59.0（完整比较）。

---

### **5. 安全性与事实性关注**
- **安全性指标**（如TruthfulQA、HaluEval）通过**自我改进预训练**显著提升：
  - **安全性**：91.1（与Llama Base的76.9相比）。
  - **HaluEval（对话）**：54.6（8次滚动） vs. 50.8（后缀作为基准）。
- **事实性基准**（如FActScore、Slimpajama）受益于**增加的滚动次数**和**基于基准的比较**，表明这些策略有助于模型更好地与事实知识对齐。

---

### **关键结论**
1. **自我改进预训练**在提升生成质量、事实性和安全性方面效果显著。
2. **更多滚动次数**和**基于基准的比较**可增强事实性基准的性能，同时降低计算成本。
3. **DPO训练中使用GPT-OSS-120B作为裁判**略优于微调后的Llama3模型。
4. **安全性和事实性**是先进训练方法可显著提升的关键领域。

该分析强调了迭代训练策略、高效比较方法和稳健评估基准在开发高性能语言模型中的重要性。

#### Reference: 

Source file: 2601.21343v2.pdf

---

**Title**: !(paper_images/2601.21343v2.pdf-0-3.png)
