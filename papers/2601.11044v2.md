## Summary  
- **Objective**: To introduce AGENCYBENCH, a comprehensive benchmark for evaluating agentic capabilities in large language models (LLMs), focusing on task execution, tool usage, and long-horizon reasoning across diverse domains.  
- **Methodologies**: The benchmark includes six core domains (Game Development, Code, Front-end, Back-end, Research, MCP) with 138 tasks. Model behavior is analyzed through tool invocation frequencies, role-based evaluation (text-based judge, vision-based judge, user simulation agent), and structured prompts to enforce objective assessment.  
- **Results**:  
  - Models exhibit distinct "cognitive styles": Navigators (e.g., GLM-4.6) prioritize exploration via `list directory`, while Executors (e.g., GPT-5.2) focus on empirical execution via `run shell command`.  
  - Tool specialization varies: GPT-5.2 uses precise `replace` edits, GLM-4.6 favors `write file` for safety, and Gemini-3-Pro leverages memory persistence via `update memory bank`.  
  - The Gomoku game development scenario highlights multi-turn task execution, emphasizing code quality, functionality, and user experience.  
- **Key Contributions**:  
  1. A structured benchmark with six domains and 32 scenarios to assess agentic capabilities.  
  2. A framework combining text, vision, and user simulation judges for rigorous evaluation.  
  3. Insights into model diversity in tool usage and trade-offs between safety, efficiency, and adaptability.  
  4. Integration of MCP tasks to align with cutting-edge agent interface standards.  
- **Conclusions**: AGENCYBENCH provides a robust framework for comparing LLMs across agentic tasks, revealing strengths and limitations in their capabilities. The benchmark underscores the importance of tool specialization, long-horizon reasoning, and alignment with real-world interaction standards.  

## Title and Authors  
**Title**: AGENCYBENCH: A Comprehensive Benchmark for Evaluating Agentic Capabilities in Large Language Models  
**Authors**: [Authors' Names]  
**Affiliations**: [Affiliations]

===============

## 中文翻译

## 摘要  
- **目标**：介绍AGENCYBENCH，一个用于评估大语言模型（LLMs）自主能力的综合性基准，重点关注任务执行、工具使用和跨多领域长周期推理能力。  
- **方法**：基准包含六个核心领域（游戏开发、代码、前端、后端、研究、MCP）共138项任务。通过工具调用频率、基于角色的评估（文本评估者、视觉评估者、用户模拟代理）以及结构化提示强制实现客观评估。  
- **结果**：  
  - 模型展现出不同的“认知风格”：导航型（如GLM-4.6）通过`list directory`优先探索，执行型（如GPT-5.2）则通过`run shell command`专注实证执行。  
  - 工具专业化存在差异：GPT-5.2使用精确的`replace`编辑，GLM-4.6因安全性倾向`write file`，Gemini-3-Pro通过`update memory bank`实现记忆持久化。  
  - 围棋游戏开发场景突显多轮任务执行，强调代码质量、功能性和用户体验。  
- **关键贡献**：  
  1. 一个包含六个领域和32个场景的结构化基准，用于评估自主能力。  
  2. 结合文本、视觉和用户模拟评估者的综合框架，实现严格评估。  
  3. 揭示模型工具使用多样性及安全性、效率与适应性之间的权衡。  
  4. 整合MCP任务以符合前沿代理接口标准。  
- **结论**：AGENCYBENCH为比较LLMs在自主任务中的能力提供了稳健框架，揭示其优势与局限。该基准强调工具专业化、长周期推理以及与现实交互标准的对齐重要性。  

## 标题与作者  
**标题**：AGENCYBENCH：用于评估大语言模型自主能力的综合性基准  
**作者**：[作者姓名]  
**单位**：[所属机构]

#### Reference: 

Source file: 2601.11044v2.pdf

---

**Title**: AGENCYBENCH: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts
