## Summary
- **Objective**: To evaluate and compare the effectiveness of SEAL (a reinforcement learning-based synthetic data generation method) against alternatives like Generative Adapter, Entigraph, and others, focusing on performance in single-passage tasks, continued pretraining (CPT), and scalability.  
- **Methodologies**: The study employs SEAL with reinforcement learning (RL) for synthetic data generation, alongside Generative Adapter, Entigraph (SCPT on SQuAD), and proxy reward mechanisms. Prompting strategies (e.g., "Implications," "Rewrite," "No-prompt") and varying dataset sizes (n=1, 200, 2067) are tested to assess performance.  
- **Results**:  
  - SEAL outperforms Generative Adapter in CPT (58.2% vs. 28.0%) and maintains robustness at scale (48.6% for n=2067).  
  - Prompt engineering significantly boosts performance: "Implications-long" (54.4%) and "Rewrite" (55.6%) outperform baseline methods, while "No-prompt" (18.9%) underperforms.  
  - Proxy rewards (45.6%) reduce training time but slightly lower performance compared to full RL (47.0%).  
  - Entigraph (pairs+triples) achieves 48.6% (n=2067), competitive with SEAL.  
- **Key Contributions**:  
  1. Introduces SEAL as a scalable RL-based approach for synthetic data generation in CPT scenarios.  
  2. Demonstrates the critical role of prompt design in enhancing model performance.  
  3. Compares proxy reward frameworks with full RL, highlighting trade-offs in efficiency vs. performance.  
  4. Establishes SEAL's flexibility in reusing synthetic data across arbitrary base models.  
- **Conclusions**: SEAL excels in large-scale CPT tasks due to its synthetic data reuse and RL adaptability. Prompt engineering and reward mechanisms significantly influence results, with proxy rewards offering practical scalability. While Generative Adapter and Entigraph are strong for smaller tasks, SEAL's scalability and flexibility position it as a leading method for resource-intensive scenarios.  

## Title and Authors (Required)  
**Title**: *SEAL: A Reinforcement Learning-Based Approach for Synthetic Data Generation in Continued Pretraining*  
**Authors**: Not specified in the provided data.  
**Affiliations**: Not specified in the provided data.

===============

## 中文翻译

## 摘要
- **目标**：评估并比较SEAL（一种基于强化学习的合成数据生成方法）与Generative Adapter、Entigraph等替代方法在单段任务、持续预训练（CPT）和可扩展性方面的有效性。  
- **方法**：研究采用SEAL结合强化学习（RL）进行合成数据生成，同时对比Generative Adapter、Entigraph（SQuAD上的SCPT）及代理奖励机制。通过测试提示策略（如“Implications”、“Rewrite”、“No-prompt”）和不同数据集规模（n=1, 200, 2067）以评估性能。  
- **结果**：  
  - SEAL在CPT任务中显著优于Generative Adapter（58.2% vs. 28.0%），并在大规模数据（n=2067）下仍保持稳健性（48.6%）。  
  - 提示工程显著提升性能：“Implications-long”（54.4%）和“Rewrite”（55.6%）优于基线方法，而“No-prompt”（18.9%）表现较差。  
  - 代理奖励（45.6%）虽减少训练时间，但性能略低于完整RL（47.0%）。  
  - Entigraph（pairs+triples）在n=2067时达到48.6%，与SEAL表现相当。  
- **关键贡献**：  
  1. 提出SEAL作为一种适用于持续预训练场景的可扩展基于强化学习的合成数据生成方法。  
  2. 证明提示设计在提升模型性能中的关键作用。  
  3. 比较代理奖励框架与完整RL，突出效率与性能之间的权衡。  
  4. 展示SEAL在任意基础模型中复用合成数据的灵活性。  
- **结论**：SEAL因合成数据复用和强化学习适应性，在大规模持续预训练任务中表现优异。提示工程和奖励机制显著影响结果，代理奖励在实际可扩展性方面具有优势。尽管Generative Adapter和Entigraph在小规模任务中表现强劲，但SEAL的可扩展性和灵活性使其成为资源密集型场景的领先方法。  

## 标题与作者（必填）  
**标题**：*SEAL：一种用于持续预训练中合成数据生成的强化学习方法*  
**作者**：提供的数据中未指定。  
**所属机构**：提供的数据中未指定。

#### Reference: 

Source file: 2506.10943v2.pdf

---

**Title**: Adam Zweiger** _[∗]_ **Jyothish Pari** _[∗]_ **[†]** **Han Guo** **Ekin Akyürek** **Yoon Kim** **Pulkit Agrawal** **[†]

**Authors & Affiliations**: {adamz, jyop, hanguo, akyurek, yoonkim, pulkitag}@mit.edu
