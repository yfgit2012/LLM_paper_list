The document presents a comprehensive analysis of challenges faced by Large Language Models (LLMs) in multi-turn conversations, with a focus on issues like **"lost in conversation"** phenomena, where models deviate from tasks due to assumptions or forgetting. Here's a structured summary of its key components:

---

### **Core Contributions & Findings**
1. **Problem: Lost in Conversation**  
   - **Example**: In Appendix J, a simulated Math task shows an LLM (Llama3.1-8B-Instruct) deviating from calculating total calories by making irrelevant assumptions (e.g., focusing on a single pastry) and forgetting the original instruction.  
   - **Cause**: Underspecified user instructions lead to unnecessary complexity and incorrect outputs.

2. **Experiments to Evaluate Performance**  
   - **Gradual Sharding (Appendix K)**:  
     - Tested how instruction granularity (number of shards) affects performance.  
     - **Findings**: Splitting instructions into smaller parts (e.g., 2–8 shards) degraded performance, suggesting LLMs struggle with multi-turn tasks due to context fragmentation.  
   - **Temperature Experiment (Appendix L)**:  
     - Evaluated how temperature settings (0.0, 0.5, 1.0) impact **aptitude** (correctness) and **reliability** (consistency) in single- and multi-turn settings.  
     - **Findings**: Lower temperatures (e.g., 0.0) improved reliability but reduced creativity, while higher temperatures increased variability.  
   - **Recap & Snowball Simulations (Appendix M)**:  
     - Compared **RECAP** (recapping past turns) and **SNOWBALL** (reconstructing history) to mitigate "lost in conversation."  
     - **Findings**: Recap improved task adherence, but snowball simulations required re-simulating entire conversations, highlighting the need for structured prompts.

3. **Deterministic Outputs & Non-Determinism (Appendix N)**  
   - Even with **temperature = 0** (greedy decoding), LLMs exhibit unreliability due to subtle non-determinism in hardware/operations.  
   - **Recommendations**: Use seeding (e.g., OpenAI's `seed` parameter) to reduce variability, though multi-turn conversations remain inherently unstable.

---

### **Methodology & Implementation**
- **Sharding Process (Appendix O.1)**:  
  - Instructions are split into shards (e.g., 6 shards for Math tasks), with prompts guiding LLMs to process each shard sequentially.  
  - Placeholders in prompts (e.g., `[[task]]`) are replaced with task-specific data.  
- **Simulated Conversations**:  
  - Used LLMs like GPT-4o and Llama3.1-8B-Instruct to simulate multi-turn interactions, varying shard counts, temperatures, and recap strategies.  
  - **Metrics**: BLEU for translation, task-specific accuracy for Code/Math, and consistency checks for reliability.

---

### **Key Takeaways**
- **Multi-Turn Challenges**: LLMs struggle with context retention and task adherence when instructions are underspecified or fragmented.  
- **Mitigation Strategies**:  
  - **Sharding**: Split instructions into manageable parts but risks performance degradation.  
  - **Recap**: Reiterating past turns improves task focus but requires additional computational overhead.  
  - **Temperature Control**: Balancing creativity and reliability via temperature settings.  
- **Determinism**: While seeding reduces variability, LLMs inherently face non-determinism in multi-turn scenarios.

---

### **Implications**
- **Practical Applications**: Developers should design tasks with clear, structured instructions and use recap mechanisms to prevent LLMs from straying off-topic.  
- **Research Directions**: Further exploration into hybrid approaches (e.g., combining sharding with recap) and hardware-level optimizations to reduce non-determinism.

This document underscores the need for careful task design and parameter tuning to harness LLMs effectively in complex, multi-turn interactions.

===============

## 中文翻译

### **核心贡献与发现**
1. **问题：对话中迷失**  
   - **示例**：附录J中的模拟数学任务显示，LLM（Llama3.1-8B-Instruct）偏离计算总卡路里的任务，因做出无关假设（如关注单一糕点）并忘记原始指令。  
   - **原因**：用户指令未明确导致不必要的复杂性和错误输出。

2. **性能评估实验**  
   - **渐进式分片（附录K）**：  
     - 测试指令粒度（分片数量）对性能的影响。  
     - **发现**：将指令拆分为更小部分（如2–8个分片）会降低性能，表明LLM在多轮任务中因上下文碎片化而难以处理。  
   - **温度实验（附录L）**：  
     - 评估温度设置（0.0、0.5、1.0）对单轮和多轮场景中**能力**（正确性）和**可靠性**（一致性）的影响。  
     - **发现**：较低温度（如0.0）提升可靠性但降低创造力，较高温度增加输出多样性。  
   - **回顾与雪球模拟（附录M）**：  
     - 比较**RECAP**（回顾过往轮次）和**SNOWBALL**（重构历史）以缓解“对话中迷失”。  
     - **发现**：回顾提升任务遵循性，但雪球模拟需重新模拟整个对话，凸显结构化提示的重要性。

3. **确定性输出与非确定性（附录N）**  
   - 即使在**温度=0**（贪心解码）下，LLM因硬件/操作中的细微非确定性仍表现出不可靠性。  
   - **建议**：使用种子（如OpenAI的`seed`参数）减少变异性，但多轮对话本质上仍不稳定。

---

### **方法论与实现**
- **分片过程（附录O.1）**：  
  - 指令被拆分为分片（如数学任务拆分为6个分片），通过提示引导LLM按顺序处理每个分片。  
  - 提示中的占位符（如`[[task]]`）会被任务特定数据替换。  
- **模拟对话**：  
  - 使用LLM（如GPT-4o和Llama3.1-8B-Instruct）模拟多轮交互，调整分片数量、温度和回顾策略。  
  - **评估指标**：翻译任务使用BLEU，代码/数学任务使用任务特定准确率，可靠性使用一致性检查。

---

### **关键结论**
- **多轮对话挑战**：LLM在指令未明确或碎片化时难以保持上下文和任务遵循。  
- **缓解策略**：  
  - **分片**：拆分指令为可管理部分但可能降低性能。  
  - **回顾**：重述过往轮次提升任务聚焦但增加计算开销。  
  - **温度控制**：通过温度设置平衡创造力与可靠性。  
- **确定性**：虽种子减少变异性，但LLM在多轮场景中仍面临非确定性。

---

### **启示**
- **实际应用**：开发者应设计清晰结构化指令并使用回顾机制防止LLM偏离主题。  
- **研究方向**：进一步探索混合方法（如结合分片与回顾）及硬件级优化以减少非确定性。

本文强调了在复杂多轮交互中谨慎设计任务和参数调优以有效利用LLM的重要性。

#### Reference: 

Source file: 2505.06120v1.pdf

---

**Title**: LLMS GET LOST IN MULTI-TURN CONVERSATION

**Authors & Affiliations**: {plaban,jenneville}@microsoft.com {hiroakihayashi,yingbo.zhou}@salesforce.com
