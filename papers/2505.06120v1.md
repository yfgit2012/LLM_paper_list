## Summary
- **Objective**: To investigate the challenges Large Language Models (LLMs) face in multi-turn conversations when instructions are fragmented into "shards" (partial steps), and to propose strategies to mitigate issues like "lost in conversation" and context drift.  
- **Methodologies**: The study employs experimental analyses, including gradual sharding experiments (testing shard granularity), temperature experiments (evaluating model reliability and creativity), and simulation strategies (recap and snowball approaches). Task-specific sharding strategies are evaluated across domains like math, translation, and code.  
- **Results**:  
  - **Sharding granularity** significantly impacts performance; more shards increase error risks (e.g., forgetting the main task).  
  - **Concat** (all shards in one turn) outperforms incremental sharding in most tasks.  
  - Lower temperatures improve reliability but do not eliminate non-determinism; higher temperatures boost diversity but risk inconsistency.  
  - **Recap** (summarizing prior inputs) and **snowball** (concatenating past turns) reduce context drift, though with computational overhead.  
- **Key Contributions**:  
  - Identification of "lost in conversation" as a critical issue caused by underspecified shards.  
  - Quantification of sharding risks and temperature trade-offs in multi-turn settings.  
  - Proposals for mitigation strategies: explicit goal statements, concat/full settings, and recap/snowball simulations.  
  - Task-specific evaluation metrics (e.g., BLEU, exact matches) for fragmented settings.  
- **Conclusions**: Sharding introduces ambiguity and context drift, risking model errors. Mitigation requires careful instruction design (explicit goals, concat settings) and model tuning (balanced temperature values). Recap and snowball strategies effectively address drift but demand computational resources. The study underscores the need for structured approaches to ensure LLM reliability in multi-turn interactions.  

## Title and Authors  
**Title**: "Lost in Conversation: Challenges and Mitigations for Sharded Multi-Turn Instructions in LLMs"  
**Authors**: [Main Authors]  
**Affiliations**: [Affiliations]

===============

## 中文翻译

## 摘要  
- **目标**：探讨大型语言模型（LLMs）在多轮对话中面对分片指令（即部分步骤）时所面临的挑战，并提出缓解“迷失对话”和上下文漂移等问题的策略。  
- **方法**：研究采用实验分析，包括渐进分片实验（测试分片粒度）、温度实验（评估模型可靠性和创造性）以及模拟策略（回顾与雪球方法）。在数学、翻译和代码等不同领域中评估任务特定的分片策略。  
- **结果**：  
  - **分片粒度**显著影响性能；分片越多，错误风险越高（例如忘记主要任务）。  
  - **拼接**（所有分片在同一轮中）在大多数任务中优于增量分片。  
  - 较低的温度值提升可靠性，但无法消除非确定性；较高的温度值增强多样性，但可能带来不一致性。  
  - **回顾**（总结先前输入）和**雪球**（拼接过往轮次）可减少上下文漂移，但伴随计算开销。  
- **关键贡献**：  
  - 识别“迷失对话”为由未明确分片引发的关键问题。  
  - 量化多轮场景中分片风险和温度权衡。  
  - 提出缓解策略：明确目标陈述、拼接/全设置、回顾/雪球模拟。  
  - 提出任务特定的评估指标（如BLEU、精确匹配）用于碎片化场景。  
- **结论**：分片引入歧义和上下文漂移，可能导致模型错误。缓解需谨慎设计指令（明确目标、拼接设置）和模型调优（平衡温度值）。回顾和雪球策略有效应对漂移，但需计算资源。研究强调需结构化方法确保LLM在多轮交互中的可靠性。  

## 标题与作者  
**标题**："迷失对话：LLM中分片多轮指令的挑战与缓解措施"  
**作者**：[主要作者]  
**所属机构**：[所属机构]

#### Reference: 

Source file: 2505.06120v1.pdf

---

**Title**: LLMS GET LOST IN MULTI-TURN CONVERSATION

**Authors & Affiliations**: {plaban,jenneville}@microsoft.com {hiroakihayashi,yingbo.zhou}@salesforce.com
