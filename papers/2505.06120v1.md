The provided text outlines a comprehensive study on the challenges LLMs face in multi-turn conversations and their implications for developers, practitioners, and the broader AI community. Here's a structured summary of the key points and implications:

---

### **Key Findings & Observations**
1. **Reliability in Multi-Turn Conversations**  
   - **SHARDED Simulations**: LLMs struggle with underspecified, multi-turn interactions, leading to **catastrophic unreliability** (e.g., 50–80% drop in unreliability when temperature is reduced). Even with temperature set to 0 (fully deterministic), reliability remains poor (~30% unreliability).  
   - **RECAP & SNOWBALL**: These methods, which repeat user input to aid LLMs, show partial success (e.g., 15–20% mitigation of SHARDED performance loss), but **RECAP is unrealistic** (intervening only on the last turn) and **SNOWBALL** still falls short of full reliability.

2. **Temperature and Determinism**  
   - Lowering the temperature (e.g., to 0) reduces randomness but **does not fully resolve reliability issues** in multi-turn settings. LLMs remain prone to cascading errors due to early turn deviations.  
   - Single-turn conversations are more predictable, but multi-turn interactions amplify instability.

3. **Granularity of Sharding**  
   - **Gradual Sharding Experiments**: Even with 2-shard instructions, models "get lost in conversation," suggesting that **any underspecification in multi-turn settings** leads to reliability degradation.  
   - **1-Shard (Full Information)** is the only effective method to ensure reliability, implying that users should avoid spreading information across turns.

---

### **Implications for Different Stakeholders**
#### **1. System & Agent Builders**  
- **Agent Frameworks**: While frameworks like Autogen or LangChain can orchestrate multi-turn interactions, they **cannot replace native multi-turn support** in LLMs.  
- **Recommendation**: Prioritize **native multi-turn capabilities** in LLMs to avoid relying on external frameworks for reliability.  
- **RECAP/SNOWBALL**: These methods are useful but limited. Developers should explore hybrid approaches to balance user experience and reliability.

#### **2. LLM Builders**  
- **Focus on Reliability**: Current efforts on **aptitude** (task-solving ability) are insufficient. Builders must prioritize **reliability** to address the randomness in text generation.  
- **Temperature Optimization**: Reducing temperature helps, but **unmodified temperature (T=1.0)** is necessary to test models under real-world variability.  
- **Design Goals**: A reliable LLM should:  
  - Maintain similar aptitude in single- and multi-turn settings.  
  - Achieve low unreliability (U < 15) in multi-turn interactions.  
  - Function effectively at standard temperature settings (T=1.0).

#### **3. NLP Practitioners**  
- **Task Design**: Tasks like programming and multi-document summarization require careful handling of underspecification. Practitioners should **avoid spreading critical information across turns** to prevent reliability issues.  
- **Model Evaluation**: Testing LLMs in multi-turn scenarios is critical to uncover hidden unreliability.  
- **User Guidance**: Users should provide **complete information upfront** (1-shard) to maximize reliability, even if it feels less natural.

---

### **Call to Action**  
The study emphasizes the need for **joint optimization of aptitude and reliability** in LLMs. Developers must:  
- **Enhance multi-turn reliability** through architectural or training improvements.  
- **Standardize evaluation protocols** that include multi-turn scenarios.  
- **Collaborate with practitioners** to design tasks that balance usability and reliability.  

By addressing these challenges, the AI community can move toward more robust, user-friendly LLM systems capable of handling real-world, complex interactions.

===============

## 中文翻译

### **关键发现与观察**
1. **多轮对话中的可靠性**  
   - **SHARDED 模拟**：LLM 在未明确指定的多轮交互中表现不佳，导致 **灾难性不可靠**（例如，温度降低时不可靠性下降 50–80%）。即使将温度设为 0（完全确定性），可靠性仍较差（约 30% 不可靠）。  
   - **RECAP 与 SNOWBALL**：这些通过重复用户输入辅助 LLM 的方法部分有效（例如，缓解 SHARDED 性能损失 15–20%），但 **RECAP 不现实**（仅在最后一轮干预），而 **SNOWBALL** 仍无法实现完全可靠性。

2. **温度与确定性**  
   - 降低温度（如设为 0）可减少随机性，但 **无法完全解决多轮场景中的可靠性问题**。LLM 仍因早期轮次偏差导致连锁错误。  
   - 单轮对话更可预测，但多轮交互会放大不稳定性。

3. **分片粒度**  
   - **渐进式分片实验**：即使使用 2 分片指令，模型也会“迷失在对话中”，表明 **任何多轮场景下的未明确指定都会导致可靠性下降**。  
   - **1 分片（完整信息）** 是唯一能确保可靠性的方法，这意味着用户应避免跨轮次分散信息。

---

### **对不同利益相关方的影响**
#### **1. 系统与代理构建者**  
- **代理框架**：尽管 Autogen 或 LangChain 等框架可协调多轮交互，但 **无法替代 LLM 原生的多轮支持**。  
- **建议**：优先提升 LLM 的 **原生多轮能力**，避免依赖外部框架确保可靠性。  
- **RECAP/SNOWBALL**：这些方法有一定用处但存在局限。开发者应探索混合方案，在用户体验与可靠性间取得平衡。

#### **2. LLM 构建者**  
- **关注可靠性**：当前对 **任务能力**（任务解决能力）的投入不足。构建者需优先提升 **可靠性** 以应对文本生成中的随机性。  
- **温度优化**：降低温度有一定帮助，但 **未修改温度（T=1.0）** 是测试模型在真实变异性下的必要条件。  
- **设计目标**：可靠 LLM 应：  
  - 单轮与多轮场景下保持相似任务能力。  
  - 多轮交互中实现低不可靠性（U < 15）。  
  - 在标准温度设置（T=1.0）下有效运行。

#### **3. NLP 实践者**  
- **任务设计**：编程、多文档摘要等任务需谨慎处理未明确指定。实践者应 **避免跨轮次分散关键信息** 以防止可靠性问题。  
- **模型评估**：在多轮场景中测试 LLM 是揭示隐藏不可靠性的关键。  
- **用户引导**：用户应 **提前提供完整信息**（1 分片）以最大化可靠性，即使这显得不够自然。

---

### **行动呼吁**  
该研究强调需 **联合优化 LLM 的任务能力与可靠性**。开发者必须：  
- **通过架构或训练改进提升多轮可靠性**。  
- **标准化包含多轮场景的评估协议**。  
- **与实践者合作设计兼顾可用性与可靠性的任务**。  

通过解决这些挑战，AI 社区可迈向更稳健、用户友好的 LLM 系统，以应对现实中的复杂交互。

#### Reference: 

Source file: 2505.06120v1.pdf