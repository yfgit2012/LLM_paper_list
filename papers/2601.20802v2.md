## Summary
- **Objective**: To compare and analyze the effectiveness of **SDPO (Self-Distilled Policy Optimization)** and **GRPO (Gradient-based Policy Optimization)** in reinforcement learning tasks, particularly for coding challenges and multiple-choice questions, by evaluating their performance under different feedback mechanisms and training strategies.  
- **Methodologies**: The study introduces two policy optimization methods:  
  - **SDPO** employs self-teaching via a teacher-student framework, using distilled knowledge from a teacher model (e.g., Qwen3-8B) and dense credit assignment across tokens via log-prob ratios.  
  - **GRPO** relies on gradient-based updates with explicit binary rewards (e.g., "Wrong Answer") and uniform negative advantages for all tokens.  
  Experiments were conducted on coding and multiple-choice tasks, using models like Qwen3-8B and Olmo3-7B-Instruct, with hyperparameter tuning and validation across datasets.  
- **Results**:  
  - SDPO outperforms GRPO in validation accuracy due to its ability to assign dense, token-specific credit for error correction, enabling precise policy adjustments.  
  - GRPO struggles with sparse feedback, leading to uniform negative advantages and less effective learning.  
  - SDPO’s self-teaching approach identifies errors (e.g., "set vs. dict" issues) without explicit solutions, improving efficiency through retrospection.  
- **Key Contributions**:  
  - Proposes **dense credit assignment** in SDPO for fine-grained error correction, contrasting with GRPO’s uniform negative advantages.  
  - Demonstrates the effectiveness of **self-teaching** and **feedback-driven learning** in reinforcement learning for complex tasks.  
  - Provides insights into hyperparameter tuning, rollout generation, and validation strategies for policy optimization.  
- **Conclusions**: SDPO is superior for tasks requiring precise feedback and error correction, such as coding and multiple-choice questions, due to its dense credit assignment and self-teaching framework. GRPO, while simpler, is less effective for sparse reward environments. The study emphasizes the importance of integrating feedback mechanisms with reinforcement learning to enhance model robustness and efficiency.  

## Title and Authors (Required)  
The paper title, main authors, and affiliations are not provided in the extracted content.

===============

## 中文翻译

## 摘要
- **目标**：通过在不同反馈机制和训练策略下评估其性能，比较和分析**SDPO（自蒸馏策略优化）**和**GRPO（基于梯度的策略优化）**在强化学习任务中的有效性，特别是在编程挑战和多项选择题等任务中的表现。  
- **方法**：研究引入了两种策略优化方法：  
  - **SDPO** 通过教师-学生框架实现自我教学，利用教师模型（如 Qwen3-8B）蒸馏的知识，并通过 log-prob 比率对 token 进行密集信用分配。  
  - **GRPO** 依赖显式的二元奖励（如“错误答案”）和统一的负优势对所有 token 进行梯度更新。  
  实验在编程和多项选择任务上进行，使用 Qwen3-8B、Olmo3-7B-Instruct 等模型，并在数据集上进行超参数调优和验证。  
- **结果**：  
  - 由于 SDPO 能够对错误进行密集的、针对单个 token 的信用分配，从而实现精准的策略调整，因此其验证准确率优于 GRPO。  
  - GRPO 在稀疏反馈环境下表现不佳，导致统一的负优势和学习效果较差。  
  - SDPO 的自我教学方法无需显式解决方案即可识别错误（如“set 与 dict 的问题”），通过回溯分析提升效率。  
- **关键贡献**：  
  - 在 SDPO 中提出**密集信用分配**，用于细粒度错误修正，与 GRPO 的统一负优势形成对比。  
  - 证明了**自我教学**和**反馈驱动学习**在复杂任务强化学习中的有效性。  
  - 提供了策略优化中超参数调优、rollout 生成和验证策略的见解。  
- **结论**：SDPO 在需要精确反馈和错误修正的任务（如编程和多项选择题）中表现更优，因其密集信用分配和自我教学框架。GRPO 虽然更简单，但在稀疏奖励环境中效果较差。研究强调了将反馈机制与强化学习结合以增强模型鲁棒性和效率的重要性。

#### Reference: 

Source file: 2601.20802v2.pdf

---

**Title**: Reinforcement Learning via Self-Distillation
