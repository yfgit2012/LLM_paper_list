The provided document outlines research and implementation details for **Self-Distilled Policy Optimization (SDPO)** and **Generalized Reinforcement Policy Optimization (GRPO)**, two reinforcement learning (RL) methods designed for training models in environments with feedback (e.g., coding tasks, multiple-choice questions). Below is a structured summary of the key components and insights:

---

### **1. Core Concepts: SDPO vs. GRPO**
- **SDPO**:
  - Uses **self-teaching** to assign **dense credit** across sequences, leveraging feedback to refine model responses iteratively.
  - Employs **KL divergence** (e.g., Reverse-KL) for distillation, with a focus on **self-teacher** feedback.
  - Visualized in **Figure 22**, where feedback is used to re-evaluate token-level log-probabilities, assigning credit to specific tokens (e.g., replacing `set` with `dict` in Python).

- **GRPO**:
  - A baseline method using **binary rewards** (e.g., "correct/incorrect") and assigns **uniform negative advantages** to all tokens in a sequence.
  - Simpler but less granular in credit assignment compared to SDPO.

---

### **2. Hyperparameter Configuration**
- **SDPO**:
  - **KL variants**: Forward KL, Jensen–Shannon divergence.
  - **Learning rates**: `1e-5` (constant) or `1e-6` (on-policy).
  - **Distillation divergence**: Reverse-KL (default).
  - **Clip advantages**: `5.0` (for gradient stability).
  - **Teacher-EMA update rate**: `0.05` (for exponential moving average).

- **GRPO**:
  - **KL coefficient (λ)**: `0.0` (no distillation).
  - **Rollout importance sampling clip**: `2`.
  - **Validation settings**: `Top-p=0.95`, `Temperature=0.6`.

- **Grid Search**:
  - For SDPO and GRPO, hyperparameters were optimized via grid search over learning rates (`1e-5`, `1e-6`) and batch sizes (`8`, `32`).

---

### **3. Experimental Setup**
- **Tasks**:
  - **Multiple-choice questions** (e.g., system/user prompts in Listings 1–2).
  - **Tool use** (e.g., code generation in Listing 3).
  - **Coding environments** (inspired by LeetCode), with feedback types like:
    - **Wrong Answer** (Listing 4).
    - **Memory Error** (Listing 5).
    - **Index Error** (Listing 6).

- **Models**:
  - **Qwen3-8B** and **Olmo3-7B-Instruct** were used for experiments.

---

### **4. Key Findings**
- **SDPO's Advantages**:
  - **Dense credit assignment**: Unlike GRPO, SDPO assigns feedback to specific tokens, enabling precise error correction (e.g., identifying `set` vs. `dict`).
  - **Sparse activation**: Focuses on critical tokens, adjusting the model's response distribution for specific errors.
  - **Retrospection**: The self-teacher identifies errors without explicit solutions, improving long-term learning.

- **GRPO Limitations**:
  - Assigns **uniform negative advantages** to all tokens, leading to less efficient learning compared to SDPO's granular feedback.

---

### **5. Visualizations and Examples**
- **Figure 21**:
  - Compares **advantage values** (positive in blue, negative in red) for SDPO and GRPO in a Chemistry task.
  - SDPO shows localized positive advantages, while GRPO has uniform negative values.

- **Figure 22**:
  - Illustrates **dense credit assignment** in SDPO:
    - Original response tokens are evaluated post-feedback.
    - Self-teacher logits (top-k) suggest alternative tokens, highlighting where mistakes occur.
    - Example: Replacing `set` with `dict` preserves order, and the model identifies an alternative solution path.

---

### **6. Practical Applications**
- **Coding Environments**:
  - Feedback mechanisms (e.g., "Wrong Answer") guide the model to refine code iteratively.
- **Tool Use**:
  - Templates ensure models follow task-specific instructions (e.g., multiple-choice formatting).

---

### **7. Challenges and Considerations**
- **Hyperparameter Sensitivity**:
  - Learning rates and batch sizes significantly impact convergence (e.g., `1e-5` vs. `1e-6`).
- **Feedback Quality**:
  - Dense credit assignment relies on accurate, granular feedback (e.g., token-level error messages).
- **Model Complexity**:
  - Larger models (e.g., Qwen3-8B) may require more careful hyperparameter tuning for stability.

---

### **Summary**
SDPO and GRPO represent different approaches to reinforcement learning with feedback:
- **SDPO** excels in **dense, token-level credit assignment**, enabling precise error correction and efficient learning.
- **GRPO** serves as a simpler baseline but lacks the granularity of SDPO.
- The document emphasizes **hyperparameter optimization**, **task-specific prompting**, and **environment feedback** as critical components for successful implementation.

===============

## 中文翻译

**Self-Distilled Policy Optimization (SDPO)** 和 **Generalized Reinforcement Policy Optimization (GRPO)** 是两种强化学习（RL）方法，用于在有反馈的环境中训练模型（例如编码任务、多项选择题）。以下是关键组件和见解的结构化总结：

---

### **1. 核心概念：SDPO 与 GRPO**
- **SDPO**：
  - 通过**自我教学**将**密集的信用分配**应用于序列，利用反馈迭代优化模型响应。
  - 采用**KL 散度**（如反向 KL）进行蒸馏，注重**自我教师反馈**。
  - 在**图 22**中可视化，反馈用于重新评估 token 级对数概率，为特定 token（如将 Python 中的 `set` 替换为 `dict`）分配信用。

- **GRPO**：
  - 基线方法，使用**二元奖励**（如“正确/错误”），并为序列中所有 token 分配**统一的负优势**。
  - 相比 SDPO，信用分配更简单但粒度更低。

---

### **2. 超参数配置**
- **SDPO**：
  - **KL 变体**：前向 KL、Jensen–Shannon 散度。
  - **学习率**：`1e-5`（固定）或 `1e-6`（on-policy）。
  - **蒸馏散度**：反向 KL（默认）。
  - **裁剪优势**：`5.0`（梯度稳定性）。
  - **教师 EMA 更新率**：`0.05`（指数移动平均）。

- **GRPO**：
  - **KL 系数（λ）**：`0.0`（无蒸馏）。
  - **Rollout 重要性采样裁剪**：`2`。
  - **验证设置**：`Top-p=0.95`，`Temperature=0.6`。

- **网格搜索**：
  - 对 SDPO 和 GRPO，通过网格搜索优化学习率（`1e-5`、`1e-6`）和批量大小（`8`、`32`）。

---

### **3. 实验设置**
- **任务**：
  - **多项选择题**（如列表 1–2 中的系统/用户提示）。
  - **工具使用**（如列表 3 中的代码生成）。
  - **编码环境**（受 LeetCode  发），反馈类型包括：
    - **错误答案**（列表 4）。
    - **内存错误**（列表 5）。
    - **索引错误**（列表 6）。

- **模型**：
  - 实验中使用了 **Qwen3-8B** 和 **Olmo3-7B-Instruct**。

---

### **4. 关键发现**
- **SDPO 的优势**：
  - **密集的信用分配**：与 GRPO 不同，SDPO 为特定 token 分配反馈，实现精确错误修正（如识别 `set` 与 `dict` 的差异）。
  - **稀疏激活**：聚焦关键 token，调整模型响应分布以修正特定错误。
  - **回溯**：自我教师无需显式解决方案即可识别错误，提升长期学习效果。

- **GRPO 的局限性**：
  - 为所有 token 分配**统一的负优势**，导致学习效率低于 SDPO 的精细反馈。

---

### **5. 可视化与示例**
- **图 21**：
  - 比较 SDPO 和 GRPO 在化学任务中的**优势值**（蓝色为正，红色为负）。
  - SDPO 显示局部正优势，而 GRPO 的优势值为统一负值。

- **图 22**：
  - 展示 SDPO 中的**密集信用分配**：
    - 原始响应 token 在反馈后重新评估。
    - 自我教师 logits（top-k）建议替代 token，突出错误发生位置。
    - 示例：将 `set` 替换为 `dict` 可保持顺序，模型识别出替代解决方案路径。

---

### **6. 实际应用**
- **编码环境**：
  - 反馈机制（如“错误答案”）引导模型迭代优化代码。
- **工具使用**：
  - 模板确保模型遵循任务特定指令（如多项选择格式）。

---

### **7. 挑战与注意事项**
- **超参数敏感性**：
  - 学习率和批量大小显著影响收敛（如 `1e-5` 与 `1e-6`）。
- **反馈质量**：
  - 密集信用分配依赖准确、精细的反馈（如 token 级错误信息）。
- **模型复杂度**：
  - 较大模型（如 Qwen3-8B）可能需要更细致的超参数调优以确保稳定性。

---

### **总结**
SDPO 和 GRPO 代表了反馈强化学习的不同方法：
- **SDPO** 在**密集的 token 级信用分配**上表现优异，实现精确错误修正和高效学习。
- **GRPO** 作为更简单的基线方法，缺乏 SDPO 的精细粒度。
- 文档强调**超参数优化**、**任务特定提示**和**环境反馈**是成功实施的关键要素。

#### Reference: 

Source file: 2601.20802v2.pdf

---

**Title**: Reinforcement Learning via Self-Distillation
