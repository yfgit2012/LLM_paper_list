The provided document outlines a detailed technical specification for a **Mixture of Experts (MoE)** model, including its architecture, training procedures, hyperparameters, and implementation considerations. Below is a structured summary of the key components and insights:

---

### **1. Core Model Components**
- **Gates (Spike Gates):**  
  - Control which experts are activated for each token.  
  - Use **temperature parameter `τz`** to soften allocation (equation 13: decays from 2.0 to 0.3 via cosine).  
  - **Gate bias initialization** is based on `τ₀ · logit(k/E)`, where `k` is the target active experts and `E` is the total experts.  
  - **Per-token centering** is applied to logits for normalization.  

- **Slab (Dirichlet Gates):**  
  - Implements a **Dirichlet distribution** to model soft expert allocation.  
  - **Prior mean mass `m`** on active experts (equation 11: `m = 0.9`).  
  - **Prior parameters `α_lo` and `α_hi`** are set via a ratio `r ≈ 39.7` (equation 12) and a floor value (0.005).  
  - **Posterior scale `λ[q]`** (equation 16) is fixed at 20.  

- **Router Output:**  
  - **Routing map threshold** is set at 0.125 for selecting active experts.  
  - **KL divergence weight `βθ`** is 1×10⁻², balancing prior and posterior distributions.  

---

### **2. Training Hyperparameters (Table 3)**
- **Optimizer:**  
  - **AdamW** with default learning rate `1×10⁻⁴`, weight decay `1×10⁻²`.  
  - **Learning rate schedule:** Warmup 1% steps → cosine decay (min lr = 0.1×base).  
  - **Gradient clipping:** Global clip at 1.0.  
  - **Precision:** `bf16` for most computations, with `fp32` for router logits and Dirichlet parameters.  

- **Batching:**  
  - **Micro-batch size:** 32.  
  - **Gradient accumulation:** 512 steps.  
  - **Seed:** 1234 for reproducibility.  

---

### **3. Implementation Details**
- **Capacity and Kernel Considerations:**  
  - **Dropless kernels** (e.g., MegaBlocks) or **high-capacity factors (CF)** are used to avoid token drops and ensure efficient parallelism.  
  - **Block-sparse GEMMs** group variable expert batches to maximize hardware utilization.  

- **Scalability:**  
  - The model scales with the number of active experts `k` (e.g., `k=1, 2, 3`).  
  - **Simpson index** and **`z`-threshold** metrics are used to evaluate expert allocation efficiency.  

---

### **4. Hyperparameters for the Router (Table 4)**
- **Experts:** `E = 8`, target active experts `k = 1`.  
- **Temperature Schedule:** `τz` decays from 2.0 to 0.3 via cosine.  
- **Prior Scale `λ[p]`:** Decays from 0.5 to 0.3 via slow exponential/cosine.  
- **Reconstruction Variance `σ²`:** 1.0 (Section 4.3).  
- **Leak Before Normalize:** `εw = 1×10⁻³`.  
- **Expected-`k` Penalty:** `λ_sparsity = 0.01` to discourage over- or under-activation.  

---

### **5. AI Usage Clarification**
- **AI Role:** Used **exclusively** for editing and syntax checks.  
- **Original Work:** All ideas, experiments, and analyses were conceived and written by the authors.  
- **Manual Review:** All AI suggestions were manually verified before implementation.  
- **No Autonomous Generation:** No text, figures, or data were generated by AI.  

---

### **6. Key Mathematical Formulations**
- **Dirichlet Prior:**  
  - `α_lo = 0.005`, `α_hi = r × α_lo ≈ 1.99` (where `r ≈ 39.7`).  
- **KL Divergence Weighting:**  
  - Balances prior and posterior distributions during training.  
- **Reconstruction Loss:**  
  - Variance `σ² = 1.0` for stability.  

---

### **7. Practical Notes**
- **Deployment:** Use **dropless kernels** or **expert-parallel engines** with sufficient capacity for large-scale training.  
- **Sparsity Control:** Maintains benefits of sparse expert allocation while stabilizing optimization.  

---

### **Summary**
This document provides a comprehensive guide to training a MoE model with **Dirichlet-based routing**, **temperature scheduling**, and **capacity-aware kernels**. The hyperparameters and architectural choices are tailored for efficiency, scalability, and stability, with clear separation of responsibilities between gates, slabs, and routers. The AI usage clarification ensures transparency in the development process.

===============

## 中文翻译

### **1. 核心模型组件**
- **门（尖峰门）：**  
  - 控制每个token激活的专家。  
  - 使用**温度参数 `τz`** 来软化分配（公式13：通过余弦函数从2.0衰减至0.3）。  
  - **门偏置初始化** 基于 `τ₀ · logit(k/E)`，其中 `k` 是目标激活专家数，`E` 是总专家数。  
  - **按token中心化** 应用于logits以进行归一化。  

- **Slab（狄利克雷门）：**  
  - 实现**狄利克雷分布**以建模软专家分配。  
  - **活跃专家的先验均质量 `m`**（公式11：`m = 0.9`）。  
  - **先验参数 `α_lo` 和 `α_hi`** 通过比值 `r ≈ 39.7`（公式12）和一个下限值（0.005）设置。  
  - **后验尺度 `λ[q]`**（公式16）固定为20。  

- **路由输出：**  
  - **路由图阈值** 设置为0.125以选择活跃专家。  
  - **KL散度权重 `βθ`** 为1×10⁻²，平衡先验和后验分布。  

---

### **2. 训练超参数（表3）**
- **优化器：**  
  - **AdamW**，默认学习率 `1×10⁻⁴`，权重衰减 `1×10⁻²`。  
  - **学习率调度：** 温升1%步骤 → 余弦衰减（最小学习率 = 0.1×基础值）。  
  - **梯度裁剪：** 全局裁剪值为1.0。  
  - **精度：** 大部分计算使用 `bf16`，路由logits和狄利克雷参数使用 `fp32`。  

- **批量处理：**  
  - **微批量大小：** 32。  
  - **梯度累积：** 512步。  
  - **种子：** 1234以确保可重复性。  

---

### **3. 实现细节**
- **容量和内核考虑：**  
  - 使用**无丢弃内核**（如MegaBlocks）或**高容量因子（CF）** 以避免token丢失并确保高效并行。  
  - **块稀疏GEMM** 将可变专家批次分组以最大化硬件利用率。  

- **可扩展性：**  
  - 模型随活跃专家数 `k`（如 `k=1, 2, 3`）扩展。  
  - 使用**辛普森指数**和**`z`-阈值**指标评估专家分配效率。  

---

### **4. 路由器超参数（表4）**
- **专家：** `E = 8`，目标活跃专家 `k = 1`。  
- **温度调度：** `τz` 通过余弦函数从2.0衰减至0.3。  
- **先验尺度 `λ[p]`：** 通过缓慢指数/余弦函数从0.5衰减至0.3。  
- **重建方差 `σ²`：** 1.0（第4.3节）。  
- **泄漏前归一化：** `εw = 1×10⁻³`。  
- **预期-`k` 惩罚：** `λ_sparsity = 0.01` 以抑制过度或不足激活。  

---

### **5. AI使用说明**
- **AI角色：** 仅用于**编辑和语法检查**。  
- **原创工作：** 所有想法、实验和分析均由作者构思和撰写。  
- **人工审核：** 所有AI建议在实施前均经过人工验证。  
- **无自主生成：** 未生成任何文本、图表或数据。  

---

### **6. 关键数学公式**
- **狄利克雷先验：**  
  - `α_lo = 0.005`，`α_hi = r × α_lo ≈ 1.99`（其中 `r ≈ 39.7`）。  
- **KL散度加权：**  
  - 在训练中平衡先验和后验分布。  
- **重建损失：**  
  - 方差 `σ² = 1.0` 以确保稳定性。  

---

### **7. 实践注意事项**
- **部署：** 使用**无丢弃内核**或**专家并行引擎**，确保大规模训练的容量。  
- **稀疏性控制：** 在保持稀疏专家分配优势的同时稳定优化。  

---

### **总结**
本文件提供了训练**基于狄利克雷路由**、**温度调度**和**容量感知内核**的MoE模型的全面指南。超参数和架构选择针对效率、可扩展性和稳定性进行了优化，明确区分了门、Slab和路由器的职责。AI使用说明确保了开发过程的透明性。

#### Reference: 

Source file: 11538_DirMoE_Dirichlet_Routed_.pdf