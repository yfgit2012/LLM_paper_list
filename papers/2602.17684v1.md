## Summary
- **Objective**: To develop CodeScaler, a novel code generation model training approach that eliminates code execution during training, reducing computational costs while maintaining high performance through execution-free reward models (RMs) and synthetic test case generation.  
- **Methodologies**:  
  - Execution-free training using human preference data (correctness, readability, efficiency) to train RMs.  
  - Reward model (RM) optimization to guide code generation without code execution.  
  - Synthetic test case generation via QueST-inspired techniques, leveraging concept graphs from seed datasets (e.g., TACO) to ensure diverse problem creation.  
- **Results**:  
  - CodeScaler outperforms existing models (e.g., SkyworkRM, AceCodeRM) across domains (LiveCodeBench, MBPP, CodeContests, etc.).  
  - Even the smallest model (CodeScaler-1.7B) surpasses larger models like AceCodeRM-32B.  
  - Scalability confirmed with consistent improvements across task difficulty levels (Hard, Normal, Easy).  
- **Key Contributions**:  
  - Introduction of execution-free RMs for cost-efficient, large-scale code generation training.  
  - Synthetic test case generation using QueST techniques to enhance problem diversity.  
  - Demonstration of RM effectiveness in achieving superior performance without code execution.  
  - Highlighting risks of incomplete synthetic test coverage (e.g., missing edge cases) and the need for robust test suites.  
- **Conclusions**: Execution-free RMs enable scalable, cost-effective code generation but require high-quality human preference data and comprehensive synthetic test coverage. The case study underscores the necessity of addressing edge cases to ensure real-world generalization. CodeScaler sets a new benchmark for efficient, practical code generation systems.  

## Title and Authors  
**Title**: CodeScaler: Execution-Free Reward Models for Scalable Code Generation  
**Authors**: [Authors' Names]  
**Affiliations**: [Affiliations of Authors]

===============

## 中文翻译

## 摘要
- **目标**：开发CodeScaler，一种新型代码生成模型训练方法，通过消除训练过程中的代码执行，降低计算成本，同时通过无执行奖励模型（RMs）和合成测试用例生成保持高性能。  
- **方法**：  
  - 利用人类偏好数据（正确性、可读性、效率）进行无执行训练以训练奖励模型。  
  - 通过奖励模型（RM）优化指导代码生成，无需执行代码。  
  - 通过受QueST启发的技术生成合成测试用例，利用种子数据集（如TACO）的概念图确保问题多样性。  
- **结果**：  
  - CodeScaler在多个领域（LiveCodeBench、MBPP、CodeContests等）的表现优于现有模型（如SkyworkRM、AceCodeRM）。  
  - 即使是最小的模型（CodeScaler-1.7B）也超越了像AceCodeRM-32B这样的大型模型。  
  - 通过任务难度级别（Hard、Normal、Easy）的持续改进验证了可扩展性。  
- **关键贡献**：  
  - 引入无执行奖励模型，实现高效、大规模代码生成训练。  
  - 通过QueST技术生成合成测试用例，增强问题多样性。  
  - 展示了奖励模型在无需代码执行即可实现卓越性能的有效性。  
  - 指出了合成测试覆盖不全（如缺失边缘情况）的风险，强调了构建强大测试套件的必要性。  
- **结论**：无执行奖励模型使代码生成具备可扩展性和成本效益，但需要高质量的人类偏好数据和全面的合成测试覆盖。案例研究强调了处理边缘情况的必要性，以确保实际应用中的泛化能力。CodeScaler为高效、实用的代码生成系统设定了新基准。  

## 标题与作者  
**标题**：CodeScaler：用于可扩展代码生成的无执行奖励模型  
**作者**：[作者姓名]  
**所属机构**：[作者所属机构]

#### Reference: 

Source file: 2602.17684v1.pdf

---

**Title**: CodeScaler: Scaling Code LLM Training and Test-Time Inference via Execution-Free Reward Models

**Authors & Affiliations**: **Github Page:** `[https://lark-ai-lab.github.io/codescaler.github.io/](https://lark-ai-lab.github.io/codescaler.github.io/)`
