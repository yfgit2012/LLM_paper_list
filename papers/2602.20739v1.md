## Summary
- **Objective**: To address interaction collapse in reinforcement learning (RL) for agentic multimodal models by stabilizing training and sustaining multi-turn interaction through novel strategies like oversampling–filtering–ranking rollouts and accumulative tool rewards. The goal is to enable scalable, open-weight multimodal agents for tasks like video reasoning.  
- **Methodologies**:  
  - **Agentic Scaffold**: Uses Python as a primitive for dynamic tooling, with on-demand context construction for video tasks to reduce visual token usage.  
  - **Accumulative Tool Reward**: Encourages sustained multi-turn tool use by accumulating rewards over interactions.  
  - **Oversampling–Filtering–Ranking Rollouts**: Enhances training stability by prioritizing informative trajectories through reward variance ranking.  
  - **Training Pipeline**: Combines supervised fine-tuning (SFT) and RL training using LLaMA-Factory, with PyVision-Image and PyVision-Video models trained on diverse multimodal tasks.  
- **Results**:  
  - **PyVision-Image** outperforms prior methods (e.g., DeepEyes-v2) by 10.2% on V*, 6.5% on HRBench-4K, and 9.6% on WeMath, achieving state-of-the-art performance in visual search and reasoning.  
  - **PyVision-Video** reduces visual token usage by ~80% compared to baselines, achieving 44.0% accuracy vs. 38.0% for Qwen2.5-VL-7B, and surpasses VITAL on VSI-Bench by +2.2%.  
  - Ablation studies confirm that standard deviation sorting and accumulative rewards improve performance, while removing standard deviation normalization enhances training stability.  
- **Key Contributions**:  
  - Introduces PyVision-RL, a framework combining dynamic tooling and RL innovations to address interaction collapse.  
  - Proposes accumulative tool rewards and oversampling–filtering–ranking rollouts to stabilize training and sustain multi-turn reasoning.  
  - Develops PyVision-Image and PyVision-Video with efficient visual processing (e.g., on-demand context construction) for scalable multimodal agents.  
  - Demonstrates the importance of RL incentives and efficient visual processing for agentic vision models.  
- **Conclusions**: Dynamic tooling and sustained interaction are critical for scalable agentic multimodal agents. PyVision-RL achieves state-of-the-art results while reducing visual token usage and improving training stability. Future work includes exploring longer-horizon reasoning and generalizing to other modalities.  

## Title and Authors (Required)  
**Title**: PyVision-RL: A Reinforcement Learning Framework for Agentic Multimodal Models  
**Authors**: Not explicitly provided in the extracted text.  
**Affiliations**: Not specified in the extracted text.

===============

## 中文翻译

## 摘要
- **目标**：通过新颖策略（如过采样–过滤–排序 rollout 和累积工具奖励）稳定训练并维持多轮交互，解决代理型多模态模型在强化学习（RL）中的交互崩溃问题。目标是使模型能够执行视频推理等任务，实现可扩展的、开放权重的多模态代理。  
- **方法**：  
  - **代理架构**：以Python作为动态工具的基础，通过按需上下文构建减少视频任务中的视觉token使用。  
  - **累积工具奖励**：通过在交互中累积奖励，鼓励持续使用工具的多轮交互。  
  - **过采样–过滤–排序 rollout**：通过奖励方差排序优先选择信息量大的轨迹，增强训练稳定性。  
  - **训练流程**：结合监督微调（SFT）和RL训练，使用LLaMA-Factory框架，PyVision-Image和PyVision-Video模型在多样化的多模态任务上进行训练。  
- **结果**：  
  - **PyVision-Image** 在V*上优于先前方法（如DeepEyes-v2）10.2%，在HRBench-4K上提升6.5%，在WeMath上提升9.6%，在视觉搜索和推理任务中达到最先进水平。  
  - **PyVision-Video** 相比基线模型减少约80%的视觉token使用，准确率达到44.0%（对比Qwen2.5-VL-7B的38.0%），在VSI-Bench上超越VITAL 2.2个百分点。  
  - 消融研究验证了标准差排序和累积奖励对性能的提升作用，而移除标准差归一化则增强了训练稳定性。  
- **关键贡献**：  
  - 引入PyVision-RL框架，结合动态工具和RL创新，解决交互崩溃问题。  
  - 提出累积工具奖励和过采样–过滤–排序 rollout 策略，以稳定训练并维持多轮推理。  
  - 开发了PyVision-Image和PyVision-Video，通过高效视觉处理（如按需上下文构建）实现可扩展的多模态代理。  
  - 证明了RL激励和高效视觉处理对代理型视觉模型的重要性。  
- **结论**：动态工具和持续交互对可扩展的代理型多模态代理至关重要。PyVision-RL在减少视觉token使用和提升训练稳定性方面达到最先进水平。未来工作包括探索更长的推理范围和扩展到其他模态。  

## 标题和作者（必填）  
**标题**：PyVision-RL：一种代理型多模态模型的强化学习框架  
**作者**：提取文本中未明确提供。  
**所属机构**：提取文本中未指定。

#### Reference: 

Source file: 2602.20739v1.pdf

---

**Title**: PyVision-RL: Forging Open Agentic Vision Models via RL
