The document presents three theorems and their proofs, focusing on the convergence properties of algorithms related to sampling from probability distributions. Here's a structured breakdown of the content:

---

### **Theorem 1: Convergence of Sampling Probability**
**Goal:** Prove that the probability of sampling a specific element $ x $ using Algorithm 2 with a batch size $ N $ converges to the target distribution $ P_T(x) = \frac{P(x)^n}{Z_T} $ as $ N \to \infty $, where $ Z_T = \sum_x P(x)^n $.

**Key Steps:**
1. **Batch Sampling:** A batch $ B = \{x_1, ..., x_N\} $ is drawn i.i.d. from the base distribution $ P(x) $.
2. **Count Variable:** For each $ x $, define $ C_x $ as the count of occurrences in $ B $.
3. **Weight Assignment:** The weight $ W_x = \frac{C_x^n}{N^n} $ is normalized to approximate $ P(x)^n $.
4. **Convergence via Weak Law of Large Numbers (WLLN):** As $ N \to \infty $, $ \frac{C_x}{N} \to P(x) $, so $ W_x \to P(x)^n $.
5. **Normalization:** The final probability $ P_{\text{alg}}(x; N) $ is derived as $ \frac{W_x}{\sum_z W_z} $, which converges to $ P_T(x) $.

**Conclusion:** The algorithm's sampling probability asymptotically matches the target distribution.

---

### **Theorem 2: Bounded Convergence for Expectations**
**Goal:** Show that if a sequence of random variables $ X_N $ converges in probability to $ X $ and is bounded, then $ \mathbb{E}[X_N] \to \mathbb{E}[X] $.

**Key Steps:**
1. **Boundedness:** Prove $ 0 \leq X_N \leq 1 $ for all $ N $.
2. **Bounded Convergence Theorem:** Apply this theorem to conclude that the expectation of $ X_N $ converges to the expectation of its limit.

**Conclusion:** Ensures that the convergence of $ X_N $ to $ P_T(x) $ implies convergence of expectations.

---

### **Theorem 3: Asymptotic Unbiasedness**
**Goal:** Prove that Algorithm 2 is asymptotically unbiased, i.e., $ \lim_{N \to \infty} P_{\text{alg}}(x; N) = P_T(x) $.

**Key Steps:**
1. **Convergence in Probability:** Use WLLN and Continuous Mapping Theorem to show $ X_N \to P_T(x) $.
2. **Boundedness:** Establish that $ X_N $ is bounded between 0 and 1.
3. **Expectation Convergence:** Apply the Bounded Convergence Theorem to confirm $ \mathbb{E}[X_N] \to P_T(x) $.

**Conclusion:** The algorithm is asymptotically unbiased, meaning it accurately approximates the target distribution as the batch size grows.

---

### **Summary of Key Concepts**
1. **Weak Law of Large Numbers (WLLN):** Ensures that sample proportions converge to true probabilities.
2. **Continuous Mapping Theorem:** Transfers convergence in probability to transformed random variables.
3. **Bounded Convergence Theorem:** Links convergence in probability to convergence of expectations for bounded variables.
4. **Target Distribution:** Defined as $ P_T(x) = \frac{P(x)^n}{Z_T} $, which arises from temperature scaling in probabilistic models (e.g., Boltzmann distributions).

These theorems collectively establish that Algorithm 2 (likely a variant of importance sampling or temperature-based sampling) becomes increasingly accurate as the batch size $ N $ increases, ensuring its asymptotic unbiasedness.

===============

## 中文翻译

### **定理1：采样概率的收敛性**
**目标：** 证明使用算法2以批量大小$ N $对特定元素$ x $进行采样时，其概率$ P_{\text{alg}}(x; N) $在$ N \to \infty $时收敛到目标分布$ P_T(x) = \frac{P(x)^n}{Z_T} $，其中$ Z_T = \sum_x P(x)^n $。

**关键步骤：**
1. **批量采样：** 从基础分布$ P(x) $中独立同分布地抽取批量$ B = \{x_1, ..., x_N\} $。
2. **计数变量：** 对每个$ x $，定义$ C_x $为$ B $中出现的次数。
3. **权重分配：** 权重$ W_x = \frac{C_x^n}{N^n} $归一化以近似$ P(x)^n $。
4. **通过弱大数定律（WLLN）实现收敛：** 当$ N \to \infty $时，$ \frac{C_x}{N} \to P(x) $，因此$ W_x \to P(x)^n $。
5. **归一化：** 最终概率$ P_{\text{alg}}(x; N) $推导为$ \frac{W_x}{\sum_z W_z} $，其收敛到$ P_T(x) $。

**结论：** 算法的采样概率渐近地匹配目标分布。

---

### **定理2：期望的有界收敛性**
**目标：** 证明若随机变量序列$ X_N $以概率收敛到$ X $且有界，则$ \mathbb{E}[X_N] \to \mathbb{E}[X] $。

**关键步骤：**
1. **有界性：** 证明对所有$ N $，$ 0 \leq X_N \leq 1 $。
2. **有界收敛定理：** 应用该定理得出$ X_N $的期望收敛到其极限的期望。

**结论：** 确保$ X_N $收敛到$ P_T(x) $意味着期望的收敛。

---

### **定理3：渐进无偏性**
**目标：** 证明算法2是渐进无偏的，即$ \lim_{N \to \infty} P_{\text{alg}}(x; N) = P_T(x) $。

**关键步骤：**
1. **概率收敛：** 使用弱大数定律和连续映射定理证明$ X_N \to P_T(x) $。
2. **有界性：** 建立$ X_N $在0和1之间有界。
3. **期望收敛：** 应用有界收敛定理确认$ \mathbb{E}[X_N] \to P_T(x) $。

**结论：** 算法是渐进无偏的，即随着批量大小$ N $增加，其准确近似目标分布。

---

### **关键概念总结**
1. **弱大数定律（WLLN）：** 确保样本比例收敛到真实概率。
2. **连续映射定理：** 将概率收敛转移到变换后的随机变量。
3. **有界收敛定理：** 将有界变量的概率收敛与期望收敛联系起来。
4. **目标分布：** 定义为$ P_T(x) = \frac{P(x)^n}{Z_T} $，来源于概率模型中的温度缩放（如玻尔兹曼分布）。

这些定理共同证明，算法2（可能是重要性采样或基于温度的采样变体）随着批量大小$ N $增加而变得越来越准确，确保其渐进无偏性。

#### Reference: 

Source file: 2510.27688v1.pdf

---

**Title**: CONTINUOUS AUTOREGRESSIVE LANGUAGE MODELS
