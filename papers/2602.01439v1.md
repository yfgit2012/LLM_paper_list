## Summary
- **Objective**: To develop a reinforcement learning framework that scales Q-function approximation using transformers while mitigating the attention collapse problem, enabling stable and efficient learning in complex environments.  
- **Methodologies**: Introduces **TQL (Transformer Q-Learning)** with two core components: (1) **Attention Entropy Target (H[̄])** to regularize attention mechanisms and prevent collapse, and (2) **QK Norm** for stabilizing query-key attention. Hyperparameter optimization, including learning rates, entropy targets, and environment-specific coefficients, is employed to enhance performance.  
- **Results**: TQL outperforms baselines like BC, IQL, Q-Transformer, and PAC in stability and performance, particularly with larger models. It achieves superior scalability across diverse environments (e.g., `cube-double-play`, `puzzle-4x4-play`) and demonstrates robustness through environment-specific hyperparameter tuning.  
- **Key Contributions**: (1) **Attention entropy regularization** to explicitly control attention diversity and prevent collapse; (2) **QK Norm** as a critical normalization technique for stabilizing transformer-based Q-learning; (3) Systematic scalability analysis and environment-specific hyperparameter optimization.  
- **Conclusions**: TQL provides a systematic solution to attention collapse in transformer-based RL, enabling scalable and stable Q-function learning. The integration of entropy control and QK Norm significantly improves performance over existing methods, highlighting the importance of attention regularization in complex reinforcement learning tasks.  

## Title and Authors (Required)  
The paper title, main authors, and affiliations are not provided in the extracted key parts.

===============

## 中文翻译

## 摘要
- **目标**：开发一种强化学习框架，利用Transformer实现Q函数近似的大规模扩展，同时缓解注意力崩溃问题，实现复杂环境中的稳定高效学习。  
- **方法**：提出**TQL（Transformer Q-Learning）**，包含两个核心组件：(1) **注意力熵目标（H[̄]）** 用于规范注意力机制并防止崩溃；(2) **QK范数** 用于稳定查询-键注意力。通过超参数优化（包括学习率、熵目标和环境特定系数）提升性能。  
- **结果**：TQL在稳定性与性能上优于基线方法（如BC、IQL、Q-Transformer和PAC），尤其在大模型中表现更优。其在多样化环境中（如`cube-double-play`、`puzzle-4x4-play`）展现出卓越的可扩展性，并通过环境特定超参数调优体现鲁棒性。  
- **关键贡献**：(1) **注意力熵正则化** 用于显式控制注意力多样性并防止崩溃；(2) **QK范数** 作为基于Transformer的Q学习的关键规范化技术；(3) 系统化的可扩展性分析与环境特定超参数优化。  
- **结论**：TQL为基于Transformer的强化学习中的注意力崩溃问题提供了系统性解决方案，实现了可扩展且稳定的Q函数学习。熵控制与QK范数的结合显著提升了现有方法的性能，突显了注意力正则化在复杂强化学习任务中的重要性。  

## 标题与作者（必填）  
提取的关键部分中未提供论文标题、主要作者及所属机构信息。

#### Reference: 

Source file: 2602.01439v1.pdf

---

**Title**: TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse
