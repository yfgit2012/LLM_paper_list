## Summary
- **Objective**: To evaluate and compare the effectiveness of different training methodologies (SFT, DFT, SDFT) in multi-task learning scenarios, with a focus on knowledge retention, task-specific performance, and generalization across domains.  
- **Methodologies**: The study compares **Supervised Fine-Tuning (SFT)**, **Distilled Fine-Tuning (DFT)**, and **Self-Distillation with Exponential Moving Average (SDFT)**. SDFT integrates **teacher EMA** (Exponential Moving Average) and **self-distillation** techniques to retain prior task knowledge while adapting to new tasks.  
- **Results**: SDFT outperforms SFT and DFT across most tasks, achieving the highest scores in **Science Q&A**, **Tool Use**, and **Medical** tasks. It excels in **new-task accuracy** and **prior-task retention**, often surpassing the base model. SDFT also matches or slightly improves upon SFT + re-invoke in reasoning tasks like Hellaswag and TruthfulQA.  
- **Key Contributions**:  
  1. Introduces **SDFT**, a novel framework combining **teacher EMA** and **self-distillation** for dynamic knowledge retention and transfer.  
  2. Demonstrates that SDFT’s hyperparameter tuning (e.g., EMA rate `α = 0.05`) optimizes performance for long-context tasks.  
  3. Highlights SDFT’s superiority in complex, knowledge-intensive domains (e.g., medical, scientific) and procedural reasoning (e.g., tool use).  
- **Conclusions**: SDFT is a robust methodology for multi-task learning, particularly in domains requiring knowledge retention and generalization. Its integration of EMA and self-distillation enables superior performance compared to traditional approaches, making it ideal for applications like scientific reasoning, medical diagnostics, and tool use.  

## Title and Authors  
**Title**: "Enhancing Multi-Task Learning with Self-Distillation and Exponential Moving Average"  
**Authors**: [Author Names]  
**Affiliations**: [Institutional Affiliations]

===============

## 中文翻译

## 摘要  
- **目标**：评估和比较不同训练方法（SFT、DFT、SDFT）在多任务学习场景中的有效性，重点关注知识保留、任务特定性能以及跨领域泛化能力。  
- **方法**：研究比较了**监督微调（SFT）**、**蒸馏微调（DFT）**和**结合指数移动平均的自蒸馏（SDFT）**。SDFT通过整合**教师指数移动平均（EMA）**和**自蒸馏**技术，在适应新任务的同时保留先前任务知识。  
- **结果**：SDFT在大多数任务中均优于SFT和DFT，在**科学问答**、**工具使用**和**医疗**任务中取得了最高得分。它在**新任务准确率**和**先前任务知识保留**方面表现突出，通常超越基础模型。SDFT在Hellaswag和TruthfulQA等需要推理的任务中也与SFT + 重新调用表现相当，甚至略有提升。  
- **关键贡献**：  
  1. 引入**SDFT**，一种结合**教师指数移动平均（EMA）**和**自蒸馏**的新颖框架，用于动态知识保留与迁移。  
  2. 证明SDFT的超参数调优（如EMA率`α = 0.05`）可优化长上下文任务的性能。  
  3. 强调SDFT在复杂、知识密集型领域（如医疗、科学）和程序性推理（如工具使用）中的优越性。  
- **结论**：SDFT是多任务学习的稳健方法，尤其适用于需要知识保留和泛化的领域。其整合EMA和自蒸馏技术使其性能优于传统方法，适用于科学推理、医疗诊断和工具使用等应用。  

## 标题与作者  
**标题**：通过自蒸馏和指数移动平均增强多任务学习  
**作者**：[作者姓名]  
**单位**：[机构隶属关系]

#### Reference: 

Source file: 2601.19897v1.pdf

---

**Title**: SELF-DISTILLATION ENABLES CONTINUAL LEARNING
