**Summary of "Self-Distillation Enables Continual Learning"**  

### **Key Contributions**  
1. **SDFT Methodology**:  
   - **Self-Distillation + On-Policy Learning**: Combines self-distillation (learning from the model's own outputs) with on-policy reinforcement learning (RL) to address catastrophic forgetting in continual learning.  
   - **Teacher-Student Framework**: Uses a trained model (teacher) to generate demonstrations, which a student model learns from during on-policy training.  

2. **Core Idea**:  
   - **On-Policy Generation**: The student model generates responses in real-time during training, using the teacher's outputs as a reference. This avoids the need for static datasets and adapts to new tasks incrementally.  
   - **Dynamic Adaptation**: Unlike offline distillation, SDFT leverages real-time feedback, enabling the model to retain past knowledge while learning new tasks.  

---

### **Experimental Findings**  
1. **Performance Gains**:  
   - **Pass@k Improvements**: SDFT consistently improves `pass@k` across a wide range of `k`, indicating better diversity and quality of high-probability generations.  
   - **Task-Specific Results**: Outperformed standard SFT (supervised fine-tuning) and offline distillation methods (e.g., KL divergence minimization) in tasks like tool use and reasoning.  

2. **Computational Costs**:  
   - **Higher Resource Use**: SDFT requires ~2.5× more FLOPs and 4× longer training time than SFT due to on-policy rollouts.  
   - **Efficiency Trade-off**: Despite higher costs, SDFT reduces total training time compared to multi-stage approaches (e.g., Re-invoke), which involve separate SFT and on-policy training phases.  

3. **Limitations**:  
   - **Dependency on ICL Capabilities**: Smaller models with weak in-context learning (ICL) abilities fail to provide meaningful teacher signals.  
   - **Learned Artifacts**: Students may inherit spurious linguistic patterns (e.g., phrases like "Based on the text...") from the teacher. Masking early tokens during training mitigates this.  

---

### **Key Takeaways**  
- **Why SDFT Works**: By integrating self-distillation with on-policy learning, SDFT enables models to retain prior knowledge while adapting to new tasks, reducing catastrophic forgetting.  
- **Practical Implications**:  
  - **Initialization for RL**: SDFT can serve as a strong initialization for subsequent RL fine-tuning.  
  - **Scalability**: Requires high-quality expert demonstrations, limiting its use with noisy or unstructured data.  
- **Future Directions**:  
  - **Noise Robustness**: Extend SDFT to handle non-expert or noisy demonstrations.  
  - **Behavioral Flexibility**: Explore modified objectives or prompting techniques to enable more aggressive behavioral changes.  

---

### **Limitations and Challenges**  
1. **Model Requirements**:  
   - Relies on a base model with strong ICL capabilities. Smaller models struggle to generate useful teacher signals.  

2. **Artifact Inheritance**:  
   - Students may adopt unintended linguistic patterns from the teacher, requiring careful loss masking during training.  

3. **Computational Overhead**:  
   - Higher resource costs compared to SFT, though offset by reduced multi-stage training in some scenarios.  

---

### **Conclusion**  
SDFT represents a significant advancement in continual learning by combining self-distillation with on-policy RL. It demonstrates superior performance in retaining knowledge and adapting to new tasks, though its effectiveness depends on high-quality demonstrations and strong ICL capabilities in the base model. Future work aims to address limitations in scalability and noise robustness.  

**Relevance**: This approach bridges gaps between supervised learning and reinforcement learning, offering a practical solution for real-world continual learning scenarios where data is dynamic and supervision is limited.

===============

## 中文翻译

**"自蒸馏实现持续学习"摘要**  

### **关键贡献**  
1. **SDFT方法**  
   - **自蒸馏 + 基于策略学习**：结合自蒸馏（从模型自身输出中学习）与基于策略的强化学习（RL），以解决持续学习中的灾难性遗忘问题。  
   - **教师-学生框架**：使用已训练的模型（教师）生成演示数据，学生模型在基于策略的训练过程中学习这些演示数据。  

2. **核心思想**  
   - **基于策略生成**：学生模型在训练过程中实时生成响应，并以教师模型的输出为参考。这避免了静态数据集的需求，并能逐步适应新任务。  
   - **动态适应**：与离线蒸馏不同，SDFT利用实时反馈，使模型在学习新任务的同时保留过往知识。  

---

### **实验发现**  
1. **性能提升**  
   - **Pass@k改进**：SDFT在广泛的`k`值范围内持续提升`pass@k`，表明高概率生成的多样性和质量更高。  
   - **任务特定结果**：在工具使用和推理等任务中，SDFT优于标准SFT（监督微调）和离线蒸馏方法（如KL散度最小化）。  

2. **计算成本**  
   - **更高资源消耗**：由于基于策略的 rollout，SDFT的 FLOPs 消耗约为 SFT 的 2.5 倍，训练时间约为 4 倍。  
   - **效率权衡**：尽管成本更高，SDFT相比多阶段方法（如 Re-invoke）减少了总训练时间，后者涉及独立的 SFT 和基于策略训练阶段。  

3. **局限性**  
   - **依赖上下文学习能力**：缺乏上下文学习（ICL）能力的小模型无法提供有意义的教师信号。  
   - **继承学习伪迹**：学生模型可能从教师模型继承虚假的语法规则（如“基于文本...”等短语）。训练过程中屏蔽早期 token 可缓解此问题。  

---

### **关键结论**  
- **为何 SDFT 有效**：通过结合自蒸馏与基于策略学习，SDFT使模型在适应新任务时保留先前知识，减少灾难性遗忘。  
- **实际意义**  
  - **强化学习初始化**：SDFT可作为后续强化学习微调的强初始化方法。  
  - **可扩展性**：需要高质量专家演示，限制其在噪声或非结构化数据中的应用。  
- **未来方向**  
  - **噪声鲁棒性**：扩展 SDFT 以处理非专家或噪声演示。  
  - **行为灵活性**：探索修改目标或提示技术，以实现更激进的行为变化。  

---

### **局限性与挑战**  
1. **模型需求**  
   - 依赖具备强上下文学习能力的基础模型。小型模型难以生成有用的教师信号。  

2. **伪迹继承**  
   - 学生模型可能无意中采纳教师模型的语法规则，需在训练过程中谨慎处理损失掩码。  

3. **计算开销**  
   - 相比 SFT，SDFT 的资源消耗更高，但在某些场景下因减少多阶段训练而有所抵消。  

---

### **结论**  
SDFT 通过结合自蒸馏与基于策略强化学习，在持续学习领域实现了显著进展。它在保留知识和适应新任务方面表现出色，但其效果依赖于高质量的演示数据和基础模型的强上下文学习能力。未来工作将致力于解决可扩展性与噪声鲁棒性的局限性。  

**相关性**：该方法弥合了监督学习与强化学习之间的差距，为数据动态且监督有限的现实持续学习场景提供了实用解决方案。

#### Reference: 

Source file: 2601.19897v1.pdf