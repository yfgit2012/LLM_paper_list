The paper "Residual Flow Steering for Dexterous Hand Manipulation" introduces a novel approach to robotic manipulation by combining flow-based reinforcement learning (RL) with residual action correction, aimed at improving sim-to-real transfer. Here's a structured summary of the key components and contributions:

---

### **Key Contributions**
1. **Residual Flow Steering (RFS):**  
   - **Hybrid Architecture:** Integrates a flow-based policy (for generating base actions) with a residual policy that adjusts these actions based on real-time observations.  
   - **Flow Matching:** Uses a flow model to generate actions, while the residual component corrects for deviations, enhancing robustness to disturbances and sim-to-real gaps.  

2. **Sim-to-Real Transfer:**  
   - **Robustness:** External disturbances and teleoperation systems are designed to test and improve the policy's resilience to real-world variability.  
   - **Data Collection:** Extensive simulation data (1,000 demonstrations per task) and real-world data (via SpaceMouse) ensure generalization to unseen objects and environments.  

3. **Reward Design:**  
   - **SAM2 Integration:** Tracks objects in image space and computes rewards based on proximity to the palm and lifting height, encouraging stable grasping.  

4. **Offline Reinforcement Learning:**  
   - **TD3+BC:** Fine-tunes the policy in the real world using behavior cloning and offline RL, leveraging human-corrected data for efficient adaptation.  

---

### **Technical Details**
- **Simulation Setup:**  
  - **IsaacLab:** Used as the primary simulator for training, with teleoperation enhancements for dexterous hand manipulation.  
  - **Observation Space:** Includes proprioception, object poses, and contact signals, ensuring rich state representation.  

- **Policy Architecture:**  
  - **MLPs:** Policy and value networks are three-layer MLPs with ReLU activations.  
  - **Latent Noise:** The flow model generates latent noise for base actions, while residual actions are computed as bounded deltas from the base policy.  

- **Hyperparameters:**  
  - **PPO:** Discount factor (γ=0.99), GAE (λ=0.95), and learning rates (policy: 3e-4, value: 1e-3).  
  - **TD3+BC:** Actor/critic learning rates (3e-4), batch size (512), and behavior cloning weight (λ_BC=0.2).  

- **Teleoperation & Distillation:**  
  - **Point Clouds:** Used as visual inputs, calibrated to the robot base frame with added noise for robustness.  
  - **Human Intervention:** SpaceMouse enables residual corrections, limiting adjustments to 1.5 cm translation and 0.05 rad finger motion.  

---

### **Baselines & Comparisons**
- **Simulation Baselines:**  
  - Competitors include DPPO, ReinFlow, FQL, AWAC, and others, all implemented in open-source frameworks.  
  - RFS is compared against these methods using the same simulator and task specifications to ensure fairness.  

---

### **Real-World Evaluation**
- **Testing Area:** Evaluated within a 30×50 cm region around the robot base, with 20 trials per known object and 10 per unknown.  
- **Performance Metrics:** Reward progression (Fig. 7) and object lifting stability demonstrate the effectiveness of the reward design.  

---

### **Conclusion**
The paper addresses the critical challenge of sim-to-real transfer by combining flow-based RL with residual correction, enabling dexterous hand manipulation in both simulated and real environments. The integration of robust data collection, offline fine-tuning, and real-time teleoperation ensures adaptability to real-world variability, making RFS a promising approach for robotic manipulation tasks.

===============

## 中文翻译

《残差流引导用于灵巧手操作》一文介绍了一种通过结合基于流的强化学习（RL）与残差动作校正的新方法，旨在提升模拟到现实的迁移效果。以下是关键组成部分和贡献的结构化摘要：

---

### **关键贡献**
1. **残差流引导（RFS）：**  
   - **混合架构：** 整合基于流的策略（用于生成基础动作）与残差策略（根据实时观测调整这些动作）。  
   - **流匹配：** 利用流模型生成动作，而残差部分校正偏差，增强对干扰和模拟到现实差距的鲁棒性。  

2. **模拟到现实迁移：**  
   - **鲁棒性：** 外部干扰和遥操作系统被设计用于测试和提升策略对现实世界变化的适应能力。  
   - **数据收集：** 通过大量模拟数据（每任务1,000次示范）和现实数据（通过SpaceMouse）确保对未知物体和环境的泛化能力。  

3. **奖励设计：**  
   - **SAM2集成：** 在图像空间中跟踪物体，并基于手部接近度和提起高度计算奖励，鼓励稳定抓取。  

4. **离线强化学习：**  
   - **TD3+BC：** 利用行为克隆和离线RL在现实世界中微调策略，借助人类修正数据实现高效适应。  

---

### **技术细节**
- **模拟设置：**  
  - **IsaacLab：** 作为训练的主模拟器，配备遥操作系统增强以实现灵巧手操作。  
  - **观测空间：** 包括本体感觉、物体姿态和接触信号，确保丰富的状态表示。  

- **策略架构：**  
  - **MLPs：** 策略和价值网络均为三层多层感知机（MLPs），使用ReLU激活函数。  
  - **潜在噪声：** 流模型生成基础动作的潜在噪声，而残差动作通过基础策略计算为有界增量。  

- **超参数：**  
  - **PPO：** 折扣因子（γ=0.99）、GAE（λ=0.95）和学习率（策略：3e-4，价值：1e-3）。  
  - **TD3+BC：** 动作/价值网络学习率（3e-4）、批量大小（512）和行为克隆权重（λ_BC=0.2）。  

- **遥操作系统与蒸馏：**  
  - **点云：** 用作视觉输入，校准至机器人基座坐标系，并添加噪声以增强鲁棒性。  
  - **人类干预：** SpaceMouse实现残差校正，限制调整为1.5厘米平移和0.05弧度手指运动。  

---

### **基线与对比**
- **模拟基线：**  
  - 竞争方法包括DPPO、ReinFlow、FQL、AWAC等，均基于开源框架实现。  
  - RFS通过相同模拟器和任务规范与这些方法进行对比，确保公平性。  

---

### **现实世界评估**
- **测试区域：** 在机器人基座周围30×50厘米区域内进行评估，每个已知物体进行20次试验，未知物体进行10次试验。  
- **性能指标：** 奖励进展（图7）和物体提起稳定性展示了奖励设计的有效性。  

---

### **结论**
该论文通过结合基于流的强化学习与残差校正，解决了模拟到现实迁移的关键挑战，使灵巧手操作在模拟和现实环境中均得以实现。通过整合鲁棒的数据收集、离线微调和实时遥操作系统，确保了对现实世界变化的适应性，使RFS成为机器人操作任务的有前景方法。

#### Reference: 

Source file: 2602.01789v3.pdf

---

**Title**: RFS: REINFORCEMENT LEARNING WITH RESIDUAL FLOW STEERING FOR DEXTEROUS MANIPULATION
