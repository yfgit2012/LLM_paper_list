## Summary
- **Objective**: To advance optimization techniques for large language models (LLMs) by integrating theoretical frameworks (e.g., mean-field analysis, random matrix theory) with practical methods (e.g., Muon optimizer, MUP, MARS), enabling efficient training, hyperparameter transfer, and scalable model architectures.  
- **Methodologies**:  
  - **Optimization Techniques**: Muon optimizer (spectral condition scaling + adaptive optimization in infinite-width limits), MUP (mean-field parameter scaling for hyperparameter transfer), and MARS (variance reduction for large models).  
  - **Theoretical Foundations**: Tensor Programs (formalizing feature learning in infinite-width/depth networks), mean-field analysis, and random matrix theory.  
  - **Practical Applications**: Norm-constrained LMOS for efficient training, benchmarking of optimizers for LLM pretraining, and tools like Gluon & Scion for bridging theory and practice.  
- **Results**:  
  - Muon optimizer and MUP enable efficient hyperparameter scaling across architectures, reducing training costs.  
  - Tensor Programs provide theoretical insights into training dynamics in infinite-width networks.  
  - Benchmarks (e.g., Semenov et al., 2025) validate the effectiveness of novel optimizers for LLM pretraining.  
  - MARS and LMOS improve training efficiency and stability for large models.  
- **Key Contributions**:  
  1. **Muon Optimizer**: Combines spectral condition scaling with adaptive optimization for LLMs.  
  2. **Tensor Programs**: Formalize feature learning in infinite-width/depth networks, enabling theoretical analysis of training dynamics.  
  3. **Zero-Shot Hyperparameter Transfer**: Simplifies hyperparameter scaling across architectures using MUP.  
  4. **Practical Tools**: Gluon & Scion framework and MARS/LMOS for efficient LLM training.  
- **Conclusions**: The integration of theoretical frameworks (e.g., mean-field analysis) with practical optimization methods (e.g., Muon, MUP) significantly enhances the efficiency and scalability of LLM training. These advancements enable zero-shot hyperparameter transfer, reduce training costs, and provide a foundation for future research in large model optimization.  

## Title and Authors (Required)  
**Title**: *Advancing Optimization and Training of Large Language Models: Theory and Practice*  
**Authors**: Yang et al., Su, Semenov et al., Pethick et al., Riabinin et al. (affiliations not specified in the extracted content).

===============

## 中文翻译

## 摘要
- **目标**：通过将理论框架（如均场分析、随机矩阵理论）与实用方法（如Muon优化器、MUP、MARS）相结合，推动大型语言模型（LLMs）的优化技术，实现高效训练、超参数迁移和可扩展的模型架构。  
- **方法论**：  
  - **优化技术**：Muon优化器（谱条件缩放 + 无限宽度极限下的自适应优化）、MUP（用于超参数迁移的均场参数缩放）、MARS（用于大型模型的方差缩减）。  
  - **理论基础**：张量程序（形式化无限宽度/深度网络中的特征学习）、均场分析和随机矩阵理论。  
  - **实际应用**：用于高效训练的范数约束LMOS、优化器在LLM预训练中的基准测试，以及Gluon & Scion等工具，用于连接理论与实践。  
- **结果**：  
  - Muon优化器和MUP实现了跨架构的高效超参数缩放，降低了训练成本。  
  - 张量程序为无限宽度网络中的训练动态提供了理论见解。  
  - 基准测试（如Semenov等人，2025）验证了新型优化器在LLM预训练中的有效性。  
  - MARS和LMOS提高了大型模型的训练效率和稳定性。  
- **关键贡献**：  
  1. **Muon优化器**：结合谱条件缩放与自适应优化，用于LLMs。  
  2. **张量程序**：形式化无限宽度/深度网络中的特征学习，使训练动态的理论分析成为可能。  
  3. **零样本超参数迁移**：通过MUP简化跨架构的超参数缩放。  
  4. **实用工具**：Gluon & Scion框架以及MARS/LMOS，用于高效LLM训练。  
- **结论**：将理论框架（如均场分析）与实用优化方法（如Muon、MUP）相结合，显著提升了LLM训练的效率和可扩展性。这些进展实现了零样本超参数迁移，降低了训练成本，并为大型模型优化的未来研究奠定了基础。  

## 标题和作者（必填）  
**标题**：*推进大型语言模型的优化与训练：理论与实践*  
**作者**：Yang等，Su，Semenov等，Pethick等，Riabinin等（提取内容中未指定所属机构）。

#### Reference: 

Source file: 2601.01306v2.pdf

---

**Title**: Towards a Principled Muon under µ P: Ensuring Spectral Conditions throughout Training
