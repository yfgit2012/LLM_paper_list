The **Muon optimizer** is closely related to **MUP (Model-Agnostic Parameter Sharing)**, but it extends MUP's principles into an optimization framework. Here's how they connect and differ:

---

### **1. MUP: The Foundation**
- **Definition**: MUP is a **hyperparameter transfer method** that assumes optimal parameters for one neural network architecture can be adapted to another via a **linear transformation**. This is rooted in the **infinite-width limit** of neural networks, where they behave like Gaussian processes, and their parameters scale linearly with width.
- **Key Idea**: By leveraging **spectral conditions** (e.g., the ratio of eigenvalues of the network's weight matrix), MUP enables **parameter sharing** across architectures, reducing the need for retraining from scratch.

---

### **2. Muon: The Optimizer Built on MUP**
- **Definition**: Muon is an **optimization algorithm** that incorporates MUP's principles. It uses **spectral condition scaling** to dynamically adjust hyperparameters (like learning rates) during training, enabling efficient training across different architectures.
- **Key Features**:
  - **Spectral Condition Scaling**: Muon scales the learning rate and other hyperparameters based on the **spectral properties** (eigenvalues) of the network's weight matrix. This ensures stability and efficiency in training.
  - **Cross-Architecture Adaptation**: By leveraging MUP's parameter sharing, Muon allows models trained on one architecture to be fine-tuned for another with minimal retraining.
  - **Higher-Order Scaling**: Some variants (e.g., "Higher-order MUP" by Su, 2025) extend MUP to handle more complex scaling laws, improving adaptability.

---

### **3. Theoretical Underpinnings**
- **Tensor Programs & Mean Field Analysis**: Both MUP and Muon are grounded in **Tensor Programs** (Yang et al., 2021–2024) and **mean field theory**, which model neural networks as Gaussian processes in the infinite-width limit. These frameworks derive the **scaling laws** for parameters and hyperparameters.
- **Random Matrix Theory**: The spectral conditions used in MUP and Muon are informed by **random matrix theory**, which describes how eigenvalues of weight matrices behave in large networks.

---

### **4. Practical Applications**
- **Pretraining Efficiency**: Muon has been shown to improve the **practical efficiency** of pretraining large models (Shah et al., 2025), leveraging MUP's cross-architecture adaptability.
- **Large-Scale Training**: Muon is particularly effective for **training large models** (e.g., LLMs) due to its ability to scale hyperparameters dynamically, reducing computational costs.

---

### **5. Relationship Summary**
| **Aspect**         | **MUP**                          | **Muon**                          |
|--------------------|----------------------------------|------------------------------------|
| **Purpose**        | Hyperparameter transfer          | Optimization algorithm             |
| **Core Idea**      | Linear parameter scaling         | Spectral condition scaling         |
| **Theoretical Base** | Tensor Programs, Mean Field     | Tensor Programs, Random Matrix Theory |
| **Application**    | Cross-architecture parameter sharing | Efficient training across architectures |

---

### **6. Key Papers**
- **MUP**: Yang et al. (2021, 2022) on tensor programs and hyperparameter transfer.
- **Muon**: Su (2025a, 2025b) on spectral condition scaling and implementation guides.
- **Theoretical Foundations**: Yang et al. (2023, 2024) on spectral conditions and infinite-depth networks.

---

### **Conclusion**
Muon is an **optimizer that builds on MUP's principles**, using **spectral condition scaling** to adapt hyperparameters dynamically. While MUP focuses on parameter sharing across architectures, Muon extends this into an optimization framework, enabling efficient and scalable training of large models. Both are underpinned by **Tensor Programs** and **mean field theory**, making them critical tools for modern large-scale machine learning.

===============

## 中文翻译

Muon 优化器与 MUP（模型无关参数共享）密切相关，但其将 MUP 的原理扩展到了优化框架中。以下是它们的联系与区别：

---

### **1. MUP：基础**
- **定义**：MUP 是一种**超参数迁移方法**，假设一个神经网络架构的最优参数可以通过**线性变换**迁移到另一个架构。这源于神经网络的**无限宽度极限**，在此极限下，神经网络表现为高斯过程，其参数随宽度线性缩放。
- **核心思想**：通过利用**谱条件**（如网络权重矩阵的特征值比），MUP 实现了架构间的**参数共享**，减少了从头训练的需要。

---

### **2. Muon：基于 MUP 的优化器**
- **定义**：Muon 是一种**优化算法**，其融合了 MUP 的原理。它通过**谱条件缩放**动态调整训练过程中的超参数（如学习率），从而在不同架构间实现高效训练。
- **关键特性**：
  - **谱条件缩放**：Muon 根据网络权重矩阵的**谱特性**（特征值）缩放学习率和其他超参数，确保训练的稳定性和效率。
  - **跨架构适应**：通过利用 MUP 的参数共享，Muon 允许在某一架构上训练的模型通过少量再训练即可适配其他架构。
  - **高阶缩放**：某些变体（如 Su, 2025 的“高阶 MUP”）进一步扩展了 MUP，以处理更复杂的缩放规律，提升适应性。

---

### **3. 理论基础**
- **张量程序与平均场分析**：MUP 和 Muon 都基于**张量程序**（Yang 等，2021–2024）和**平均场理论**，将神经网络建模为无限宽度极限下的高斯过程。这些框架推导了参数和超参数的**缩放规律**。
- **随机矩阵理论**：MUP 和 Muon 中使用的谱条件受到**随机矩阵理论**的启发，该理论描述了大型网络中权重矩阵特征值的行为。

---

### **4. 实际应用**
- **预训练效率**：Muon 被证明能提升大型模型的**实际预训练效率**（Shah 等，2025），利用 MUP 的跨架构适应性。
- **大规模训练**：由于 Muon 能动态缩放超参数，其在**大规模模型训练**（如大语言模型）中尤为有效，降低了计算成本。

---

### **5. 关系总结**
| **方面**         | **MUP**                          | **Muon**                          |
|--------------------|----------------------------------|------------------------------------|
| **目的**        | 超参数迁移          | 优化算法             |
| **核心思想**      | 线性参数缩放         | 谱条件缩放         |
| **理论基础** | 张量程序、平均场     | 张量程序、随机矩阵理论 |
| **应用**    | 跨架构参数共享 | 跨架构高效训练 |

---

### **6. 关键论文**
- **MUP**：Yang 等（2021, 2022）关于张量程序和超参数迁移。
- **Muon**：Su（2025a, 2025b）关于谱条件缩放及实现指南。
- **理论基础**：Yang 等（2023, 2024）关于谱条件和无限深度网络。

---

### **结论**
Muon 是一种**基于 MUP 原理的优化器**，利用**谱条件缩放**动态适应超参数。虽然 MUP 聚焦于架构间的参数共享，但 Muon 将其扩展为优化框架，实现了大型模型的高效可扩展训练。两者均以**张量程序**和**平均场理论**为理论基础，成为现代大规模机器学习的关键工具。

#### Reference: 

Source file: 2601.01306v2.pdf

---

**Title**: Towards a Principled Muon under µ P: Ensuring Spectral Conditions throughout Training
