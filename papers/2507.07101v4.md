## Summary： Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful
- **Objective**: To analyze machine learning training experiments, focusing on model scaling, optimization strategies, and computational efficiency, while identifying trade-offs between model size, precision, batch size, and training stability.  
- **Methodologies**: Comparative analysis of model architectures (parameters, training tokens, sequence lengths), learning rate scheduling (linear warmup + cosine decay), stochastic rounding for bfloat16 precision, computational cost evaluation on TPU v4 hardware, and experimental validation of momentum in optimization using a toy problem.  
- **Results**: Larger models (e.g., GPT-3) require more data and compute but offer better performance. Stochastic rounding enables bfloat16 to match float32 accuracy. Batch size trade-offs impact MFU (Model Flops per Second) and training stability, with momentum mitigating oscillations in large batches. Momentum is unnecessary for small batches due to reduced noise.  
- **Key Contributions**:  
  1. Quantification of model scaling trade-offs (parameters, tokens, layers).  
  2. Stochastic rounding as a critical technique for efficient bfloat16 training.  
  3. Empirical insights into batch size impacts on computational efficiency and training dynamics.  
  4. Momentum’s role in stabilizing large-batch training, with reduced necessity for small batches.  
- **Conclusions**: Model scaling improves performance but demands significant resources. Stochastic rounding enables precision-efficient training. Batch size trade-offs require balancing MFU and stability, with momentum being essential for large batches. The study provides actionable guidelines for optimizing training efficiency and model performance.  

## Title and Authors (Required)  
Title: *Not provided in the extracted content*  
Authors: *Not provided in the extracted content*  
Affiliations: *Not provided in the extracted content*

===============

## 中文翻译

## 摘要
- **目标**：分析机器学习训练实验，重点研究模型扩展、优化策略及计算效率，同时识别模型规模、精度、批量大小与训练稳定性之间的权衡。  
- **方法**：对模型架构（参数、训练令牌、序列长度）进行对比分析，采用学习率调度（线性预热 + 余弦衰减），使用随机舍入实现bfloat16精度，评估TPU v4硬件上的计算成本，并通过玩具问题验证优化中的动量作用。  
- **结果**：更大的模型（如GPT-3）需要更多数据和计算资源，但性能更优。随机舍入使bfloat16能够达到与float32相当的精度。批量大小的权衡影响MFU（模型每秒浮点运算次数）和训练稳定性，动量可缓解大批量训练中的震荡。由于小批量训练噪声较低，动量在此场景下并非必需。  
- **关键贡献**：  
  1. 量化模型扩展的权衡（参数、令牌、层数）。  
  2. 随机舍入是高效bfloat16训练的关键技术。  
  3. 实证分析批量大小对计算效率和训练动态的影响。  
  4. 动量在稳定大批量训练中的作用，且小批量训练中动量需求较低。  
- **结论**：模型扩展可提升性能，但需大量资源。随机舍入实现了精度高效的训练。批量大小的权衡需在MFU和稳定性之间平衡，动量对大批量训练至关重要。本研究为优化训练效率和模型性能提供了可操作的指导方针。  

只要正确设置 Adam 优化器，大语言模型（LLM）的训练在 batch size = 1 的情况下也能保持稳定。他们按照 token 数量来调整 Adam 的超参数，这样 batch size 为 1 也能稳定地训练 LLM。这大幅降低了内存占用和调参难度，让单张 GPU 上进行完整微调（full fine-tuning）变得更加现实。很多人觉得梯度累积（gradient accumulation）是安全的，但这篇论文认为：在 LLM 训练中，它往往会浪费训练步数和内存。   
LLM 是通过预测下一个 token 来学习的，batch size 就是每次更新使用的文本量，所以大家通常害怕极小 batch，于是用梯度累积来“伪造”更大的 batch（但这会延迟更新）。   
作者指出：不稳定现象主要来源于在不同 batch size 下没有调整 Adam 的 beta2（方差估计的遗忘率），于是他们让 Adam 的“记忆”在 token 数量上保持恒定。在 batch size 从 1 到 4096 的实验中，以及多种优化器下，小 batch 达到了相近甚至更好的 loss，而且对学习率、动量等超参数的敏感度更低（更鲁棒）。在这种设置下，连最简单的随机梯度下降（SGD，啥动量都不存的优化器）在 ~13 亿参数规模时都能有竞争力。   
他们给出的建议是：   
- 选择能让硬件保持忙碌的最小 batch size   
- 在单设备上尽量跳过梯度累积    
- 使用 Adafactor（一种内存占用很低的 Adam 变体）来实现接近 LoRA 内存开销的全参数微调    


## 标题与作者（必填）  
标题：*提取内容中未提供*  
作者：*提取内容中未提供*  
单位：*提取内容中未提供*

#### Reference: 

Source file: 2507.07101v4.pdf

---

**Title**: Martin Marek

**Authors & Affiliations**: martin.m@nyu.edu
