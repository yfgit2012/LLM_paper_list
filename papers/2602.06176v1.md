## Summary
- **Objective**: To identify and analyze critical limitations in AI models across formal reasoning, embodied reasoning, and general capabilities, emphasizing the need for improved training, interdisciplinary approaches, and robust validation to bridge the gap between AI and human-like reasoning.  
- **Methodologies**: The study employs structured analysis of failure cases in logical tasks (e.g., counting, arithmetic), code robustness tests, physics-based reasoning scenarios (1D, 2D, 3D), and perception-based tasks (e.g., visual anomaly detection). It evaluates model outputs against ground truth and human expectations.  
- **Results**: AI models exhibit systematic failures in logical reasoning (e.g., miscounting, arithmetic errors), code robustness (e.g., sensitivity to minor changes), and physical reasoning (e.g., misunderstanding Newtonian mechanics, visual-text alignment). Embodied systems also show safety risks and inability to handle ambiguity or adversarial inputs.  
- **Key Contributions**: The paper systematically categorizes AI limitations into formal, embodied, and general domains, highlighting the need for physics-informed models, adversarial robustness, and hybrid human-AI collaboration. It underscores the importance of interdisciplinary validation frameworks and enhanced training data.  
- **Conclusions**: Current AI models lack commonsense knowledge, fail in real-world safety-critical scenarios, and struggle with ambiguity. Addressing these gaps requires advanced training methodologies, domain-specific knowledge integration, and rigorous validation to ensure reliable, secure, and context-aware AI systems.  

## Title and Authors (Required)  
**Title**: "Critical Limitations in AI Models: Systematic Failures in Reasoning, Perception, and Real-World Interaction"  
**Authors**: [Name(s) not specified in the extracted text]  
**Affiliations**: [Not provided in the extracted text]

===============

## 中文翻译

## 摘要
- **目标**：识别并分析AI模型在形式推理、具身推理及通用能力方面的关键局限性，强调改进训练、跨学科方法及稳健验证的重要性，以弥合AI与类人推理之间的差距。  
- **方法**：研究采用逻辑任务（如计数、算术）中的失败案例结构化分析、代码鲁棒性测试、基于物理的推理场景（1D、2D、3D）及基于感知的任务（如视觉异常检测），并对比模型输出与真实值及人类预期。  
- **结果**：AI模型在逻辑推理（如计数错误、算术错误）、代码鲁棒性（如对微小变化的敏感性）及物理推理（如误解牛顿力学、视觉-文本对齐）方面存在系统性失败。具身系统也表现出安全风险及无法处理模糊性或对抗性输入的问题。  
- **关键贡献**：论文系统地将AI局限性归类为形式、具身及通用领域，强调需构建融入物理知识的模型、增强对抗鲁棒性及推动人机混合协作。同时突出了跨学科验证框架及增强训练数据的重要性。  
- **结论**：当前AI模型缺乏常识性知识，在现实安全关键场景中表现不佳，且难以处理模糊性。解决这些差距需采用先进训练方法、整合领域知识及严格验证，以确保AI系统具备可靠性、安全性及上下文感知能力。  

## 标题及作者（必填）  
**标题**：“AI模型的关键局限性：推理、感知与现实交互中的系统性失败”  
**作者**：[提取文本中未指定姓名]  
**所属机构**：[提取文本中未提供]

#### Reference: 

Source file: 2602.06176v1.pdf

---

**Title**: Large Language Model Reasoning Failures

**Authors & Affiliations**: **Peiyang Song** _[∗†]_ _psong@caltech.edu_ **Pengrui Han** _[∗]_ _barryhan@carleton.edu_ **Noah Goodman** _ngoodman@stanford.edu_
