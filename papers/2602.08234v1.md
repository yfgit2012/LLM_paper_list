The paper introduces **SKILLRL**, a framework for evolving agents through **recursive skill augmentation**, combining **skill distillation**, **cold-start fine-tuning (SFT)**, and **reinforcement learning (RL)**. Here's a structured breakdown of its key components and contributions:

---

### **1. Core Methodology**
- **Skill Distillation**:  
  - Extracts actionable patterns from raw trajectories (e.g., exploration, state changes, error recovery) into a **skill library**.  
  - Example skills:  
    - *Systematic Exploration*: Visit all plausible surfaces/containers once.  
    - *State-Changing Actions*: Use tools (heat/cool/clean) before placing objects.  
    - *Error Recovery*: Escape loops by switching search strategies.  

- **Cold-Start SFT**:  
  - Fine-tunes a base model using distilled skills to initialize agent behavior.  
  - Example: For **ALFWorld**, 7,500 trajectories; for **WebShop**, 2,400.  

- **RL Training**:  
  - Reinforces skill usage through reward maximization, with hyperparameters like learning rate (`1e-4`), batch size (`64`), and KL loss coefficient (`0.01`).  

---

### **2. Skill Library Structure**
- **ALFWorld Skills**:  
  - Focus on **embodied manipulation** (e.g., object acquisition, spatial reasoning).  
  - Example: *Destination First Policy* (navigate to target receptacle after picking up an object).  

- **WebShop Skills**:  
  - Prioritize **search strategies** and **constraint satisfaction** (e.g., price, variants).  
  - Example: *Scan Before You Click* (verify product details before opening links).  

- **Error Taxonomy**:  
  - Categorizes failures (e.g., redundant revisits, price shifts) and mitigation strategies (e.g., exploration maps, ad-label detection).  

---

### **3. Training Workflow**
- **Compute Resources**:  
  - 8 NVIDIA H100 GPUs; total time per experiment: ~30 hours.  

- **Phases**:  
  1. **Trajectory Collection**: 3 hours.  
  2. **Skill Distillation**: 0.5 hours.  
  3. **Cold-Start SFT**: 2 hours.  
  4. **RL Training**: 24 hours.  

---

### **4. Key Innovations**
- **Recursive Skill Augmentation**:  
  - Skills are iteratively refined during RL training, allowing agents to adapt to new tasks while retaining foundational knowledge.  

- **Error Mitigation**:  
  - Proactive strategies (e.g., *Loop Escape*, *Pre-Action Sanity Checks*) reduce failure rates in dynamic environments.  

---

### **5. Evaluation & Results**
- **ALFWorld**:  
  - Mitigates failures like redundant revisits (via exploration maps) and skipped state changes (via precondition checks).  

- **WebShop**:  
  - Addresses issues like price shifts (re-check prices after variant selection) and premature purchases (validate variants before buying).  

---

### **6. Figures & Tables**
- **Tables 5–8**: Detail distilled skills, failure cases, and mitigation strategies for both environments.  
- **Figures**: Likely illustrate the skill library structure, training pipeline, or performance comparisons.  

---

### **Summary**
SKILLRL bridges skill-based learning and reinforcement learning by:  
1. **Extracting reusable skills** from raw data.  
2. **Fine-tuning** agents with these skills for initial behavior.  
3. **Reinforcing** skill usage through RL to adapt to complex tasks.  
This approach enables agents to handle both **embodied manipulation** (ALFWorld) and **web-based shopping** (WebShop) with robust error recovery and constraint satisfaction.  

**Final Answer:**  
SKILLRL integrates skill distillation, cold-start SFT, and RL to evolve agents with a structured skill library, enabling robust task execution in dynamic environments through recursive skill augmentation and proactive error mitigation.

===============

## 中文翻译

SKILLRL 是一种通过 **递归技能增强** 进化智能体的框架，结合了 **技能蒸馏**、**冷启动微调 (SFT)** 和 **强化学习 (RL)**。以下是其关键组成部分和贡献的结构化解析：

---

### **1. 核心方法**
- **技能蒸馏**：  
  - 从原始轨迹中提取可操作模式（如探索、状态变化、错误恢复）并生成 **技能库**。  
  - 示例技能：  
    - *系统性探索*：访问所有合理表面/容器一次。  
    - *状态改变操作*：在放置物体前使用工具（加热/冷却/清洁）。  
    - *错误恢复*：通过切换搜索策略逃离循环。  

- **冷启动 SFT**：  
  - 使用蒸馏后的技能对基础模型进行微调，以初始化智能体行为。  
  - 示例：对于 **ALFWorld**，使用 7,500 条轨迹；对于 **WebShop**，使用 2,400 条轨迹。  

- **RL 训练**：  
  - 通过奖励最大化强化技能使用，超参数包括学习率 (`1e-4`)、批量大小 (`64`) 和 KL 损失系数 (`0.01`)。  

---

### **2. 技能库结构**
- **ALFWorld 技能**：  
  - 聚焦 **具身操作**（如物体获取、空间推理）。  
  - 示例：*目标优先策略*（拾取物体后导航至目标容器）。  

- **WebShop 技能**：  
  - 优先 **搜索策略** 和 **约束满足**（如价格、变体）。  
  - 示例：*点击前扫描*（打开链接前验证商品详情）。  

- **错误分类学**：  
  - 分类失败（如重复访问、价格波动）及缓解策略（如探索地图、广告标签检测）。  

---

### **3. 训练流程**
- **计算资源**：  
  - 8 块 NVIDIA H100 GPU；每轮实验总耗时约 30 小时。  

- **阶段**：  
  1. **轨迹收集**：3 小时。  
  2. **技能蒸馏**：0.5 小时。  
  3. **冷启动 SFT**：2 小时。  
  4. **RL 训练**：24 小时。  

---

### **4. 关键创新**
- **递归技能增强**：  
  - 技能在 RL 训练中迭代优化，使智能体能适应新任务并保留基础能力。  

- **错误缓解**：  
  - 主动策略（如 *循环逃离*、*预操作合理性检查*）降低动态环境中的失败率。  

---

### **5. 评估与结果**
- **ALFWorld**：  
  - 缓解重复访问（通过探索地图）和跳过状态变化（通过前提条件检查）等失败。  

- **WebShop**：  
  - 解决价格波动（变体选择后重新检查价格）和过早购买（购买前验证变体）等问题。  

---

### **6. 图表与表格**
- **表 5–8**：详细说明两个环境中的蒸馏技能、失败案例及缓解策略。  
- **图表**：可能展示技能库结构、训练流程或性能对比。  

---

### **总结**
SKILLRL 通过以下方式连接基于技能的学习与强化学习：  
1. **从原始数据中提取可复用技能**。  
2. **利用这些技能对智能体进行微调以初始化行为**。  
3. **通过 RL 强化技能使用，适应复杂任务**。  
该方法使智能体能够通过 **递归技能增强** 和 **主动错误缓解**，稳健处理 **具身操作**（ALFWorld）和 **基于网页的购物**（WebShop）。  

**最终答案**：  
SKILLRL 整合技能蒸馏、冷启动 SFT 和 RL，通过结构化的技能库进化智能体，借助递归技能增强和主动错误缓解，在动态环境中实现稳健的任务执行。

#### Reference: 

Source file: 2602.08234v1.pdf

---

**Title**: SKILLRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning
