## Summary  
- **Objective**: To develop an efficient framework (SKILLRL) that combines skill-based reinforcement learning (RL) and skill distillation to train agents in complex environments, addressing challenges like error mitigation and sample inefficiency.  
- **Methodologies**:  
  - **Skill Distillation**: Uses cold-start supervised fine-tuning (SFT) to extract generalizable skills from trajectories, categorized into exploration, state-changing, reliability, and error recovery.  
  - **RL Training**: Integrates skill retrieval via top-K selection with RL, using penalties for invalid actions and structured hyperparameters for training.  
  - **Skill Library**: Structured to iteratively improve via recursive augmentation, combining SFT and RL for enhanced performance.  
- **Results**:  
  - Reduced training time and improved sample efficiency through hybrid SFT-RL training.  
  - Mitigated common agent failures (e.g., redundant revisits, constraint oversights) using structured skill patterns.  
  - Demonstrated generalization across tasks (e.g., "Scan Before You Click" for WebShop).  
- **Key Contributions**:  
  - **Skill Library**: Repository of reusable, task-agnostic skills distilled from trajectories.  
  - **Recursive Skill Augmentation**: Iterative improvement of skills via SFT and RL.  
  - **Error Mitigation**: Structured skill patterns to address specific failure modes (e.g., exploration maps for ALFWorld, constraint assembly for WebShop).  
- **Conclusions**: SKILLRL achieves efficient, robust agent training in complex environments through modular skill learning and error-aware design, offering scalability for real-world applications requiring adaptability and explicit planning.  

## Title and Authors  
**Title**: SKILLRL: Skill-Based Reinforcement Learning with Distilled Skill Libraries for Efficient Agent Training  
**Authors**: [Authors' Names]  
**Affiliations**: [Affiliations]

===============

## 中文翻译

## 摘要  
- **目标**：开发一种高效的框架（SKILLRL），结合基于技能的强化学习（RL）和技能蒸馏技术，用于在复杂环境中训练智能体，解决误差缓解和样本效率低等挑战。  
- **方法**：  
  - **技能蒸馏**：采用冷启动监督微调（SFT）从轨迹中提取可泛化的技能，技能类别包括探索、状态转换、可靠性及错误恢复。  
  - **RL训练**：通过Top-K选择集成技能检索与RL训练，对无效动作施加惩罚，并采用结构化超参数进行训练。  
  - **技能库**：通过递归增强进行结构化迭代优化，结合SFT与RL以提升性能。  
- **结果**：  
  - 通过混合SFT-RL训练减少训练时间并提升样本效率。  
  - 通过结构化技能模式缓解常见智能体故障（如冗余重访、约束遗漏）。  
  - 在任务中展现泛化能力（如WebShop中的“点击前扫描”）。  
- **关键贡献**：  
  - **技能库**：从轨迹中蒸馏出的可复用、任务无关的技能集合。  
  - **递归技能增强**：通过SFT与RL实现技能的迭代优化。  
  - **误差缓解**：通过结构化技能模式应对特定故障模式（如ALFWorld的探索地图、WebShop的约束组装）。  
- **结论**：SKILLRL通过模块化技能学习和误差感知设计，在复杂环境中实现高效、稳健的智能体训练，具备可扩展性以适应需要适应性与显式规划的实际应用。  

## 标题与作者  
**标题**：SKILLRL：基于蒸馏技能库的技能强化学习框架用于高效智能体训练  
**作者**：[作者姓名]  
**所属机构**：[所属机构]

#### Reference: 

Source file: 2602.08234v1.pdf

---

**Title**: SKILLRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning
