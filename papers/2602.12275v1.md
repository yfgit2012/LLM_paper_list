**Summary of the Document: Experiential Learning via On-Policy Context Distillation (OPCD)**

---

### **Key Concepts and Methodology**
1. **Experiential Knowledge Extraction**:
   - **Math Problems**: Extracted from DAPO-Math17K (14K problems) using prompts that identify lines starting with `– EXPERIENCE ITEM:`.
   - **Text Games**: Extracted from environments like **Frozen Lake** (grid navigation) and **Sokoban** (spatial reasoning) via TextArena, which provides textual game state descriptions.
   - **Knowledge Accumulation**: Validation accuracy improves with accumulated experiential knowledge from diverse problems (Figure 7).

2. **Training Process**:
   - **Teacher Model**: Samples 30 validation problems, generates response traces, and extracts experiential knowledge iteratively (300 contexts total).
   - **Student Model Distillation**:
     - Trained on 50 steps using **reverse KL divergence** over top 256 vocabulary tokens.
     - Batch size: 128; learning rate: [1e-6, 5e-6].
     - **Math**: Max response length = 16384 tokens.
     - **Text Games**: 5 rounds of interaction, each with a max response length of 1024 tokens.
     - Checkpoints saved every 2 steps; best test accuracy checkpoint selected.

3. **Experiential Knowledge Distillation Settings**:
   - **Test-Time Distillation**: Randomly selects from 300 contexts without ground-truth labels.
   - **Filtered Distillation**: Scores contexts by evaluating performance on 1000 math or 128 text-game examples, selecting top-performing knowledge.

---

### **Datasets**
- **DAPO-Math17K**: 14K verifiable English math problems with numerical answers.
- **Frozen Lake**: Grid-based navigation task with 3×3 grid and 2 holes.
- **Sokoban**: Spatial reasoning puzzle with a 6×6 grid, 1 box, and removed explicit rules for exploration.
- **TextArena**: Provides textual game state descriptions for interaction.

---

### **System Prompt Distillation**
- **Medical Tasks**: Uses optimized prompts from **MetaSPO [CBH25]**, trained on **MedMCQA [PUS22]** (500 test samples). Queries require answering medical multiple-choice questions with explanations.
- **Safety Tasks**: Combines **Tweet Eval [BCCAN20]**, **Hatecheck [RVN+21]**, and **Ethos [MCKT22]** datasets. Queries assess safety compliance with justifications.
- **User Instructions**: Provided in figures (Figures 12 and 13 for MedMCQA and safety datasets).

---

### **Training Details for System Prompts**
- **Reverse KL Divergence**: Applied to top 256 tokens based on student model predictions.
- **Batch Size**: 128; learning rate sweep: [1e-6, 5e-6].
- **Response Length**: Max 512 tokens for safety tasks.
- **Evaluation**: Test accuracy averaged over three best checkpoints.

---

### **Key Findings**
- **Experiential Knowledge Accumulation**: Validation accuracy improves with diverse problem exposure (Figure 7).
- **Efficiency**: OPCD enables knowledge transfer without ground-truth labels, mimicking real-world scenarios.
- **Task-Specific Optimization**: System prompts for medical/safety tasks enhance model reliability through structured explanations.

---

### **Applications**
- **Education**: Math problem-solving via experiential learning.
- **Game AI**: Text-based game environments with adaptive reasoning.
- **Safety/Healthcare**: Robust models for compliance and explanation tasks.

This approach bridges the gap between human-like exploration and model training, enabling scalable, context-aware knowledge distillation.

===============

## 中文翻译

**文档摘要：通过On-Policy上下文蒸馏（OPCD）实现体验式学习**

---

### **关键概念与方法论**
1. **体验式知识提取**：
   - **数学问题**：从DAPO-Math17K（14K问题）中通过识别以`– EXPERIENCE ITEM:`开头的行提取。
   - **文本游戏**：通过TextArena从**Frozen Lake**（网格导航）和**Sokoban**（空间推理）等环境中提取，提供文本形式的游戏状态描述。
   - **知识积累**：验证准确率随多样化问题中积累的体验式知识提升（图7）。

2. **训练过程**：
   - **教师模型**：采样30个验证问题，生成响应轨迹，并迭代提取体验式知识（总计300个上下文）。
   - **学生模型蒸馏**：
     - 采用**反向KL散度**在前256个词汇标记上训练50步。
     - 批量大小：128；学习率：[1e-6, 5e-6]。
     - **数学**：最大响应长度=16384个标记。
     - **文本游戏**：5轮交互，每轮最大响应长度=1024个标记。
     - 每2步保存检查点，选择最佳测试准确率检查点。

3. **体验式知识蒸馏设置**：
   - **测试时蒸馏**：从300个上下文中随机选择，无需真实标签。
   - **过滤蒸馏**：通过在1000个数学或128个文本游戏示例上评估性能，选择表现最佳的知识。

---

### **数据集**
- **DAPO-Math17K**：14K可验证的英文数学问题，包含数值答案。
- **Frozen Lake**：基于网格的导航任务，包含3×3网格和2个洞。
- **Sokoban**：空间推理谜题，包含6×6网格、1个箱子，移除显式规则以促进探索。
- **TextArena**：提供文本形式的游戏状态描述以支持交互。

---

### **系统提示蒸馏**
- **医疗任务**：使用**MetaSPO [CBH25]**优化的提示，基于**MedMCQA [PUS22]**（500个测试样本）训练。查询需回答医疗多选题并附带解释。
- **安全任务**：结合**Tweet Eval [BCCAN20]**、**Hatecheck [RVN+21]**和**Ethos [MCKT22]**数据集。查询评估安全合规性并附带理由。
- **用户指令**：见图（图12和图13为MedMCQA和安全数据集）。

---

### **系统提示训练细节**
- **反向KL散度**：基于学生模型预测的前256个词汇标记应用。
- **批量大小**：128；学习率扫描：[1e-6, 5e-6]。
- **响应长度**：安全任务最大512个标记。
- **评估**：测试准确率取三个最佳检查点的平均值。

---

### **关键发现**
- **体验式知识积累**：多样化问题暴露使验证准确率提升（图7）。
- **效率**：OPCD无需真实标签即可实现知识迁移，模拟现实场景。
- **任务特定优化**：医疗/安全任务的系统提示通过结构化解释增强模型可靠性。

---

### **应用**
- **教育**：通过体验式学习解决数学问题。
- **游戏AI**：基于文本的游戏环境与自适应推理。
- **安全/医疗**：用于合规性和解释任务的鲁棒模型。

该方法弥合了类人探索与模型训练之间的差距，实现可扩展、上下文感知的知识蒸馏。

#### Reference: 

Source file: 2602.12275v1.pdf

---

**Title**: Xun Wu** **Shaohan Huang** **Furu Wei
