## Summary
- **Objective**: To enhance model performance in specific tasks (e.g., math problem-solving, text-based games, medical/safety domains) through experiential knowledge distillation and system prompt distillation, enabling learning from experience and structured prompts.  
- **Methodologies**:  
  - **Experiential Knowledge Distillation**: Uses prompt templates to extract knowledge from problem-solving traces (e.g., math problems, navigation tasks) and accumulates it via iterative training with OPCD (On-Policy Context Distillation), reverse KL divergence, and filtering techniques.  
  - **System Prompt Distillation**: Optimizes system prompts for medical/safety tasks using human-curated datasets, training student models with reverse KL divergence and evaluating performance on test splits.  
- **Results**: Validation accuracy improves with experiential knowledge accumulation (Figure 7), and system prompt distillation achieves higher test accuracy in medical/safety tasks. Knowledge extraction examples (Figures 8–9) and prompt templates (Figures 4–6) demonstrate effective knowledge transfer.  
- **Key Contributions**:  
  1. **Experiential Knowledge Distillation**: Enables unsupervised learning from interaction traces without explicit rules.  
  2. **System Prompt Optimization**: Enhances critical-domain tasks (medical/safety) with human-curated prompts.  
  3. **On-Policy Compatibility**: Integrates reinforcement learning principles for iterative knowledge accumulation and filtering.  
- **Conclusions**: The work bridges knowledge distillation and reinforcement learning, enabling models to learn from experience while adhering to structured prompts for critical tasks, with applications in education, game AI, healthcare, and safety compliance.  

## Title and Authors (Required)  
The paper title, main authors, and affiliations are not provided in the extracted key parts.

===============

## 中文翻译

## 摘要  
- **目标**：通过经验知识蒸馏和系统提示蒸馏提升模型在特定任务（如数学问题解决、基于文本的游戏、医疗/安全领域）中的性能，实现从经验中学习和利用结构化提示。  
- **方法**：  
  - **经验知识蒸馏**：利用提示模板从问题解决轨迹（如数学问题、导航任务）中提取知识，并通过基于策略的上下文蒸馏（OPCD）、反向KL散度和过滤技术进行迭代训练以积累知识。  
  - **系统提示蒸馏**：使用人工整理的数据集优化医疗/安全任务的系统提示，通过反向KL散度训练学生模型，并在测试集上评估性能。  
- **结果**：经验知识积累使验证准确率提升（图7），系统提示蒸馏在医疗/安全任务中实现了更高的测试准确率。知识提取示例（图8–9）和提示模板（图4–6）展示了有效的知识迁移。  
- **关键贡献**：  
  1. **经验知识蒸馏**：无需显式规则，从交互轨迹中实现无监督学习。  
  2. **系统提示优化**：通过人工编纂的提示增强关键领域任务（如医疗/安全）。  
  3. **基于策略的兼容性**：整合强化学习原理，实现知识的迭代积累与过滤。  
- **结论**：该工作将知识蒸馏与强化学习结合，使模型能够从经验中学习，同时通过结构化提示满足关键任务的需求，应用于教育、游戏AI、医疗和安全合规等领域。  

## 标题与作者（必填）  
提取的关键部分中未提供论文标题、主要作者及所属机构信息。

#### Reference: 

Source file: 2602.12275v1.pdf

---

**Title**: Xun Wu** **Shaohan Huang** **Furu Wei
