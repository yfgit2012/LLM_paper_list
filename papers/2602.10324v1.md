## Summary
- **Objective**: To analyze and compare three distinct reinforcement learning models (SBB programs) under different design philosophies, focusing on their mechanisms for decision-making, adaptation, and opponent modeling.  
- **Methodologies**: The paper employs a structured comparison of core components (e.g., Q-value updates, opponent modeling, state tracking) and evaluates differences in reward transformation, persistence mechanisms, switch bonuses, and exploration-exploitation strategies across the models.  
- **Results**: The models exhibit significant differences in reward processing (e.g., Prospect Theory in GPT OSS 120B), persistence mechanisms (e.g., persistence trace in GPT 5.1), and opponent modeling approaches. All share common elements like counterfactual learning and exploration-exploitation balancing via scaling factors.  
- **Key Contributions**:  
  1. Introduces "You" with alternation and fictitious play for dynamic strategies.  
  2. Proposes GPT 5.1 with persistence trace and switch bonuses modulated by prediction error.  
  3. Integrates Prospect Theory in GPT OSS 120B for gain-loss asymmetry and reward-sensitive switching.  
  4. Highlights trade-offs between adaptability, exploration, and opponent modeling in reinforcement learning.  
- **Conclusions**: The models demonstrate diverse design philosophies for reinforcement learning, with trade-offs in adaptability, exploration, and strategic depth. GPT OSS 120B’s integration of Prospect Theory offers nuanced risk-sensitive decision-making, while "You" emphasizes dynamic alternation. These insights advance understanding of model design in adaptive decision-making systems.  

## Title and Authors (Required)  
**Title**: Not provided in the extracted content.  
**Authors**: Not provided in the extracted content.  
**Affiliations**: Not provided in the extracted content.

===============

## 中文翻译

## 摘要
- **目标**：分析并比较三种不同的强化学习模型（SBB程序），重点研究其在决策机制、适应性及对手建模方面的差异。  
- **方法**：本文通过结构化对比核心组件（如Q值更新、对手建模、状态追踪）来评估模型间在奖励转换、持久性机制、切换奖励以及探索-利用策略上的差异。  
- **结果**：模型在奖励处理（如GPT OSS 120B中的前景理论）、持久性机制（如GPT 5.1中的持久性追踪）及对手建模方法上存在显著差异。所有模型均包含反事实学习和通过缩放因子实现的探索-利用平衡等共性元素。  
- **关键贡献**：  
  1. 引入“你”模型，结合交替策略和虚构博弈实现动态策略。  
  2. 提出GPT 5.1模型，通过预测误差调节的持久性追踪和切换奖励。  
  3. 在GPT OSS 120B中整合前景理论，实现收益-损失不对称性及奖励敏感的切换机制。  
  4. 强调强化学习中适应性、探索与对手建模之间的权衡。  
- **结论**：这些模型展示了不同的强化学习设计哲学，在适应性、探索性及策略深度之间存在权衡。GPT OSS 120B通过整合前景理论实现了更精细的风险敏感决策，而“你”模型则强调动态交替。这些见解推动了对适应性决策系统中模型设计的理解。  

## 标题与作者（必填）  
**标题**：提取内容中未提供。  
**作者**：提取内容中未提供。  
**所属机构**：提取内容中未提供。

#### Reference: 

Source file: 2602.10324v1.pdf

---

**Title**: Discovering Differences in Strategic Behavior between Humans and LLMs
