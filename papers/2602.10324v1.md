The provided text outlines research comparing strategic behavior between humans and large language models (LLMs), particularly in game-theoretic contexts like repeated games (e.g., rock-paper-scissors). Here's a structured summary of the key findings and differences:

---

### **Key Findings and Differences in Strategic Behavior**
1. **Human vs. LLM Adaptation in Repeated Games**:
   - **Humans**: 
     - Use **Experience-Weighted Attraction (EWA)** learning, where past outcomes influence future decisions. 
     - Exhibit **cognitive hierarchies** (e.g., Camerer’s Cognitive Hierarchy model), where players assume others act at lower levels of strategic reasoning.
     - Show **adaptive sequential behavior** with limits, as seen in studies by Brockbank et al. (2024), where humans struggle to maintain optimal strategies over time.
   - **LLMs (e.g., Gemini 2.5, Centaur)**:
     - Demonstrate **superior adaptability** in complex, multi-round games by leveraging vast training data and computational power.
     - Can simulate **perfect Bayesian equilibrium** strategies, avoiding the "limits of adaptive sequential behavior" observed in humans.
     - Outperform humans in **long-term strategic planning** and **pattern recognition**, as highlighted in studies comparing AI agents to human players.

2. **Methodological Insights**:
   - **Computational Models**: 
     - Human behavior is often modeled using **EWA** or **Cognitive Hierarchy** frameworks, which capture bounded rationality.
     - LLMs are evaluated via **game reasoning benchmarks** (e.g., Game Reasoning Arena) and **symbolic cognitive models** derived from human/animal behavior (Castro et al., 2025).
   - **Experimental Setup**:
     - Human studies rely on **controlled experiments** (e.g., repeated rock-paper-scissors) to measure strategic adjustments.
     - AI models are tested in **simulated environments** with varying complexity, emphasizing scalability and efficiency.

3. **Key Differences**:
   - **Learning Speed**: LLMs can process and adapt to new strategies instantaneously, while humans require iterative learning.
   - **Optimal Play**: LLMs can theoretically achieve **Nash equilibrium** in games, whereas humans often deviate due to cognitive biases or limited foresight.
   - **Cognitive Load**: Humans use **heuristics** and **intuition**, while LLMs rely on **data-driven pattern recognition** and **rule-based optimization**.

---

### **Implications and Future Directions**
- **Human Advantages**: 
  - **Creativity** and **contextual understanding** in unpredictable scenarios (e.g., social interactions).
  - **Emotional and ethical reasoning**, which LLMs struggle to replicate.
- **AI Advantages**: 
  - **Scalability** in high-dimensional strategy spaces (e.g., multiplayer games, real-time decision-making).
  - **Consistency** in executing optimal strategies without fatigue or bias.
- **Hybrid Approaches**: 
  - Combining human intuition with AI’s computational power (e.g., **CogBench** experiments) could yield better outcomes in complex tasks.

---

### **Challenges and Limitations**
- **LLM Limitations**: 
  - May lack **empathy** or **ethical reasoning**, leading to suboptimal decisions in social contexts.
  - Requires **training data** that may not capture all human behavioral nuances.
- **Human Limitations**: 
  - **Cognitive biases** (e.g., overconfidence, loss aversion) can hinder strategic performance.
  - **Fatigue** and **attention limits** reduce adaptability in prolonged games.

---

### **Conclusion**
The research underscores that while humans excel in **contextual and creative problem-solving**, LLMs outperform in **computational strategy** and **long-term optimization**. Future work may focus on integrating human-like reasoning into AI models or leveraging AI to enhance human decision-making in strategic domains.

===============

## 中文翻译

### **关键发现与战略行为差异**
1. **人类与LLM在重复博弈中的适应性差异**：
   - **人类**：
     - 采用**经验加权吸引（EWA）**学习机制，过往结果影响未来决策。
     - 展现出**认知层级**（如Camerer的认知层级模型），玩家假设他人处于较低层次的战略推理中。
     - 表现出**适应性序列行为**但存在局限，如Brockbank等人（2024）研究中，人类难以长期维持最优策略。
   - **LLM（如Gemini 2.5、Centaur）**：
     - 在复杂多轮博弈中展现出**优越的适应性**，依赖海量训练数据和计算能力。
     - 可模拟**完美贝叶斯均衡**策略，避免人类观察到的“适应性序列行为局限”。
     - 在**长期战略规划**和**模式识别**方面优于人类，相关研究比较了AI代理与人类玩家的表现。

2. **方法论洞察**：
   - **计算模型**：
     - 人类行为常使用**EWA**或**认知层级**框架，反映有限理性。
     - LLM通过**博弈推理基准测试**（如Game Reasoning Arena）和**基于人类/动物行为的符号认知模型**（Castro等人，2025）进行评估。
   - **实验设置**：
     - 人类研究依赖**受控实验**（如重复的石头剪刀布）测量策略调整。
     - AI模型在**复杂度可变的模拟环境**中测试，强调可扩展性与效率。

3. **关键差异**：
   - **学习速度**：LLM可瞬时处理并适应新策略，而人类需迭代学习。
   - **最优博弈**：LLM理论上可在博弈中达成**纳什均衡**，而人类常因认知偏差或有限远见偏离。
   - **认知负荷**：人类依赖**启发式方法**和**直觉**，LLM则基于**数据驱动的模式识别**和**规则优化**。

---

### **意义与未来方向**
- **人类优势**：
  - 在不可预测场景中的**创造力**与**情境理解**（如社交互动）。
  - **情感与伦理推理**，这是LLM难以复制的。
- **AI优势**：
  - 在高维策略空间（如多人博弈、实时决策）中的**可扩展性**。
  - 执行最优策略的**一致性**，无需疲劳或偏见。
- **混合方法**：
  - 结合人类直觉与AI计算能力（如**CogBench**实验），可提升复杂任务表现。

---

### **挑战与局限**
- **LLM局限**：
  - 可能缺乏**同理心**或**伦理推理**，导致社交场景中决策欠佳。
  - 需要**训练数据**，但可能无法全面捕捉人类行为细微差别。
- **人类局限**：
  - **认知偏差**（如过度自信、损失厌恶）可能阻碍策略表现。
  - **疲劳**和**注意力限制**降低长期博弈中的适应性。

---

### **结论**
研究强调，尽管人类在**情境化与创造性问题解决**方面表现卓越，LLM在**计算策略**和**长期优化**上更具优势。未来研究可能聚焦于将人类推理融入AI模型，或利用AI增强人类在战略领域的决策能力。

#### Reference: 

Source file: 2602.10324v1.pdf