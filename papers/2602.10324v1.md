The provided code snippets represent **Stochastic Behavioral Models (SBBs)** for different agents, each with distinct mechanisms for decision-making, learning, and opponent modeling. Below is a breakdown of **key components** and **differences** between the three models:

---

### **1. Generic Agent (Listing 7)**
**Core Features:**
- **Q-Learning with Regret Minimization**: Updates Q-values based on prediction errors and applies regret to unchosen actions.
- **Opponent Modeling via Fictitious Play**: Uses opponent's historical choice frequencies to compute expected rewards.
- **Combined Value Estimation**: Blends specific-state Q-values, opponent-focused, and my-focused Q-values with weighted coefficients.
- **Dynamic Decision-Making**: Final choice logits combine Q-values and opponent predictions, scaled by a temperature parameter (`beta`).

**Key Equations:**
- **Prediction Error**:  
  $ \text{prediction\_error} = \text{reward\_transformed} - Q[\text{choice}] $
- **Regret Update**:  
  $ Q = Q - \text{unchosen\_mask} \times \text{learning\_rate\_q} \times \text{prediction\_error} $
- **Fictitious Play**:  
  $ \text{fictitious\_play\_expected\_values} = \text{reward\_matrix} \times \text{opponent\_prediction\_probs} $

---

### **2. GPT 5.1 (Listing 8)**
**Core Features:**
- **Prospect Theory Adjustment**: Transforms rewards using gain-loss asymmetry (e.g., negative rewards are scaled differently).
- **Opponent Frequency Tracking**: Maintains a probability distribution over opponent choices with exponential decay.
- **Persistence Mechanism**: Tracks previous choices to bias toward persistence (staying with the same action) or switching.
- **Switch Bonus**: Adds a reward for switching actions, modulated by reward sensitivity.

**Key Equations:**
- **Reward Transformation**:  
  $ \text{reward\_transformed} = \text{reward} \times \text{gain\_loss\_asymmetry\_reward} \text{ if } \text{reward} < 0 $
- **Opponent Frequency Update**:  
  $ \text{opponent\_choice\_frequencies} = (1 - \text{learning\_rate\_opponent}) \times \text{frequencies} + \text{learning\_rate\_opponent} \times \text{one\_hot}(\text{opponent\_choice}) $
- **Persistence Trace**:  
  $ \text{persistence\_trace}[choice] += 1.0 \quad \text{(decays over time)} $

---

### **3. GPT OSS 120B (Listing 9)**
**Core Features:**
- **Decayed Persistence**: Persistence traces decay over time, with a bonus for switching actions.
- **Reward Sensitivity to Switching**: Modulates the switch bonus based on prediction error magnitude.
- **Combined Value Estimation**: Integrates Q-values, opponent model predictions, and persistence traces.
- **Inverse Temperature Scaling**: Controls the "exploration vs. exploitation" trade-off via `inverse_temperature`.

**Key Equations:**
- **Switch Bonus Modulation**:  
  $ \text{modulated\_switch\_bonus} = \text{switch\_bonus} + \text{reward\_sensitivity\_to\_switch} \times |\text{prediction\_error}| $
- **Final Choice Logits**:  
  $ \text{choice\_logits} = \text{inverse\_temperature} \times (\text{Q\_values} + \text{opponent\_model\_weight} \times \text{expected\_rewards}) + \text{switch\_effect} $

---

### **Key Differences**
| **Feature**                | **Generic Agent**         | **GPT 5.1**                     | **GPT OSS 120B**                     |
|---------------------------|---------------------------|----------------------------------|--------------------------------------|
| **Reward Transformation** | None                      | Gain-loss asymmetry (Prospect Theory) | None                                 |
| **Opponent Modeling**     | Fictitious Play          | Frequency-based probabilities   | Frequency-based probabilities        |
| **Persistence Mechanism** | No                        | Yes (trace decay)               | Yes (decayed persistence + switch bonus) |
| **Switch Bonus**          | No                        | No                              | Yes (modulated by reward sensitivity) |
| **Inverse Temperature**   | Yes                       | No                              | Yes                                 |
| **Regret Minimization**   | Yes                       | No                              | No                                   |

---

### **Summary**
- **Generic Agent**: Focuses on Q-learning with regret and opponent prediction via fictitious play.
- **GPT 5.1**: Incorporates **prospect theory** and **persistence** to model human-like biases.
- **GPT OSS 120B**: Adds **decayed persistence** and **switch bonuses** with reward sensitivity, emphasizing adaptive exploration.

These models reflect different trade-offs between **exploration**, **opponent modeling**, and **behavioral biases**, tailored for varying strategic goals.

===============

## 中文翻译

  
提供的代码片段代表了**随机行为模型 (SBBs)**，用于不同智能体的建模，每个模型在决策、学习和对手建模机制上存在差异。以下是三种模型的**核心组件**和**差异**分析：

---

### **1. 通用智能体（代码清单7）**  
**核心特性：**  
- **带有遗憾最小化的Q学习**：根据预测误差更新Q值，并对未选动作施加遗憾。  
- **通过 fictitious play 进行对手建模**：利用对手的历史选择频率计算预期奖励。  
- **综合价值估计**：通过加权系数融合特定状态Q值、以对手为中心的Q值和以自身为中心的Q值。  
- **动态决策**：最终选择的logits结合Q值和对手预测，通过温度参数（`beta`）进行缩放。  

**关键公式：**  
- **预测误差**：  
  $ \text{prediction\_error} = \text{reward\_transformed} - Q[\text{choice}] $  
- **遗憾更新**：  
  $ Q = Q - \text{unchosen\_mask} \times \text{learning\_rate\_q} \times \text{prediction\_error} $  
- **fictitious play**：  
  $ \text{fictitious\_play\_expected\_values} = \text{reward\_matrix} \times \text{opponent\_prediction\_probs} $  

---

### **2. GPT 5.1（代码清单8）**  
**核心特性：**  
- **前景理论调整**：通过收益-损失不对称性（如负奖励以不同方式缩放）转换奖励。  
- **对手频率追踪**：通过指数衰减维护对手选择的概率分布。  
- **持续性机制**：追踪历史选择以偏向持续性（保持相同动作）或切换动作。  
- **切换奖励**：添加切换动作的奖励，受奖励敏感度调节。  

**关键公式：**  
- **奖励转换**：  
  $ \text{reward\_transformed} = \text{reward} \times \text{gain\_loss\_asymmetry\_reward} \text{ if } \text{reward} < 0 $  
- **对手频率更新**：  
  $ \text{opponent\_choice\_frequencies} = (1 - \text{learning\_rate\_opponent}) \times \text{frequencies} + \text{learning\_rate\_opponent} \times \text{one\_hot}(\text{opponent\_choice}) $  
- **持续性痕迹**：  
  $ \text{persistence\_trace}[choice] += 1.0 \quad \text{(随时间衰减)} $  

---

### **3. GPT OSS 120B（代码清单9）**  
**核心特性：**  
- **衰减持续性**：持续性痕迹随时间衰减，切换动作会获得额外奖励。  
- **切换奖励敏感度**：根据预测误差幅度调节切换奖励。  
- **综合价值估计**：整合Q值、对手模型预测和持续性痕迹。  
- **逆温度缩放**：通过 `inverse_temperature` 参数控制“探索 vs. 利用”权衡。  

**关键公式：**  
- **切换奖励调节**：  
  $ \text{modulated\_switch\_bonus} = \text{switch\_bonus} + \text{reward\_sensitivity\_to\_switch} \times |\text{prediction\_error}| $  
- **最终选择logits**：  
  $ \text{choice\_logits} = \text{inverse\_temperature} \times (\text{Q\_values} + \text{opponent\_model\_weight} \times \text{expected\_rewards}) + \text{switch\_effect} $  

---

### **关键差异**  
| **特性**                | **通用智能体**         | **GPT 5.1**                     | **GPT OSS 120B**                     |  
|---------------------------|---------------------------|----------------------------------|--------------------------------------|  
| **奖励转换**            | 无                      | 收益-损失不对称性（前景理论） | 无                                 |  
| **对手建模**            | fictitious play        | 基于频率的概率分布             | 基于频率的概率分布                 |  
| **持续性机制**          | 无                      | 是（痕迹衰减）                 | 是（衰减持续性 + 切换奖励）        |  
| **切换奖励**            | 无                      | 无                            | 是（受奖励敏感度调节）             |  
| **逆温度**              | 是                      | 否                            | 是                                 |  
| **遗憾最小化**          | 是                      | 否                            | 否                                 |  

---

### **总结**  
- **通用智能体**：聚焦于带有遗憾最小化的Q学习和通过fictitious play进行的对手预测。  
- **GPT 5.1**：结合**前景理论**和**持续性机制**，模拟人类偏见。  
- **GPT OSS 120B**：添加**衰减持续性**和**奖励敏感度调节的切换奖励**，强调自适应探索。  

这些模型反映了在**探索**、**对手建模**和**行为偏见**之间的不同权衡，适用于不同的战略目标。  


#### Reference: 

Source file: 2602.10324v1.pdf

---

**Title**: Discovering Differences in Strategic Behavior between Humans and LLMs
