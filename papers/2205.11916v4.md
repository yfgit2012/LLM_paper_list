The provided text discusses **Zero-shot Chain-of-Thought (CoT)** reasoning and its effectiveness in improving model performance across various tasks and model scales. Here's a structured summary of the key findings and insights:

---

### **Key Findings**
1. **Zero-shot-CoT vs. Zero-shot**:
   - **Zero-shot-CoT** significantly outperforms **Zero-shot** in multiple benchmarks, especially when combined with **self-consistency** (majority voting over multiple reasoning paths).
   - Example results (from Table 25):
     - **MultiArith**: Zero-shot-CoT + self-consistency achieves **89.0% accuracy** (vs. Zero-shot: 25.5%).
     - **SVAMP**: Zero-shot-CoT + self-consistency reaches **80.5%** (vs. Zero-shot: 63.1%).

2. **Role of Self-Consistency**:
   - Self-consistency (generating multiple reasoning paths and selecting the majority vote) further boosts performance, particularly for large models.
   - Example: **PaLM 540B** with Zero-shot-CoT + self-consistency achieves **89.0%** on MultiArith.

3. **Model Scaling Benefits**:
   - Larger models (e.g., **GPT-3 175B**, **PaLM 540B**) benefit more from CoT and self-consistency.
   - For **MultiArith**:
     - **GPT-3 175B**: Zero-shot-CoT reaches **78.7%** (vs. Zero-shot: 3.3%).
     - **PaLM 540B**: Zero-shot-CoT + self-consistency achieves **89.0%**.

4. **Few-shot vs. Zero-shot-CoT**:
   - **Few-shot-CoT** (with 8 examples) also performs well, but **Zero-shot-CoT** (no examples) matches or exceeds it for large models.
   - Example: **GPT-3 175B**:
     - Zero-shot-CoT: **78.7%** (vs. Few-shot-CoT: 93.0%).

---

### **Datasets and Models Tested**
- **Datasets**:
  - **AQUA-RAT**, **SVAMP**, **GSM8K**, **MultiArith** (math reasoning tasks).
- **Models**:
  - **GPT-3 variants** (text-davinci-001, text-davinci-002, 175B parameters).
  - **PaLM** (8B, 62B, 540B parameters).
  - **GPT-2**, **GPT-Neo**, **GPT-J**, **T0**, **OPT** (various parameter counts).

---

### **Implications**
- **CoT is effective for large models**: Larger models benefit more from CoT, suggesting that reasoning capabilities scale with model size.
- **Self-consistency enhances reliability**: By aggregating multiple reasoning paths, self-consistency reduces errors and improves accuracy.
- **Zero-shot-CoT is competitive**: Even without examples, Zero-shot-CoT performs well, especially with self-consistency, making it a promising approach for low-resource scenarios.

---

### **Conclusion**
The experiments highlight that **Zero-shot-CoT**, especially when combined with **self-consistency**, is a powerful method for improving reasoning tasks across diverse models and datasets. Larger models (e.g., PaLM 540B, GPT-3 175B) achieve the highest performance gains, underscoring the importance of model scale in leveraging CoT. This approach has significant implications for applications requiring robust, zero-shot reasoning without explicit training data.

===============

## 中文翻译

### **主要发现**
1. **零样本链式推理（Zero-shot-CoT） vs. 零样本（Zero-shot）**：
   - **零样本链式推理（Zero-shot-CoT）** 在多个基准测试中显著优于 **零样本（Zero-shot）**，尤其是在结合 **自洽性**（对多个推理路径进行多数投票）时表现更佳。
   - 示例结果（来自表25）：
     - **MultiArith**：零样本链式推理 + 自洽性达到 **89.0% 准确率**（对比零样本：25.5%）。
     - **SVAMP**：零样本链式推理 + 自洽性达到 **80.5%**（对比零样本：63.1%）。

2. **自洽性的作用**：
   - 自洽性（生成多个推理路径并采用多数投票）进一步提升了性能，尤其是对大模型表现尤为显著。
   - 示例：**PaLM 540B** 配合零样本链式推理 + 自洽性在 MultiArith 上达到 **89.0%**。

3. **模型规模的提升效益**：
   - 更大的模型（如 **GPT-3 175B**、**PaLM 540B**）从链式推理（CoT）和自洽性中获益更多。
   - 对于 **MultiArith**：
     - **GPT-3 175B**：零样本链式推理达到 **78.7%**（对比零样本：3.3%）。
     - **PaLM 540B**：零样本链式推理 + 自洽性达到 **89.0%**。

4. **少样本 vs. 零样本链式推理**：
   - **少样本链式推理（Few-shot-CoT）**（使用8个示例）表现良好，但 **零样本链式推理（Zero-shot-CoT）**（无需示例）在大模型中表现匹配或超越前者。
   - 示例：**GPT-3 175B**：
     - 零样本链式推理：**78.7%**（对比少样本链式推理：93.0%）。

---

### **测试的数据集与模型**
- **数据集**：
  - **AQUA-RAT**、**SVAMP**、**GSM8K**、**MultiArith**（数学推理任务）。
- **模型**：
  - **GPT-3 变体**（text-davinci-001、text-davinci-002、175B 参数）。
  - **PaLM**（8B、62B、540B 参数）。
  - **GPT-2**、**GPT-Neo**、**GPT-J**、**T0**、**OPT**（不同参数量）。

---

### **启示**
- **链式推理（CoT）对大模型有效**：更大的模型从 CoT 中获益更多，表明推理能力随模型规模提升。
- **自洽性增强可靠性**：通过聚合多个推理路径，自洽性可减少错误并提高准确率。
- **零样本链式推理（Zero-shot-CoT）具有竞争力**：即使没有示例，零样本链式推理（尤其是结合自洽性）表现优异，是低资源场景下的有力方法。

---

### **结论**
实验表明，**零样本链式推理（Zero-shot-CoT）**，尤其是结合 **自洽性**，是提升各类模型和数据集推理任务的有力方法。更大的模型（如 PaLM 540B、GPT-3 175B）获得最高性能提升，突显了模型规模在利用 CoT 中的重要性。该方法对无需显式训练数据即可实现稳健零样本推理的应用具有重要意义。

#### Reference: 

Source file: 2205.11916v4.pdf

---

**Title**: Takeshi Kojima

**Authors & Affiliations**: t.kojima@weblab.t.u-tokyo.ac.jp
