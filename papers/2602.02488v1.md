The document describes a **task-specific algorithm** for adaptive task generation and policy refinement, primarily in reinforcement learning (RL) settings. Below is a structured breakdown of the algorithm and its components:

---

### **Key Components**
1. **Policy (ğœ‹ğœƒ)**: The agent's strategy for interacting with the environment.
2. **Reward Model (ğ‘Ÿğœ™)**: Evaluates task performance, providing feedback for policy updates.
3. **Thresholds (ğ›¼_high, ğ›¼_low)**: Define performance bounds for task difficulty adaptation.
4. **Task Types**: 
   - **OSWorld/Alf World**: Simulated environments (e.g., robotic tasks).
   - **Coding Tasks**: Code generation/execution tasks with unit tests.

---

### **Algorithm Overview (Algorithm 2)**
#### **Step 1: Sampling Tasks**
- **Input**: A set of tasks $ Q $, policy $ \pi_\theta $, reward model $ r_\phi $, and thresholds $ \alpha_{\text{high}}, \alpha_{\text{low}} $.
- **Process**:
  - Sample a subset $ S \subset Q \setminus T $ (where $ T $ is a temporary task set).
  - For $ k \geq 2 $, merge $ S $ with $ T $ to avoid redundant sampling.

#### **Step 2: Task Evaluation**
- **OSWorld/Alf World**:
  - Policy $ \pi_\theta $ generates trajectories $ \mathcal{T}_q $ for each task $ q \in S' $.
  - Reward model $ r_\phi $ computes step-wise quality $ R_{\tau_i} $ using a weighted sum of outcomes $ O_\tau $ and reward scores $ S_{\tau i,j} $.
- **Coding Tasks**:
  - Policy generates code $ \tau $, which is evaluated against unit tests $ U_q $.
  - Reward model computes accuracy $ R_\tau $ based on test passes and failure rates.

#### **Step 3: Task Adaptation**
- **Accept New Tasks**:
  - If a task $ q $'s accuracy $ \text{acc}(q) $ exceeds $ \alpha_{\text{high}} $, replace it with a **harder** task $ q' $.
  - If $ \text{acc}(q) < \alpha_{\text{low}} $, replace it with an **easier** task $ q' $.
- **Adapt Environment**:
  - Summarize step-wise errors for tasks with low accuracy.
  - Propose new tasks (easier/harder) based on error summaries.

#### **Step 4: Policy and Reward Model Training**
- **Policy Update**:
  - Compute advantages $ A^{[\pi]}_{\tau_i} $ using standardized rewards $ R_{\tau_i} $.
  - Train $ \pi_\theta $ with pairs $ (\tau_i, A^{[\pi]}_{\tau_i}) $.
- **Reward Model Update**:
  - Compute advantages $ A^{[r]}_{\tau i,j} $ using standardized reward scores $ R_{\tau i,j} $.
  - Train $ r_\phi $ with pairs $ (r_{\tau i,j}, A^{[r]}_{\tau i,j}) $.

---

### **Task-Specific Details**
- **OSWorld/Alf World**:
  - Trajectories $ \tau $ are evaluated for outcomes $ O_\tau \in \{-1, 1\} $.
  - Reward scores $ S_{\tau i,j} $ are aggregated to compute step-wise quality.
- **Coding Tasks**:
  - Code is tested against unit tests $ U_q $, with outcomes $ O_\tau \in \{-1, 1\} $.
  - Reward model uses pass rates $ p $ to compute task-specific rewards.

---

### **Key Concepts**
- **Adaptive Difficulty**: Tasks are dynamically adjusted (easier/harder) based on the policy's performance.
- **Reward Standardization**: Ensures fair comparison of rewards across tasks and steps.
- **Multi-Task Learning**: The algorithm handles diverse task types (simulated, coding) with unified evaluation.

---

### **Final Answer**
$$
\boxed{
\text{The algorithm adaptively adjusts task difficulty based on policy performance, using a reward model to refine the policy and task set for improved learning efficiency.}
}
$$

===============

## ä¸­æ–‡ç¿»è¯‘

**å…³é”®ç»„ä»¶**  
1. **ç­–ç•¥ï¼ˆğœ‹ğœƒï¼‰**ï¼šæ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’çš„ç­–ç•¥ã€‚  
2. **å¥–åŠ±æ¨¡å‹ï¼ˆğ‘Ÿğœ™ï¼‰**ï¼šè¯„ä¼°ä»»åŠ¡è¡¨ç°ï¼Œä¸ºç­–ç•¥æ›´æ–°æä¾›åé¦ˆã€‚  
3. **é˜ˆå€¼ï¼ˆğ›¼_high, ğ›¼_lowï¼‰**ï¼šå®šä¹‰ä»»åŠ¡éš¾åº¦é€‚åº”çš„æ€§èƒ½è¾¹ç•Œã€‚  
4. **ä»»åŠ¡ç±»å‹**ï¼š  
   - **OSWorld/Alf World**ï¼šæ¨¡æ‹Ÿç¯å¢ƒï¼ˆå¦‚æœºå™¨äººä»»åŠ¡ï¼‰ã€‚  
   - **ç¼–ç¨‹ä»»åŠ¡**ï¼šä»£ç ç”Ÿæˆ/æ‰§è¡Œä»»åŠ¡ï¼ŒåŒ…å«å•å…ƒæµ‹è¯•ã€‚  

---

**ç®—æ³•æ¦‚è¿°ï¼ˆç®—æ³•2ï¼‰**  
#### **æ­¥éª¤1ï¼šä»»åŠ¡é‡‡æ ·**  
- **è¾“å…¥**ï¼šä»»åŠ¡é›†åˆ $ Q $ã€ç­–ç•¥ $ \pi_\theta $ã€å¥–åŠ±æ¨¡å‹ $ r_\phi $ ä»¥åŠé˜ˆå€¼ $ \alpha_{\text{high}}, \alpha_{\text{low}} $ã€‚  
- **è¿‡ç¨‹**ï¼š  
  - ä» $ Q \setminus T $ï¼ˆ$ T $ ä¸ºä¸´æ—¶ä»»åŠ¡é›†ï¼‰ä¸­é‡‡æ ·å­é›† $ S \subset Q \setminus T $ã€‚  
  - å¯¹äº $ k \geq 2 $ï¼Œå°† $ S $ ä¸ $ T $ åˆå¹¶ä»¥é¿å…å†—ä½™é‡‡æ ·ã€‚  

#### **æ­¥éª¤2ï¼šä»»åŠ¡è¯„ä¼°**  
- **OSWorld/Alf World**ï¼š  
  - ç­–ç•¥ $ \pi_\theta $ ä¸ºæ¯ä¸ªä»»åŠ¡ $ q \in S' $ ç”Ÿæˆè½¨è¿¹ $ \mathcal{T}_q $ã€‚  
  - å¥–åŠ±æ¨¡å‹ $ r_\phi $ é€šè¿‡ç»“æœ $ O_\tau $ çš„åŠ æƒå’Œä¸å¥–åŠ±å¾—åˆ† $ S_{\tau i,j} $ è®¡ç®—åˆ†æ­¥è´¨é‡ $ R_{\tau_i} $ã€‚  
- **ç¼–ç¨‹ä»»åŠ¡**ï¼š  
  - ç­–ç•¥ç”Ÿæˆä»£ç  $ \tau $ï¼Œå¹¶é€šè¿‡å•å…ƒæµ‹è¯• $ U_q $ è¿›è¡Œè¯„ä¼°ã€‚  
  - å¥–åŠ±æ¨¡å‹åŸºäºæµ‹è¯•é€šè¿‡ç‡å’Œå¤±è´¥ç‡è®¡ç®—å‡†ç¡®æ€§ $ R_\tau $ã€‚  

#### **æ­¥éª¤3ï¼šä»»åŠ¡é€‚åº”**  
- **æ¥å—æ–°ä»»åŠ¡**ï¼š  
  - è‹¥ä»»åŠ¡ $ q $ çš„å‡†ç¡®æ€§ $ \text{acc}(q) $ è¶…è¿‡ $ \alpha_{\text{high}} $ï¼Œåˆ™æ›¿æ¢ä¸ºæ›´éš¾çš„ä»»åŠ¡ $ q' $ã€‚  
  - è‹¥ $ \text{acc}(q) < \alpha_{\text{low}} $ï¼Œåˆ™æ›¿æ¢ä¸ºæ›´æ˜“çš„ä»»åŠ¡ $ q' $ã€‚  
- **é€‚åº”ç¯å¢ƒ**ï¼š  
  - å¯¹ä½å‡†ç¡®æ€§ä»»åŠ¡æ±‡æ€»åˆ†æ­¥é”™è¯¯ã€‚  
  - æ ¹æ®é”™è¯¯æ±‡æ€»æå‡ºæ–°ä»»åŠ¡ï¼ˆæ›´æ˜“/æ›´éš¾ï¼‰ã€‚  

#### **æ­¥éª¤4ï¼šç­–ç•¥ä¸å¥–åŠ±æ¨¡å‹è®­ç»ƒ**  
- **ç­–ç•¥æ›´æ–°**ï¼š  
  - ä½¿ç”¨æ ‡å‡†åŒ–å¥–åŠ± $ R_{\tau_i} $ è®¡ç®—ä¼˜åŠ¿ $ A^{[\pi]}_{\tau_i} $ã€‚  
  - ç”¨ $ (\tau_i, A^{[\pi]}_{\tau_i}) $ å¯¹ $ \pi_\theta $ è¿›è¡Œè®­ç»ƒã€‚  
- **å¥–åŠ±æ¨¡å‹æ›´æ–°**ï¼š  
  - ä½¿ç”¨æ ‡å‡†åŒ–å¥–åŠ±å¾—åˆ† $ R_{\tau i,j} $ è®¡ç®—ä¼˜åŠ¿ $ A^{[r]}_{\tau i,j} $ã€‚  
  - ç”¨ $ (r_{\tau i,j}, A^{[r]}_{\tau i,j}) $ å¯¹ $ r_\phi $ è¿›è¡Œè®­ç»ƒã€‚  

---

**ä»»åŠ¡ç‰¹å®šç»†èŠ‚**  
- **OSWorld/Alf World**ï¼š  
  - è½¨è¿¹ $ \tau $ çš„ç»“æœ $ O_\tau \in \{-1, 1\} $ ç”¨äºè¯„ä¼°ã€‚  
  - å¥–åŠ±å¾—åˆ† $ S_{\tau i,j} $ èšåˆä»¥è®¡ç®—åˆ†æ­¥è´¨é‡ã€‚  
- **ç¼–ç¨‹ä»»åŠ¡**ï¼š  
  - ä»£ç é€šè¿‡å•å…ƒæµ‹è¯• $ U_q $ è¿›è¡Œæµ‹è¯•ï¼Œç»“æœ $ O_\tau \in \{-1, 1\} $ã€‚  
  - å¥–åŠ±æ¨¡å‹ä½¿ç”¨é€šè¿‡ç‡ $ p $ è®¡ç®—ä»»åŠ¡ç‰¹å®šå¥–åŠ±ã€‚  

---

**å…³é”®æ¦‚å¿µ**  
- **è‡ªé€‚åº”éš¾åº¦**ï¼šæ ¹æ®ç­–ç•¥è¡¨ç°åŠ¨æ€è°ƒæ•´ä»»åŠ¡éš¾åº¦ï¼ˆæ›´æ˜“/æ›´éš¾ï¼‰ã€‚  
- **å¥–åŠ±æ ‡å‡†åŒ–**ï¼šç¡®ä¿è·¨ä»»åŠ¡å’Œæ­¥éª¤çš„å¥–åŠ±å…¬å¹³æ¯”è¾ƒã€‚  
- **å¤šä»»åŠ¡å­¦ä¹ **ï¼šç®—æ³•ç»Ÿä¸€è¯„ä¼°å¤šæ ·ä»»åŠ¡ç±»å‹ï¼ˆæ¨¡æ‹Ÿã€ç¼–ç¨‹ï¼‰ã€‚  

---

**æœ€ç»ˆç­”æ¡ˆ**  
$$
\boxed{
\text{è¯¥ç®—æ³•æ ¹æ®ç­–ç•¥è¡¨ç°è‡ªé€‚åº”è°ƒæ•´ä»»åŠ¡éš¾åº¦ï¼Œåˆ©ç”¨å¥–åŠ±æ¨¡å‹ä¼˜åŒ–ç­–ç•¥å’Œä»»åŠ¡é›†ä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚}
}
$$

#### Reference: 

Source file: 2602.02488v1.pdf

---

**Title**: RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System

**Authors & Affiliations**: We also that optimized reward-model signals outperform outcomes that rely on human labels.
