The provided document discusses the development and evaluation of **DR Tulu**, a model trained using reinforcement learning (RL) with an auto-search pipeline. Here's a structured summary of key findings and insights:

---

### **1. RL Training vs. Downstream Evaluation Mismatch**
- **Issue**: Models achieving high training rewards (e.g., from search-based rubrics) did not necessarily outperform in downstream tasks (e.g., ResearchQA, Healthbench).
- **Reasons for Mismatch**:
  - **Different Rubrics**: Training rubrics (e.g., Healthbench uses expert-annotated metrics) differ from evaluation rubrics, leading to misaligned optimization goals.
  - **Reward Hacking**: RL training used a different judge model (GPT-4.1-mini) than downstream evaluators (e.g., Gemini Flash 2.5, SQAv2), causing models to prioritize features preferred by the training judge.
  - **Model Priors**: Variations in model priors (e.g., weaker models producing low-quality answers) may lead to suboptimal reward exploitation during training.

---

### **2. Search Engine Variability**
- **Unstable Results**: Search engines (e.g., Serper) produce inconsistent top-k snippets over time, even for the same query:
  - **Short Interval**: Top-10 overlap averaged ~7.67, with rank matches dropping to 5.64.
  - **One Week Apart**: Divergence increases significantly, highlighting reliability concerns in data sourcing.
- **Impact**: This variability affects the model's ability to generate consistent answers, as search results drive the training process.

---

### **3. Inference and Judge Model Variance**
- **Inference Variability**: 
  - For **short-form tasks** (e.g., 2Wiki), GPT-4.1's outputs differed by 29.3% across trajectories.
  - **Long-form tasks** (e.g., Healthbench, ResearchQA) showed lower variance (17.1% and 9.81%, respectively).
- **Judge Model Consistency**: 
  - GPT-4.1 evaluations showed high reliability (e.g., 67.67% agreement on 2Wiki) but highlighted the need for standardized rubrics.

---

### **4. Qualitative Examples**
- **ResearchQA Trajectories**: The document includes example trajectories of DR Tulu's reasoning process, showing how it integrates search results and generates responses. These examples are available at [https://dr-tulu.github.io/](https://dr-tulu.github.io/).

---

### **5. Key Takeaways for Future Work**
- **Rubric Alignment**: Addressing mismatches between training and evaluation rubrics could improve RL effectiveness.
- **Consistent Evaluation**: Using standardized judges and metrics is critical to ensure models are optimized for real-world tasks.
- **Search Stability**: Reducing search engine variability (e.g., through caching or hybrid strategies) may enhance model reliability.

---

### **Summary**
DR Tulu highlights challenges in aligning RL training with practical evaluation, emphasizing the need for robust rubrics, consistent judges, and reliable data sources. The findings underscore the importance of bridging the gap between synthetic training environments and real-world performance.

===============

## 中文翻译

**DR Tulu** 的开发与评估讨论了使用强化学习（RL）和自动搜索流水线训练的模型。以下是关键发现和洞察的结构化摘要：

---

### **1. RL 训练与下游评估不匹配**
- **问题**：在训练中获得高奖励（如基于搜索的评分标准）的模型，未必在下游任务（如 ResearchQA、Healthbench）中表现更优。
- **不匹配原因**：
  - **不同评分标准**：训练评分标准（如 Healthbench 使用专家标注的指标）与评估评分标准不同，导致优化目标不一致。
  - **奖励黑客行为**：RL 训练使用了不同的判断模型（如 GPT-4.1-mini），而下游评估使用其他模型（如 Gemini Flash 2.5、SQAv2），导致模型优先考虑训练判断模型偏好的特征。
  - **模型先验差异**：模型先验差异（如较弱模型生成低质量答案）可能导致训练期间奖励利用不充分。

---

### **2. 搜索引擎的不稳定性**
- **不稳定结果**：搜索引擎（如 Serper）会随时间产生不一致的前 k 个摘要片段，即使对同一查询：
  - **短时间间隔**：前 10 个片段重叠平均为 7.67，排名匹配降至 5.64。
  - **相隔一周**：分歧显著增加，凸显了数据来源的可靠性问题。
- **影响**：这种不稳定性影响了模型生成一致答案的能力，因为搜索结果驱动了训练过程。

---

### **3. 推理与判断模型的差异**
- **推理差异**：
  - 对于**短文本任务**（如 2Wiki），GPT-4.1 的输出在不同轨迹中差异达 29.3%。
  - **长文本任务**（如 Healthbench、ResearchQA）差异较低（分别为 17.1% 和 9.81%）。
- **判断模型一致性**：
  - GPT-4.1 的评估显示高可靠性（如 2Wiki 的 67.67% 一致性），但也凸显了标准化评分标准的必要性。

---

### **4. 定性示例**
- **ResearchQA 轨迹**：文档包含 DR Tulu 推理过程的示例轨迹，展示了其如何整合搜索结果并生成回答。这些示例可在 [https://dr-tulu.github.io/](https://dr-tulu.github.io/) 查看。

---

### **5. 未来工作的关键要点**
- **评分标准对齐**：解决训练与评估评分标准之间的不匹配，可提升 RL 的有效性。
- **一致评估**：使用标准化的判断者和指标是确保模型优化实际任务的关键。
- **搜索稳定性**：减少搜索引擎的不稳定性（如通过缓存或混合策略）可能增强模型可靠性。

---

### **总结**
DR Tulu 揭示了强化学习训练与实际评估对齐的挑战，强调了需要稳健的评分标准、一致的判断者和可靠的数据来源。这些发现突显了弥合合成训练环境与实际表现之间差距的重要性。

#### Reference: 

Source file: 2511.19399v2.pdf

---

**Title**: DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research

**Authors & Affiliations**: _♡_ Joint first authors, _†_ Core contributors. See full author contributions here. **Correspondence:** `[rulins@cs.washington.edu](mailto:rulins@cs.washington.edu)`, `[akaria@allenai.org](mailto:akaria@allenai.org)`
