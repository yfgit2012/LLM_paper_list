## Summary
- **Objective**: To evaluate the DR Tulu model's performance across tasks, focusing on tool call budgets, result variance, and training-evaluation mismatches, with the goal of identifying practical implications and research directions for improving model efficiency and alignment.  
- **Methodologies**: Ablation studies on tool call budgets, variance analysis using metrics like `set_same` and `pos_match`, benchmark evaluations (2Wiki, Healthbench, ResearchQA), and comparison of training-reward alignment with downstream performance. Judge model consistency was assessed using GPT-4.1 and Gemini Flash 2.5.  
- **Results**:  
  - Performance saturation at ~5 tool calls (SFT/RL) with marginal gains up to 10 calls.  
  - Search engine instability caused inconsistent results (e.g., 7.67 overlaps in top-10 snippets).  
  - Inference variance: 29.3% answer divergence in 2Wiki, 17.1% score differences in Healthbench.  
  - RL-trained models underperformed in downstream tasks due to rubric mismatches, reward hacking, and model prior differences.  
- **Key Contributions**:  
  - Quantified tool call budget limits and search result instability.  
  - Identified variance sources (tool, inference, judge models) and training-evaluation mismatches.  
  - Highlighted the need for standardized rubrics and judge models to ensure evaluation consistency.  
- **Conclusions**: Balancing tool efficiency, evaluation rigor, and training alignment is critical for deploying DR Tulu in real-world tasks. Addressing training-evaluation mismatches and variance sources requires aligning rewards with downstream objectives and improving judge model consistency.  

## Title and Authors  
**Title**: "Analyzing DR Tulu: Tool Call Budgets, Result Variance, and Training-Evaluation Mismatches"  
**Authors**: [Authors' Names]  
**Affiliations**: [Affiliations]

===============

## 中文翻译

## 摘要
- **目标**：评估DR Tulu模型在各项任务中的表现，重点关注工具调用预算、结果方差以及训练-评估不匹配问题，旨在识别模型效率与对齐性的实际影响及研究方向。  
- **方法**：通过工具调用预算的消融实验、使用`set_same`和`pos_match`等指标进行方差分析、基准测试（2Wiki、Healthbench、ResearchQA）以及训练-奖励对齐与下游性能的对比分析。模型一致性由GPT-4.1和Gemini Flash 2.5进行评估。  
- **结果**：  
  - 性能在约5次工具调用时达到饱和（SFT/RL），边际收益可提升至10次调用。  
  - 搜索引擎的不稳定性导致结果不一致（例如，前10个摘要片段重合度达7.67%）。  
  - 推理方差：2Wiki中29.3%的答案分歧，Healthbench中17.1%的得分差异。  
  - 由于评分标准不匹配、奖励篡改和模型先验差异，RL训练模型在下游任务中表现欠佳。  
- **关键贡献**：  
  - 量化了工具调用预算的限制和搜索结果的不稳定性。  
  - 识别了方差来源（工具、推理、评估模型）及训练-评估不匹配问题。  
  - 强调了标准化评分标准和评估模型的必要性，以确保评估一致性。  
- **结论**：在实际任务中部署DR Tulu需平衡工具效率、评估严谨性与训练对齐性。解决训练-评估不匹配和方差来源需将奖励与下游目标对齐，并提升评估模型的一致性。  

## 标题与作者  
**标题**："分析DR Tulu：工具调用预算、结果方差及训练-评估不匹配"  
**作者**：[作者姓名]  
**所属机构**：[所属机构]

#### Reference: 

Source file: 2511.19399v2.pdf

---

**Title**: DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research

**Authors & Affiliations**: _♡_ Joint first authors, _†_ Core contributors. See full author contributions here. **Correspondence:** `[rulins@cs.washington.edu](mailto:rulins@cs.washington.edu)`, `[akaria@allenai.org](mailto:akaria@allenai.org)`
