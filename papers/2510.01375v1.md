## Summary  
- **Objective**: To evaluate the performance of large language models (LLMs) across different prompting strategies and tasks, with a focus on the impact of distillation and retrieval optimization on efficiency and accuracy.  
- **Methodologies**: The study employs distillation techniques on 7B and 14B models using prompting strategies like ReAct and StateAct. Ablation studies on retrieval depth (k=Top-k) are conducted, and baseline comparisons (e.g., RAG) are included. External LLMs (e.g., ChatGPT) are used for drafting and editing, but the final work is attributed to the authors.  
- **Results**:  
  - Distillation significantly improves performance for both 7B and 14B models, with 14B models achieving 92.54% success in ALFWorld and 72.32 score in WebShop, while 7B models match or exceed larger models’ performance with efficiency gains.  
  - Distilled models reduce token usage (e.g., 5.95k vs. 9.36k tokens in WebShop for 7B models) and steps per episode.  
  - Retrieval depth (k=Top-k) ablations show k=3 is optimal for both tasks, balancing performance and efficiency.  
- **Key Contributions**:  
  1. Demonstrates that distillation enhances small models’ performance, enabling them to rival larger models in complex tasks.  
  2. Identifies task-specific retrieval depth optimizations (k=3 for WebShop, k=6 for ALFWorld) for efficiency.  
  3. Highlights the trade-off between efficiency and accuracy, showing distilled models achieve a balanced performance.  
  4. Emphasizes the role of external LLMs in research workflows while underscoring human oversight.  
- **Conclusions**: Distillation is a critical technique for improving efficiency and accuracy in LLMs, particularly for smaller models. Task-specific retrieval depth optimization is essential for performance, and smaller models can achieve comparable results to larger ones with distillation. Human oversight remains vital in LLM-driven research.  

## Title and Authors  
**Title**: "Distilling Prompting Strategies for Efficient and Accurate Large Language Models"  
**Authors**: [Author Names]  
**Affiliations**: [Institutional Affiliations]

===============

## 中文翻译

## 摘要  
- **目标**：评估大型语言模型（LLMs）在不同提示策略和任务中的表现，重点关注蒸馏和检索优化对效率和准确性的影晌。  
- **方法**：研究采用蒸馏技术对7B和14B模型进行优化，结合ReAct和StateAct等提示策略。对检索深度（k=Top-k）进行消融实验，并包含基线对比（如RAG）。使用外部LLMs（如ChatGPT）进行初稿和编辑，但最终成果归作者所有。  
- **结果**：  
  - 蒸馏显著提升了7B和14B模型的性能，其中14B模型在ALFWorld任务中达成92.54%的成功率，在WebShop任务中得分72.32；7B模型在效率提升的前提下，表现可匹配甚至超越更大模型。  
  - 蒸馏模型减少了token使用量（如7B模型在WebShop任务中从9.36k降至5.95k token）和每轮交互步骤。  
  - 检索深度（k=Top-k）的消融实验表明，k=3在两项任务中均达到性能与效率的平衡。  
- **关键贡献**：  
  1. 证明蒸馏可提升小模型性能，使其在复杂任务中与大模型竞争。  
  2. 识别任务特定的检索深度优化（如WebShop任务k=3，ALFWorld任务k=6）以提高效率。  
  3. 强调效率与准确性的权衡，表明蒸馏模型可实现性能平衡。  
  4. 强调外部LLMs在研究流程中的作用，同时突出人类监督的必要性。  
- **结论**：蒸馏是提升LLMs效率和准确性的关键技术，尤其对小型模型至关重要。任务特定的检索深度优化对性能至关重要，蒸馏使小型模型可达到与大型模型相当的成果。人类监督在LLM驱动的研究中仍不可或缺。  

## 标题与作者  
**标题**：“用于高效准确大型语言模型的提示策略蒸馏”  
**作者**：[作者姓名]  
**所属机构**：[机构信息]

#### Reference: 

Source file: 2510.01375v1.pdf

---

**Title**: Humaid Ibrahim, Nikolai Rozanov, Marek Rei

**Authors & Affiliations**: _{_ humaid.ibrahim24, nikolai.rozanov13, marek.rei _}_ @imperial.ac.uk
