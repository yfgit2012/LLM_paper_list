The document presents a study on distilling large language models (LLMs) for specific tasks, focusing on **Qwen-2.5** models (14B and 7B parameters) across two environments: **ALFWorld** and **WebShop**. Key findings and insights include:

---

### **1. Distillation's Effectiveness Across Prompting Styles**
- **14B Models**:
  - **ReAct + Distillation** achieves the highest success rate in **ALFWorld** (92.54%) and improves **WebShop** score to 72.32.
  - **StateAct + Distillation** outperforms baselines in **WebShop** (72.49 score).
- **7B Models**:
  - Distillation significantly boosts efficiency and performance:
    - **ReAct + Distillation** improves **WebShop** score from 44.0 to 57.1.
    - **StateAct + Distillation** reaches 64.97% success in ALFWorld and 65.0 score in WebShop, outperforming baselines.

**Key Takeaway**: Distillation enhances both **effectiveness** (task performance) and **efficiency** (token/step usage) across different prompting strategies (ReAct, StateAct).

---

### **2. Efficiency Gains with Distillation**
- **7B Model Efficiency** (Table 8):
  - Distilled models use **fewer tokens** and **steps** than baselines:
    - **WebShop**: Distilled model uses **5.95k tokens** (vs. 9.36k base) and **7.15 steps** (vs. 7.89 base).
    - **ALFWorld**: Distilled model achieves **68.66% success** (vs. 11.19% base) with **67.62k tokens**.
  - Distillation reduces reliance on retrieval (e.g., RAG), improving efficiency without sacrificing performance.

---

### **3. Retrieval Depth (k) Ablations**
- **Optimal Retrieval Depth**: 
  - **k=3** balances performance and efficiency:
    - **ALFWorld**: 82.09% success with 53.97k tokens.
    - **WebShop**: 43.50% success (67.08 score) with 11.05k tokens.
  - Larger k (e.g., 6, 9) leads to **diminishing returns**:
    - Increased token usage (e.g., 12.75k for k=6 in WebShop) and lower scores (≈61).
    - k=9 causes instability (76.9% success, 57.26k tokens).

**Recommendation**: Use **k=3** as the default. For structure-heavy tasks (ALFWorld), use **k=6**; for noisy, attribute-heavy tasks (WebShop), use **k=3**.

---

### **4. Model-Specific Insights**
- **14B vs. 7B**:
  - **14B models** benefit more from distillation in **ALFWorld** (ReAct jumps from 77.6% to 92.5%).
  - **7B models** show stronger efficiency gains (e.g., WebShop score improves from 44.0 to 57.1 with distillation).
- **RAG (Retrieval-Augmented Generation)**:
  - Provides performance boosts but at the cost of higher token usage (e.g., 62.74k tokens for ALFWorld).
  - Distillation avoids this trade-off, achieving competitive results with fewer resources.

---

### **5. Ethical Considerations**
- The study acknowledges using **ChatGPT (GPT-5)** for drafting and editing, but emphasizes that **all outputs were reviewed, fact-checked, and substantially edited** by the authors. This raises questions about the role of closed-source models in research, though the authors assert ownership of the work.

---

### **Summary of Contributions**
1. **Distillation** is critical for improving both **performance** and **efficiency** of LLMs across tasks.
2. **k=3** is optimal for retrieval depth, balancing accuracy and resource usage.
3. **7B models** benefit more from distillation in terms of efficiency, while **14B models** see greater performance gains.
4. The study highlights the trade-offs between retrieval-augmented methods (RAG) and distilled models, advocating for distillation as a practical solution.

This work underscores the importance of distillation in deploying LLMs for real-world tasks, especially when computational resources are constrained.

===============

## 中文翻译

文档呈现了对大型语言模型（LLMs）进行任务特定蒸馏研究的成果，重点聚焦于**Qwen-2.5**模型（140亿参数和70亿参数）在**ALFWorld**和**WebShop**两个环境中的表现。关键发现和洞察包括：

---

### **1. 蒸馏在不同提示风格中的有效性**
- **140亿参数模型**：
  - **ReAct + 蒸馏**在**ALFWorld**中达到最高成功率（92.54%），并将**WebShop**得分提升至72.32。
  - **StateAct + 蒸馏**在**WebShop**中优于基线模型（得分72.49）。
- **70亿参数模型**：
  - 蒸馏显著提升了效率和性能：
    - **ReAct + 蒸馏**将**WebShop**得分从44.0提升至57.1。
    - **StateAct + 蒸馏**在**ALFWorld**中实现64.97%的成功率，在**WebShop**中达到65.0分，均优于基线模型。

**关键结论**：蒸馏在不同提示策略（ReAct、StateAct）中同时提升了**有效性**（任务性能）和**效率**（token/步骤使用）。

---

### **2. 蒸馏带来的效率提升**
- **70亿参数模型效率**（表8）：
  - 蒸馏模型比基线模型使用更少的**token**和**步骤**：
    - **WebShop**：蒸馏模型使用**5,950个token**（对比基线的9,360个）和**7.15步**（对比基线的7.89步）。
    - **ALFWorld**：蒸馏模型以**67,620个token**实现**68.66%的成功率**（对比基线的11.19%）。
  - 蒸馏减少了对检索（如RAG）的依赖，提升了效率而不牺牲性能。

---

### **3. 检索深度（k）消融实验**
- **最优检索深度**：
  - **k=3**在性能和效率间取得平衡：
    - **ALFWorld**：成功率82.09%，使用53,970个token。
    - **WebShop**：成功率43.50%（得分67.08），使用11,050个token。
  - 较大的k值（如6、9）导致**边际收益递减**：
    - token使用量增加（如WebShop中k=6时使用12,750个token），得分下降（≈61）。
    - k=9导致不稳定（成功率76.9%，使用57,260个token）。

**建议**：默认使用**k=3**。对于结构密集型任务（ALFWorld），使用**k=6**；对于噪声大、属性密集型任务（WebShop），使用**k=3**。

---

### **4. 模型特定洞察**
- **140亿参数 vs. 70亿参数**：
  - **140亿参数模型**在**ALFWorld**中受益更多（ReAct从77.6%提升至92.5%）。
  - **70亿参数模型**在效率提升方面更显著（如WebShop得分从44.0提升至57.1）。
- **RAG（检索增强生成）**：
  - 提升性能但以更高的token使用为代价（如ALFWorld使用62,740个token）。
  - 蒸馏避免了这一权衡，以更少资源实现竞争力结果。

---

### **5. 伦理考量**
- 该研究承认使用**ChatGPT（GPT-5）**进行初稿和编辑，但强调所有输出均由作者**审核、事实核查并大幅修改**。这引发了关于闭源模型在研究中角色的讨论，尽管作者声称对工作拥有所有权。

---

### **贡献总结**
1. **蒸馏**对提升LLMs在不同任务中的**性能**和**效率**至关重要。
2. **k=3**是检索深度的最优选择，平衡了准确性和资源使用。
3. **70亿参数模型**在效率方面更受益于蒸馏，而**140亿参数模型**在性能提升上更显著。
4. 该研究突出了检索增强方法（RAG）与蒸馏模型之间的权衡，倡导蒸馏作为实用解决方案。

本研究强调了蒸馏在受限计算资源下部署LLMs进行实际任务中的重要性。

#### Reference: 

Source file: 2510.01375v1.pdf

---

**Title**: Humaid Ibrahim, Nikolai Rozanov, Marek Rei

**Authors & Affiliations**: _{_ humaid.ibrahim24, nikolai.rozanov13, marek.rei _}_ @imperial.ac.uk
