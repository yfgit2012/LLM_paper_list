## Summary  
- **Objective**: To develop a reinforcement learning framework (FRPO) that balances safety constraints and downstream performance in large language models (LLMs), ensuring robustness against harmful behaviors while maintaining task-specific capabilities.  
- **Methodologies**:  
  - Integrates **KL divergence constraints** to regularize policy updates and ensure safety during fine-tuning.  
  - Uses **importance sampling** for log partition function estimation, mitigating gradient bias via the **jackknife technique** (reducing bias from $ O(1/G) $ to $ O(1/G^2) $).  
  - Balances **learning rates** and KL constraints to avoid overfitting while preserving performance.  
- **Results**:  
  - FRPO outperforms baselines (e.g., GRPO, Llama-3-TAR) in **safety robustness** (higher StrongREJECT scores) and **downstream performance** (better Alpaca and GSM8K results).  
  - Llama-3-TAR sacrifices general capabilities for safety, leading to significant drops in helpfulness scores.  
  - FRPO maintains strong performance on benchmarks like Alpaca and GSM8K, even after fine-tuning.  
- **Key Contributions**:  
  - Proposes **FRPO** as a novel framework combining **KL constraints** and **bias-reduction techniques** (jackknife) for safe RL.  
  - Demonstrates the critical role of **moderate learning rates** in balancing safety and performance.  
  - Introduces **StrongREJECT** and **helpfulness rewards** as key metrics for evaluating safety-performance trade-offs.  
- **Conclusions**: FRPO effectively balances safety and downstream performance in LLMs, offering practical applications in safety-critical domains. The work highlights the necessity of addressing gradient bias and KL regularization in safe reinforcement learning for large models.  

## Title and Authors (Required)  
**Title**: FRPO: Frank-Wolfe Policy Optimization for Safe Reinforcement Learning in Large Language Models  
**Authors**: [Authors not specified in the provided text]  
**Affiliations**: [Affiliations not specified in the provided text]

===============

## 中文翻译

## 摘要  
- **目标**：开发一种强化学习框架（FRPO），在大语言模型（LLMs）中平衡安全约束与下游性能，确保在对抗有害行为时的鲁棒性，同时保持任务特定能力。  
- **方法**：  
  - 集成**KL散度约束**以规范策略更新，在微调过程中确保安全性。  
  - 采用**重要性采样**估计对数分区函数，通过**jackknife技术**（将偏差从 $ O(1/G) $ 降低至 $ O(1/G^2) $）减轻梯度偏差。  
  - 平衡**学习率**与KL约束，避免过拟合同时保持性能。  
- **结果**：  
  - FRPO在**安全性鲁棒性**（更高的StrongREJECT分数）和**下游性能**（更优的Alpaca和GSM8K结果）上优于基线模型（如GRPO、Llama-3-TAR）。  
  - Llama-3-TAR为安全牺牲了通用能力，导致帮助性分数显著下降。  
  - FRPO在微调后仍能保持在Alpaca和GSM8K等基准测试中的强性能。  
- **关键贡献**：  
  - 提出**FRPO**作为结合**KL约束**和**偏差减少技术**（jackknife）的新型安全强化学习框架。  
  - 证明**适中学习率**在平衡安全与性能中的关键作用。  
  - 引入**StrongREJECT**和**帮助性奖励**作为评估安全-性能权衡的关键指标。  
- **结论**：FRPO有效平衡了LLMs中的安全性和下游性能，为安全关键领域提供了实际应用价值。该研究强调了在大型模型安全强化学习中解决梯度偏差和KL正则化的重要性。  

## 标题与作者（必填）  
**标题**：FRPO：面向大语言模型安全强化学习的Frank-Wolfe策略优化  
**作者**：[文本中未指定作者]  
**单位**：[文本中未指定单位]

#### Reference: 

Source file: 2602.08813v1.pdf

---

**Title**: Robust Policy Optimization to Prevent Catastrophic Forgetting
