The document discusses **FRPO (Frankenstein Regularized Policy Optimization)**, a method for improving the **safety and robustness** of large language models (LLMs) during fine-tuning while maintaining **general capabilities**. Below is a structured summary of the key points and contributions:

---

### **1. Key Concepts and Contributions**
#### **A. KL Constraint in Supervised Fine-Tuning (SFT)**
- **Implicit KL Constraint**: FRPO ensures the KL divergence between the fine-tuned model and the base model is implicitly constrained during SFT. This is achieved either via **LoRA (Low-Rank Adaptation)** or by using **moderate learning rates** and training durations.
- **Empirical Validation**: The paper verifies this through experiments (e.g., Figure 3), showing that FRPO maintains safety without sacrificing performance.

#### **B. Bias Reduction via Jackknife Technique**
- **Problem**: Estimators in FRPO (e.g., for policy gradients) have **bias** of order $ O(1/G) $, where $ G $ is the number of samples.
- **Solution**: The **jackknife technique** reduces this bias to $ O(1/G^2) $ by:
  - Removing one sample at a time and recalculating the estimator.
  - Using a linear combination of these leave-one-out estimates to cancel the leading bias term.
- **Impact**: This improves the **robustness** of policy optimization, especially for small $ G $ or heavy-tailed weights.

#### **C. FRPO vs. Baselines**
- **Safety vs. Performance Trade-off**: FRPO balances safety (e.g., higher refusal rates, lower harmfulness) with downstream task performance (e.g., SFT loss, helpfulness scores).
- **Comparisons**:
  - **Alpaca Experiments**: FRPO-trained models match GRPO and Llama-3.1 in SFT loss but outperform others in safety metrics (e.g., StrongREJECT score).
  - **GSM8K Experiments**: FRPO maintains general capabilities better than Llama-3-TAR, which loses safety after fine-tuning.
  - **Llama-3-RR**: This model "unlearns" unsafe responses, making it less comparable to FRPO's robust RLHF (Reinforcement Learning with Human Feedback) approach.

---

### **2. Technical Insights**
#### **A. High Learning Rates and Overfitting**
- **Risk**: High learning rates degrade **safety** (lower safety reward) and **general capabilities** (e.g., helpfulness scores drop).
- **Implication**: The KL constraint remains critical to prevent overfitting, even with high learning rates.

#### **B. Self-Normalized Importance Sampling (SNIS)**
- **Bias in Estimators**: SNIS-based gradient estimators have $ O(1/G) $ bias due to covariance terms in the Taylor expansion.
- **Jackknife as a Fix**: The jackknife technique mitigates this by eliminating the leading-order bias term.

---

### **3. Empirical Results**
#### **A. Alpaca Fine-Tuning**
- **SFT Loss**: FRPO aligns with GRPO and Llama-3.1 but maintains higher safety rewards.
- **Helpfulness Scores**: FRPO outperforms GRPO by the end of fine-tuning, suggesting safety is not an artifact of overfitting.

#### **B. GSM8K and HarmBench**
- **StrongREJECT Score**: Llama-3-TAR's score drops due to a collapse in helpfulness (to ~0.1), indicating poor adaptation.
- **FRPO's Advantage**: Maintains general capabilities and improves on GSM8K, outperforming Llama-3-RR after Alpaca fine-tuning.

---

### **4. Practical Implications**
- **Safety-Centric Training**: FRPO enables safer models without sacrificing performance, critical for real-world applications.
- **Avoiding Overfitting**: The KL constraint and jackknife technique ensure models generalize well while resisting adversarial adaptation.
- **Scalability**: The method is applicable to large-scale models (e.g., Qwen, Mistral) and datasets (e.g., Alpaca, GSM8K).

---

### **5. Limitations and Future Work**
- **High Learning Rates**: Require careful tuning to balance safety and capability.
- **Heavy-Tailed Weights**: The jackknife technique is most effective when weights are not heavy-tailed (i.e., for moderate $ \lambda $).
- **Broader Applications**: Extending FRPO to other domains or RLHF frameworks could further enhance safety.

---

### **Summary**
FRPO introduces a **KL-constrained policy optimization** framework with **bias-reduction via jackknife** to improve safety and robustness in LLMs. By balancing safety metrics (e.g., refusal rates, harmfulness) with downstream performance (e.g., SFT loss, helpfulness), it outperforms baselines like GRPO and Llama-3 variants. The method highlights the importance of **explicit safety constraints** and **bias mitigation** in reinforcement learning for real-world deployment.

===============

## 中文翻译

文档讨论了**FRPO（Frankenstein正则化策略优化）**，一种在微调过程中提升**大语言模型（LLMs）安全性和鲁棒性**的方法，同时保持**通用能力**。以下是关键点和贡献的结构化摘要：

---

### **1. 关键概念与贡献**
#### **A. 监督微调（SFT）中的KL约束**
- **隐式KL约束**：FRPO在SFT过程中隐式约束微调模型与基础模型之间的KL散度。通过**LoRA（低秩适配）**或使用**适中学习率和训练时长**实现。
- **实证验证**：论文通过实验（如图3）验证了这一点，表明FRPO在不牺牲性能的情况下保持了安全性。

#### **B. 通过Jackknife技术减少偏差**
- **问题**：FRPO中的估计器（如策略梯度）具有**O(1/G)**阶的偏差，其中G为样本数。
- **解决方案**：**Jackknife技术**通过以下方式将偏差降低至**O(1/G²)**：
  - 逐个移除样本并重新计算估计器。
  - 使用这些留一法估计值的线性组合来抵消主导偏差项。
- **影响**：这提高了策略优化的**鲁棒性**，尤其在小G或重尾权重的情况下。

#### **C. FRPO与基线对比**
- **安全性与性能的权衡**：FRPO在保持安全性（如更高的拒绝率、更低的危害性）的同时，平衡了下游任务性能（如SFT损失、有用性评分）。
- **对比实验**：
  - **Alpaca实验**：FRPO训练的模型在SFT损失上与GRPO和Llama-3.1相当，但在安全指标（如StrongREJECT得分）上优于其他模型。
  - **GSM8K实验**：FRPO在通用能力方面优于Llama-3-TAR，后者在微调后丧失了安全性。
  - **Llama-3-RR**：该模型“遗忘”了不安全响应，使其在鲁棒RLHF（人类反馈强化学习）方法上不如FRPO。

---

### **2. 技术洞察**
#### **A. 高学习率与过拟合**
- **风险**：高学习率会降低**安全性**（安全奖励降低）和**通用能力**（如有用性评分下降）。
- **影响**：即使在高学习率下，KL约束仍是防止过拟合的关键。

#### **B. 自归一化重要性采样（SNIS）**
- **估计器偏差**：基于SNIS的梯度估计器因泰勒展开中的协方差项，存在**O(1/G)**阶偏差。
- **Jackknife作为解决方案**：通过消除主导阶偏差项，Jackknife技术缓解了这一问题。

---

### **3. 实证结果**
#### **A. Alpaca微调**
- **SFT损失**：FRPO与GRPO和Llama-3.1一致，但保持更高的安全奖励。
- **有用性评分**：FRPO在微调结束时优于GRPO，表明安全性并非过拟合的产物。

#### **B. GSM8K和HarmBench**
- **StrongREJECT得分**：Llama-3-TAR的得分因有用性崩溃（降至约0.1）而下降，表明适应性差。
- **FRPO的优势**：在保持通用能力的同时改进GSM8K表现，微调后优于Llama-3-RR。

---

### **4. 实践意义**
- **以安全为中心的训练**：FRPO在不牺牲性能的前提下实现更安全的模型，这对实际应用至关重要。
- **避免过拟合**：KL约束和Jackknife技术确保模型在抵御对抗性适应的同时具备良好的泛化能力。
- **可扩展性**：该方法适用于大规模模型（如Qwen、Mistral）和数据集（如Alpaca、GSM8K）。

---

### **5. 局限性与未来工作**
- **高学习率**：需仔细调整以平衡安全性和能力。
- **重尾权重**：Jackknife技术在权重非重尾（即适中λ）时效果最佳。
- **更广泛的应用**：将FRPO扩展到其他领域或RLHF框架可能进一步提升安全性。

---

### **总结**
FRPO引入了**KL约束的策略优化框架**，结合**Jackknife技术的偏差减少**，以提升LLMs的安全性和鲁棒性。通过平衡安全性指标（如拒绝率、危害性）与下游性能（如SFT损失、有用性），FRPO在基线模型（如GRPO和Llama-3变体）上表现更优。该方法突显了在强化学习中**显式安全约束**和**偏差缓解**对实际部署的重要性。

#### Reference: 

Source file: 2602.08813v1.pdf

---

**Title**: Robust Policy Optimization to Prevent Catastrophic Forgetting
