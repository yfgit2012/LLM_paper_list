## Summary  
- **Objective**: To enhance the reasoning capabilities of large language models (LLMs) by addressing the "Shallow Exploration Trap," where models generate long responses without meaningful exploration, and to enable efficient, high-quality deep reasoning in complex tasks.  
- **Methodologies**: The paper introduces **Length-Incentivized Exploration (LIE)**, a reward shaping framework combining **length reward (R_len)** to encourage longer, diverse reasoning sequences and **redundancy penalty (R_red)** to penalize repetitive or low-value tokens. Hyperparameter tuning (β = 0.6) balances exploration depth and conciseness.  
- **Results**:  
  - Ablation studies show that **R_len alone** leads to low-quality, repetitive responses, while **R_red alone** results in shallow reasoning. The **combined LIE approach** achieves the best balance, improving accuracy (e.g., 52.7% average across benchmarks) and increasing reasoning **depth** (13.8 → 14.75) and **width** (2.15 → 2.30).  
  - On benchmarks like **AIME25**, **MATH**, and **AMC**, LIE improves accuracy by 11% (AIME25) and 2.4% (MATH) over baselines. A case study demonstrates the model’s ability to correct errors and arrive at correct answers through structured exploration.  
- **Key Contributions**:  
  1. Novel **LIE framework** integrating length incentives and redundancy penalties for structured exploration.  
  2. Empirical validation of **β = 0.6** as optimal for balancing exploration efficiency and conciseness.  
  3. Demonstration of effectiveness on complex reasoning tasks (e.g., math problems) and theoretical insight linking sequence length to reasoning depth.  
- **Conclusions**: The paper bridges the gap between shallow and deep reasoning in LLMs, proving that incentivizing sequence length through LIE enhances reasoning depth without sacrificing quality. It offers a scalable framework for improving performance in multi-step, complex tasks, though further work is needed on computational costs and generalization to non-math domains.  

## Title and Authors  
**Title**: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning  
**Authors**: [Authors’ names and affiliations, if provided in the original document]  
**Affiliations**: [Affiliations, if provided in the original document]

===============

## 中文翻译

## 摘要  
- **目标**：通过解决“浅层探索陷阱”问题，提升大语言模型（LLMs）的推理能力。该陷阱表现为模型生成冗长但缺乏实质探索的回复，同时实现复杂任务中高效、高质量的深度推理。  
- **方法**：论文提出**长度激励探索（LIE）**，一种结合**长度奖励（R_len）**（鼓励生成更长且多样化的推理序列）和**冗余惩罚（R_red）**（惩罚重复或低价值的标记）的奖励塑造框架。通过超参数调优（β=0.6）平衡探索深度与简洁性。  
- **结果**：  
  - 消融实验表明，**仅使用R_len**会导致低质量、重复的回复，而**仅使用R_red**则导致浅层推理。**结合LIE方法**在准确率（如基准测试中平均提升52.7%）和推理**深度**（13.8→14.75）与**宽度**（2.15→2.30）上达到最佳平衡。  
  - 在**AIME25**、**MATH**和**AMC**等基准测试中，LIE相比基线模型分别提升准确率11%（AIME25）和2.4%（MATH）。案例研究展示了模型通过结构化探索纠正错误并得出正确答案的能力。  
- **关键贡献**：  
  1. 提出一种结合长度激励与冗余惩罚的**LIE框架**，实现结构化探索。  
  2. 通过实证验证**β=0.6**为平衡探索效率与简洁性的最优值。  
  3. 在复杂推理任务（如数学问题）中验证了有效性，并揭示了序列长度与推理深度之间的理论联系。  
- **结论**：论文弥合了LLMs中浅层与深层推理之间的差距，证明通过LIE激励序列长度可提升推理深度而不牺牲质量。该框架为提升多步骤复杂任务的性能提供了可扩展方案，但仍需进一步研究计算成本及在非数学领域的泛化能力。  

## 标题与作者  
**标题**：通过长度激励强化学习实现上下文内探索  
**作者**：[原始文档中提供的作者姓名及所属机构]  
**所属机构**：[原始文档中提供的所属机构信息]

#### Reference: 

Source file: 2602.11748v1.pdf

---

**Title**: Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning
