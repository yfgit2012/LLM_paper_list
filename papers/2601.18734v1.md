## Summary
- **Objective**: To evaluate and compare the effectiveness of three training approaches—Supervised Fine-Tuning (SFT), GRPO (Gradient Regularized Policy Optimization), and OPSD (On-Policy Self-Distillation)—for training Large Language Models (LLMs), with a focus on balancing efficiency, performance, and robustness for downstream tasks.  
- **Methodologies**:  
  - **SFT**: Supervised fine-tuning on a 30k-sample dataset using LoRA for parameter updates.  
  - **GRPO**: Reinforcement learning with policy optimization, employing LoRA and partial logit distillation, with no KL divergence penalty.  
  - **OPSD**: On-policy self-distillation using full-vocabulary logit distillation, shorter sequence lengths (2048 tokens), and fewer generations per prompt.  
  - **Shared Techniques**: All methods use AdamW optimizer, gradient checkpointing, Flash Attention 2, and LoRA for efficiency.  
- **Results**:  
  - SFT is simpler and faster but lacks robustness for complex tasks.  
  - GRPO and OPSD (RL-based methods) improve performance for tasks requiring reasoning, with OPSD emphasizing robustness via full-vocabulary distillation.  
  - OPSD’s shorter sequence length restricts long-form generation but enhances training efficiency.  
  - GRPO avoids KL divergence penalties (β=0.0), prioritizing policy updates.  
- **Key Contributions**:  
  - Introduce GRPO and OPSD as novel RL-based training frameworks for LLMs.  
  - Demonstrate the effectiveness of LoRA for efficient parameter adaptation.  
  - Highlight the trade-offs between sequence length, logit distillation, and KL divergence penalties in RL training.  
  - Provide hyperparameter configurations and practical insights for replicating experiments.  
- **Conclusions**:  
  - SFT serves as a baseline for supervised tasks, while GRPO and OPSD excel in reinforcement learning scenarios requiring reasoning and alignment.  
  - OPSD’s focus on full-vocabulary distillation and shorter sequences offers a balance between robustness and computational efficiency.  
  - The study underscores the importance of hyperparameter tuning (e.g., LoRA rank/alpha, sequence length) and method selection for specific application goals.  

## Title and Authors (Required)  
**Title**: *Efficient and Effective Training of Large Language Models: A Comparative Study of SFT, GRPO, and OPSD*  
**Authors**: [Authors not provided in the extracted content]  
**Affiliations**: [Affiliations not provided in the extracted content]

===============

## 中文翻译

## 摘要
- **目标**：评估和比较三种训练方法——监督微调（SFT）、梯度正则化策略优化（GRPO）和基于策略的自蒸馏（OPSD）在训练大型语言模型（LLMs）中的有效性，重点在于平衡效率、性能和下游任务的鲁棒性。  
- **方法**：  
  - **SFT**：使用LoRA进行参数更新，在30,000个样本数据集上进行监督微调。  
  - **GRPO**：采用强化学习与策略优化，结合LoRA和部分logit蒸馏，不使用KL散度惩罚。  
  - **OPSD**：使用完整词汇表logit蒸馏的基于策略的自蒸馏，采用较短的序列长度（2048个标记）和每提示更少的生成次数。  
  - **共用技术**：所有方法均使用AdamW优化器、梯度检查点、Flash Attention 2和LoRA以提高效率。  
- **结果**：  
  - SFT简单且快速，但在复杂任务中缺乏鲁棒性。  
  - GRPO和OPSD（基于强化学习的方法）在需要推理的任务中提升了性能，其中OPSD通过完整词汇表蒸馏强调鲁棒性。  
  - OPSD的较短序列长度限制了长文本生成，但提高了训练效率。  
  - GRPO避免了KL散度惩罚（β=0.0），优先进行策略更新。  
- **关键贡献**：  
  - 引入GRPO和OPSD作为新颖的基于强化学习的LLMs训练框架。  
  - 证明了LoRA在高效参数适应中的有效性。  
  - 强调了强化学习训练中序列长度、logit蒸馏和KL散度惩罚之间的权衡。  
  - 提供了超参数配置和实用见解，以复现实验。  
- **结论**：  
  - SFT作为监督任务的基准，而GRPO和OPSD在需要推理和对齐的强化学习场景中表现更优。  
  - OPSD对完整词汇表蒸馏和较短序列的侧重，在鲁棒性与计算效率之间实现了平衡。  
  - 该研究强调了超参数调优（如LoRA秩/α、序列长度）和方法选择对特定应用目标的重要性。  

## 标题和作者（必填）  
**标题**：*大型语言模型的高效有效训练：SFT、GRPO和OPSD的比较研究*  
**作者**：[提取内容中未提供作者信息]  
**所属机构**：[提取内容中未提供所属机构信息]

#### Reference: 

Source file: 2601.18734v1.pdf

---

**Title**: Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models
