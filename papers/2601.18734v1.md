The provided document outlines the **experimental setup and hyperparameters** for training and evaluating large language models using three methodologies: **Supervised Fine-tuning (SFT)**, **GRPO (Gradient-based Reinforcement Policy Optimization)**, and **OPSD (On-Policy Self-Distillation)**. Below is a structured summary of the key details and insights:

---

### **1. Experimental Overview**
- **Objective**: Compare the performance of SFT, GRPO, and OPSD for training large language models, focusing on efficiency, scalability, and alignment with specific tasks (e.g., reasoning, generation).
- **Hardware**: Experiments were conducted on **8 A100 GPUs** with optimizations like **gradient checkpointing** and **Flash Attention 2** to reduce memory usage.

---

### **2. Training Configurations**
#### **Table 4: SFT (Supervised Fine-tuning)**
- **Learning Rate**: `2 × 10⁻⁵`
- **Batch Size (per device)**: 2
- **Gradient Accumulation Steps**: 4 → **Effective Batch Size**: 64
- **LoRA Settings**:
  - Rank (`r`): 64
  - Alpha (`α`): 128
  - Target Modules: Attention layers (`q_proj`, `k_proj`, `v_proj`, `o_proj`), gate and up/down projections.
- **Sequence Length**: Max 16,000 tokens
- **Training Dataset Size**: 30k examples

#### **Table 6: GRPO & OPSD**
- **Shared Hyperparameters**:
  - Learning Rate: `2 × 10⁻⁵`
  - Batch Size (per device): 1
  - Gradient Accumulation Steps: 4 → **Effective Batch Size**: 32
  - LoRA Settings: Same as SFT
- **Differences**:
  - **GRPO**:
    - Max Completion Length: 16,000 tokens
    - Generations per Prompt: 8
    - Temperature: 1.2
    - KL Coefficient (β): 0.0 (no KL divergence constraint)
  - **OPSD**:
    - Max Completion Length: 2048 tokens (shorter sequences)
    - Generations per Prompt: 1 (focused on single outputs)
    - Full-vocabulary logit distillation (used by default for OPSD)

---

### **3. Evaluation Parameters (Table 5)**
- **Max New Tokens**: 38,912 (long generation capacity)
- **Thinking Mode**: Enabled (likely for reasoning tasks)
- **Sampling Strategy**:
  - **Top-p**: 0.95 (diverse sampling)
  - **Top-k**: -1 (no restriction)
  - **Min-p**: 0.0 (no minimum probability threshold)
- **Penalties**:
  - **Presence Penalty**: 0.0 (no repetition penalty)
- **Samples per Prompt**: 16 (for diversity in outputs)

---

### **4. Key Technical Details**
- **Optimization**:
  - **AdamW Optimizer** (Loshchilov & Hutter, 2017) with **bfloat16 precision**.
- **LoRA (Low-Rank Adaptation)**:
  - Used to fine-tune the model efficiently by modifying only specific layers (attention modules).
  - Reduces memory and computational overhead while maintaining performance.
- **On-Policy Self-Distillation (OPSD)**:
  - Likely involves using the model's own generated outputs as a "teacher" during training, leveraging self-supervised learning.
  - Shorter sequence lengths (2048 tokens) suggest focus on efficiency and specific task alignment.
- **Reinforcement Learning (RL) Systems**:
  - References to **Dapo** and **Vapo** (open-source RL systems) imply integration of RL techniques for advanced reasoning tasks.

---

### **5. Comparison of Methods**
| Method   | Key Focus                      | Sequence Length | Generations per Prompt | Efficiency |
|----------|-------------------------------|------------------|--------------------------|------------|
| **SFT**  | Supervised fine-tuning         | 16,000 tokens    | N/A                      | Moderate   |
| **GRPO** | Reinforcement policy optimization | 16,000 tokens    | 8                        | High       |
| **OPSD** | On-policy self-distillation    | 2048 tokens      | 1                        | High       |

- **GRPO** is optimized for long sequences and diverse outputs, while **OPSD** prioritizes efficiency and focused generation.
- **SFT** serves as the baseline for supervised training.

---

### **6. Implications**
- **OPSD** and **GRPO** demonstrate the trade-off between **sequence length** and **efficiency**. OPSD’s shorter sequences may enable faster training and inference, while GRPO’s longer sequences cater to complex reasoning tasks.
- The use of **LoRA** and **Flash Attention 2** highlights the importance of **memory-efficient training** for large models.
- The **full-vocabulary logit distillation** in OPSD suggests a focus on preserving the model’s ability to generate diverse outputs while minimizing resource usage.

---

### **7. Future Directions**
- Explore hybrid approaches combining SFT, GRPO, and OPSD for different stages of model training.
- Investigate the impact of **thinking mode** and **sampling strategies** on reasoning accuracy.
- Scale experiments to larger models or distributed setups for real-world applications.

Let me know if you’d like further analysis on specific aspects (e.g., hyperparameter tuning, RL integration, or LoRA implementation)!

===============

## 中文翻译

  
**实验设置与超参数**  
本文档概述了使用三种方法（**监督微调（SFT）**、**基于梯度的强化策略优化（GRPO）**、**基于策略的自我蒸馏（OPSD）**）训练和评估大语言模型的实验设置与超参数。以下是关键细节和洞察的结构化总结：  

---

### **1. 实验概述**  
- **目标**：比较 SFT、GRPO 和 OPSD 在训练大语言模型中的性能，重点关注效率、可扩展性以及与特定任务（如推理、生成）的对齐。  
- **硬件**：实验在 **8 块 A100 GPU** 上进行，采用 **梯度检查点** 和 **Flash Attention 2** 等优化技术以减少内存使用。  

---

### **2. 训练配置**  
#### **表 4：SFT（监督微调）**  
- **学习率**：`2 × 10⁻⁵`  
- **每设备批量大小**：2  
- **梯度累积步数**：4 → **有效批量大小**：64  
- **LoRA 设置**：  
  - 秩 (`r`)：64  
  - Alpha (`α`)：128  
  - 目标模块：注意力层（`q_proj`、`k_proj`、`v_proj`、`o_proj`）、门控和上下文投影。  
- **序列长度**：最大 16,000 个 token  
- **训练数据集大小**：30k 示例  

#### **表 6：GRPO 与 OPSD**  
- **共享超参数**：  
  - 学习率：`2 × 10⁻⁵`  
  - 每设备批量大小：1  
  - 梯度累积步数：4 → **有效批量大小**：32  
  - LoRA 设置：与 SFT 相同  
- **差异**：  
  - **GRPO**：  
    - 最大生成长度：16,000 个 token  
    - 每提示生成次数：8  
    - 温度：1.2  
    - KL 系数 (β)：0.0（无 KL 散度约束）  
  - **OPSD**：  
    - 最大生成长度：2048 个 token（较短序列）  
    - 每提示生成次数：1（专注于单个输出）  
    - 全词表 logits 蒸馏（默认用于 OPSD）  

---

### **3. 评估参数（表 5）**  
- **最大新 token 数**：38,912（长生成能力）  
- **思考模式**：启用（可能用于推理任务）  
- **采样策略**：  
  - **Top-p**：0.95（多样化采样）  
  - **Top-k**：-1（无限制）  
  - **Min-p**：0.0（无最小概率阈值）  
- **惩罚项**：  
  - **存在惩罚**：0.0（无重复惩罚）  
- **每提示样本数**：16（输出多样性）  

---

### **4. 关键技术细节**  
- **优化**：  
  - 使用 **AdamW 优化器**（Loshchilov & Hutter, 2017）和 **bfloat16 精度**。  
- **LoRA（低秩适应）**：  
  - 通过仅修改特定层（注意力模块）高效微调模型。  
  - 减少内存和计算开销，同时保持性能。  
- **基于策略的自我蒸馏（OPSD）**：  
  - 可能利用模型自身生成的输出作为“教师”进行训练，采用自监督学习。  
  - 较短的序列长度（2048 个 token）表明注重效率和任务对齐。  
- **强化学习（RL）系统**：  
  - 提及 **Dapo** 和 **Vapo**（开源 RL 系统），表明集成 RL 技术以应对复杂推理任务。  

---

### **5. 方法对比**  
| 方法   | 核心关注点               | 序列长度     | 每提示生成次数 | 效率   |  
|------|------------------------|-------------|-------------|--------|  
| **SFT** | 监督微调               | 16,000 token | 无           | 中等   |  
| **GRPO** | 基于梯度的强化策略优化 | 16,000 token | 8           | 高     |  
| **OPSD** | 基于策略的自我蒸馏     | 2048 token  | 1           | 高     |  

- **GRPO** 优化长序列和多样化输出，**OPSD** 优先效率和聚焦生成。  
- **SFT** 作为监督训练的基线。  

---

### **6. 启示**  
- **OPSD** 和 **GRPO** 体现了 **序列长度** 与 **效率** 的权衡。OPSD 的短序列可能实现更快的训练和推理，而 GRPO 的长序列适用于复杂推理任务。  
- **LoRA** 和 **Flash Attention 2** 的应用突显了 **内存高效训练** 对大模型的重要性。  
- **OPSD** 中的全词表 logits 蒸馏表明，模型更注重在减少资源使用的同时保留多样化输出能力。  

---

### **7. 未来方向**  
- 探索结合 SFT、GRPO 和 OPSD 的混合方法，用于模型训练的不同阶段。  
- 研究 **思考模式** 和 **采样策略** 对推理准确率的影响。  
- 将实验扩展到更大模型或分布式设置，以支持实际应用。  

如需进一步分析特定方面（如超参数调优、RL 集成或 LoRA 实现），请告知！  


#### Reference: 

Source file: 2601.18734v1.pdf

---

**Title**: Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models
