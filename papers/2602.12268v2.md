The provided text appears to be an excerpt from a research paper titled **"CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use"**. Below is a structured summary of its key components and potential next steps for analysis or implementation:

---

### **1. Core Contributions**
- **CM2 Framework**: A reinforcement learning (RL) approach for training agents to perform multi-turn, multi-step tasks using **checklist rewards**. This ensures structured, goal-oriented interactions with tools.
- **Checklist Labeling**: A high-quality labeling system using **GPT-5** with progressive filtering (low → medium → high effort) to ensure data quality while minimizing costs.
- **Rule-Based Filtering**: Seven criteria to validate tool interactions (e.g., schema compliance, role ordering, JSON formatting, etc.).
- **Cold-Start Training**: Uses **LLaMAFactory** framework with custom token embeddings for special tokens (e.g., `</think>`, `<tool_call>`) to handle untrained tokens during pre-training.

---

### **2. Key Technical Details**
#### **A. Cold-Start Training**
- **Framework**: LLaMAFactory.
- **Hyperparameters**:
  - 2 epochs on a cold-start dataset.
  - AdamW optimizer with cosine learning rate schedule (`lr=1e-6`, warmup ratio `0.1`).
  - Batch size: `64`.
  - Special token initialization:
    - `</think>` = avg("think", "finish")
    - `<tool_call>` = avg("tool", "call", "start")
    - `<im_start>` = avg("role", "enter")
    - `<im_end>` = avg("role", "exit")
- **Training Loss**: Visualized in Figure **??** (not included here).

#### **B. Reinforcement Learning (RL)**
- **Algorithm**: GRPO (Generalized Reinforce with Policy Optimization) with:
  - Mini-batch size: `128`.
  - Learning rate: `3e-6`.
  - KL divergence coefficient: `0.001`.
  - Group size: `48` trajectories per question.
  - Trajectory-level rewards for the final **CM2** model.

#### **C. Filtering Criteria**
- **Rule-Based Checks**:
  1. Tool schema violations.
  2. Incorrect role ordering (e.g., assistant → user → assistant).
  3. Mismatch between tool calls and responses.
  4. Tool responses placed in assistant messages.
  5. Invalid JSON formatting.
  6. Duplicate tool schemas/names.
  7. Missing/redundant thinking tags (`</think>`).

---

### **3. Prompts and Pipelines**
- **Checklist Labeling**: Uses GPT-5 with high-effort labeling to ensure quality.
- **LLM-Based Filtering**: Aggressive early-exit strategy (evaluates samples at low, medium, high effort).
- **CoT Compression**: Simplifies reasoning steps (Chain-of-Thought) using GPT-5's default settings.
- **LLM Judge**: Evaluates output quality (Prompt .4, not fully detailed here).

---

### **4. Potential Applications**
- **Multi-Agent Systems**: Training agents to collaborate on complex tasks (e.g., customer service, data analysis).
- **Tool Integration**: Enabling LLMs to interact with APIs, databases, or other external tools in structured workflows.
- **Dialogue Systems**: Ensuring multi-turn conversations follow logical, goal-oriented paths.

---

### **5. Next Steps for Analysis/Implementation**
1. **Code Implementation**:
   - Replicate the cold-start training with LLaMAFactory.
   - Implement GRPO for RL training, ensuring trajectory grouping and reward shaping.
   - Use GPT-5 for labeling and filtering, following the described progressive evaluation strategy.

2. **Special Token Handling**:
   - Experiment with initializing special tokens (e.g., `<tool_call>`, `

===============

## 中文翻译

  
**CM2：基于清单奖励的多轮多步骤代理工具使用强化学习**  
以下是该研究论文的关键组成部分及后续分析或实施的潜在步骤总结：  

---

### **1. 核心贡献**  
- **CM2框架**：一种强化学习（RL）方法，用于训练代理完成多轮、多步骤任务，通过**清单奖励**确保结构化、目标导向的工具交互。  
- **清单标注**：使用**GPT-5**进行高质量标注，采用渐进式过滤（低→中→高成本）以确保数据质量并降低成本。  
- **基于规则的过滤**：七项标准验证工具交互（如模式合规性、角色顺序、JSON格式等）。  
- **冷启动训练**：采用**LLaMAFactory**框架，结合自定义特殊标记嵌入（如`<tool_call>`、`

#### Reference: 

Source file: 2602.12268v2.pdf

---

**Title**: !(paper_images/2602.12268v2.pdf-0-1.png)
