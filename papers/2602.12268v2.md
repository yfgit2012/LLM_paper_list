## Summary
- **Objective**: To develop a reinforcement learning (RL) framework, CM2, for enabling multi-turn, multi-step agentic tool use through checklist-based rewards, ensuring robust, scalable, and high-quality reasoning in complex environments.  
- **Methodologies**: Combines **checklist-based rewards** (using GPT-5 for labeling), **LLM-based filtering** (progressive evaluation with GPT-5), **cold-start training** (LLaMAFactory with AdamW optimizer), and **RL training** (GRPO algorithm with trajectory-level rewards). Integrates rule-based filtering and special token initialization for stability.  
- **Results**: Enables complex, multi-step reasoning with tools (e.g., API calls) through structured rewards and validation. Achieves cost-effective scaling (~$0.1 per trajectory) and high-quality outputs via LLM filtering and rule checks. Demonstrates scalability for large-scale training.  
- **Key Contributions**:  
  1. **Checklist rewards** for guiding multi-step reasoning and tool invocation.  
  2. **LLM-based filtering** with progressive evaluation for sample quality.  
  3. **Cold-start training** with specialized token initialization.  
  4. **RL training** using GRPO with trajectory-level rewards and KL divergence loss.  
  5. **Rule-based validation** for tool schema compliance and JSON formatting.  
- **Conclusions**: CM2 addresses challenges in training agentic models by integrating checklist rewards, LLM filtering, and rule validation, enabling robust, scalable, and efficient tool use in complex tasks.  

## Title and Authors (Required)  
**Title**: CM2: A Reinforcement Learning Framework for Multi-Turn, Multi-Step Agentic Tool Use with Checklist-Based Rewards  
**Authors**: [Authors]  
**Affiliations**: [Affiliations]

===============

## 中文翻译

## 摘要
- **目标**：开发一种强化学习（RL）框架CM2，通过基于清单的奖励机制实现多轮、多步骤的代理工具使用，确保在复杂环境中具备稳健、可扩展且高质量的推理能力。  
- **方法**：结合**基于清单的奖励机制**（使用GPT-5进行标签标注）、**基于大语言模型（LLM）的过滤**（使用GPT-5进行渐进式评估）、**冷启动训练**（使用LLaMAFactory与AdamW优化器）以及**强化学习训练**（使用GRPO算法与轨迹级奖励）。集成基于规则的过滤和特殊标记初始化以提升稳定性。  
- **结果**：通过结构化奖励和验证机制，实现复杂多步骤工具推理（如API调用）。通过LLM过滤和规则检查，实现成本效益的扩展（约每轨迹0.1美元）和高质量输出。展示了大规模训练的可扩展性。  
- **关键贡献**：  
  1. **基于清单的奖励机制**，用于指导多步骤推理和工具调用。  
  2. **基于LLM的过滤**，结合渐进式评估以提升样本质量。  
  3. **冷启动训练**，结合专用标记初始化。  
  4. **基于GRPO的强化学习训练**，使用轨迹级奖励和KL散度损失。  
  5. **基于规则的验证**，用于工具模式合规性检查和JSON格式校验。  
- **结论**：CM2通过整合基于清单的奖励、LLM过滤和规则验证，解决了代理模型训练中的挑战，实现了复杂任务中稳健、可扩展且高效的工具使用。  

## 标题与作者（必填）  
**标题**：CM2：一种基于清单奖励的多轮、多步骤代理工具使用强化学习框架  
**作者**：[作者]  
**单位**：[单位]

#### Reference: 

Source file: 2602.12268v2.pdf

---

**Title**: 1. Introduction
