## Summary
- **Objective**: To evaluate and enhance the reasoning capabilities of large language models (LLMs) in solving math problems, particularly in competitive exams like AIME and HMMT, by introducing novel metrics and methods to improve accuracy and structured reasoning.  
- **Methodologies**:  
  - **Deep-Thinking Ratio (DTR)**: A metric to quantify the proportion of tokens in model responses that reflect logical reasoning and problem-solving steps.  
  - **Think@_n_ Method**: A technique where models generate *n* responses to a problem, with the best ones selected via voting based on the top-ğœ‚ percentage of responses.  
  - **Datasets**: Evaluated on AIME 2024/2025, HMMT 2025, and GPQA.  
  - **Prompts**: Customized to emphasize step-by-step reasoning for math tasks.  
- **Results**:  
  - Correct answers are more concise (e.g., 3,725 tokens) with higher DTR (19.0) compared to verbose, incorrect outputs (27,724 tokens, DTR 13.9).  
  - **Think@_n_ outperforms self-consistency (Cons@_n_)** as *n* increases, with optimal performance at ğœ‚ = 50% and larger *n* (e.g., n=48).  
  - Incorrect outputs are verbose, repetitive, and lack structured reasoning, while correct outputs are logically structured.  
- **Key Contributions**:  
  - Introduces **DTR** as a novel metric for assessing reasoning depth in LLMs.  
  - Proposes **Think@_n_** as an effective method to enhance reasoning quality by prioritizing high-quality responses.  
  - Demonstrates the utility of concise, structured reasoning for improving accuracy in complex tasks.  
- **Conclusions**:  
  - Concise, structured reasoning (high DTR) is critical for accurate math problem-solving.  
  - **Think@_n_** with optimized parameters (ğœ‚ = 50%, larger *n*) balances quality and robustness, improving performance.  
  - The framework can be applied to other reasoning tasks (e.g., coding, logic puzzles) but requires model-specific calibration and task adjustments.  

## Title and Authors (Required)  
**Title**: Enhancing Reasoning Capabilities of Large Language Models in Math Problem-Solving: Metrics and Methods  
**Authors**: [Authorsâ€™ Names]  
**Affiliations**: [Institutions]

===============

## ä¸­æ–‡ç¿»è¯‘

## æ‘˜è¦
- **ç›®æ ‡**ï¼šé€šè¿‡å¼•å…¥æ–°é¢–çš„æŒ‡æ ‡å’Œæ–¹æ³•ï¼Œè¯„ä¼°å¹¶æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³æ•°å­¦é—®é¢˜ï¼ˆå°¤å…¶æ˜¯AIMEå’ŒHMMTç­‰ç«èµ›è€ƒè¯•ï¼‰ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œä»¥æé«˜å‡†ç¡®æ€§å’Œç»“æ„åŒ–æ¨ç†ã€‚  
- **æ–¹æ³•**ï¼š  
  - **æ·±åº¦æ€è€ƒæ¯”ä¾‹ï¼ˆDTRï¼‰**ï¼šç”¨äºé‡åŒ–æ¨¡å‹å“åº”ä¸­ä½“ç°é€»è¾‘æ¨ç†å’Œè§£é¢˜æ­¥éª¤çš„æ ‡è®°æ¯”ä¾‹ã€‚  
  - **Think@_n_ æ–¹æ³•**ï¼šä¸€ç§æŠ€æœ¯ï¼Œæ¨¡å‹ç”Ÿæˆ *n* ä¸ªé—®é¢˜å“åº”ï¼Œé€šè¿‡é€‰å–å‰ ğœ‚ ç™¾åˆ†æ¯”çš„å“åº”è¿›è¡ŒæŠ•ç¥¨é€‰å‡ºæœ€ä½³ç­”æ¡ˆã€‚  
  - **æ•°æ®é›†**ï¼šåœ¨AIME 2024/2025ã€HMMT 2025å’ŒGPQAæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚  
  - **æç¤º**ï¼šå®šåˆ¶åŒ–æç¤ºä»¥å¼ºè°ƒæ•°å­¦ä»»åŠ¡çš„åˆ†æ­¥æ¨ç†ã€‚  
- **ç»“æœ**ï¼š  
  - æ­£ç¡®ç­”æ¡ˆæ›´ç®€æ´ï¼ˆä¾‹å¦‚3,725ä¸ªæ ‡è®°ï¼‰ï¼Œä¸”DTRæ›´é«˜ï¼ˆ19.0ï¼‰ï¼Œç›¸æ¯”å†—é•¿çš„é”™è¯¯è¾“å‡ºï¼ˆ27,724ä¸ªæ ‡è®°ï¼ŒDTR 13.9ï¼‰ã€‚  
  - **Think@_n_ åœ¨ *n* å¢å¤§æ—¶ä¼˜äºè‡ªæ´½æ€§ï¼ˆCons@_n_ï¼‰**ï¼Œåœ¨ ğœ‚ = 50% å’Œè¾ƒå¤§ *n*ï¼ˆä¾‹å¦‚n=48ï¼‰æ—¶è¡¨ç°æœ€ä½³ã€‚  
  - é”™è¯¯è¾“å‡ºå†—é•¿ã€é‡å¤ä¸”ç¼ºä¹ç»“æ„åŒ–æ¨ç†ï¼Œè€Œæ­£ç¡®è¾“å‡ºé€»è¾‘ç»“æ„æ¸…æ™°ã€‚  
- **å…³é”®è´¡çŒ®**ï¼š  
  - å¼•å…¥ **DTR** ä½œä¸ºè¯„ä¼°LLMsæ¨ç†æ·±åº¦çš„æ–°é¢–æŒ‡æ ‡ã€‚  
  - æå‡º **Think@_n_** ä½œä¸ºé€šè¿‡ä¼˜å…ˆé€‰æ‹©é«˜è´¨é‡å“åº”æå‡æ¨ç†è´¨é‡çš„æœ‰æ•ˆæ–¹æ³•ã€‚  
  - å±•ç¤ºäº†ç®€æ´ã€ç»“æ„åŒ–æ¨ç†åœ¨æå‡å¤æ‚ä»»åŠ¡å‡†ç¡®æ€§æ–¹é¢çš„å®ç”¨æ€§ã€‚  
- **ç»“è®º**ï¼š  
  - ç®€æ´ã€ç»“æ„åŒ–çš„æ¨ç†ï¼ˆé«˜DTRï¼‰å¯¹å‡†ç¡®è§£å†³æ•°å­¦é—®é¢˜è‡³å…³é‡è¦ã€‚  
  - **Think@_n_** åœ¨ä¼˜åŒ–å‚æ•°ï¼ˆğœ‚ = 50%ï¼Œè¾ƒå¤§ *n*ï¼‰ä¸‹å¹³è¡¡äº†è´¨é‡å’Œé²æ£’æ€§ï¼Œæå‡äº†æ€§èƒ½ã€‚  
  - è¯¥æ¡†æ¶å¯åº”ç”¨äºå…¶ä»–æ¨ç†ä»»åŠ¡ï¼ˆå¦‚ç¼–ç¨‹ã€é€»è¾‘è°œé¢˜ï¼‰ï¼Œä½†éœ€æ ¹æ®æ¨¡å‹å’Œä»»åŠ¡è¿›è¡Œç‰¹å®šæ ¡å‡†å’Œè°ƒæ•´ã€‚  

## æ ‡é¢˜å’Œä½œè€…ï¼ˆå¿…å¡«ï¼‰  
**æ ‡é¢˜**ï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦é—®é¢˜è§£å†³ä¸­çš„æ¨ç†èƒ½åŠ›ï¼šæŒ‡æ ‡ä¸æ–¹æ³•  
**ä½œè€…**ï¼š[ä½œè€…å§“å]  
**å•ä½**ï¼š[æ‰€å±æœºæ„]

#### Reference: 

Source file: 2602.13517v1.pdf

---

**Title**: Think Deep, Not Just Long: Measuring LLM Reasoning Effort via Deep-Thinking Tokens

**Authors & Affiliations**: **baselines. Leveraging this insight, we introduce Think@** _ğ‘›_ **, a test-time scaling strategy that prioritizes** **samples with high deep-thinking ratios. We demonstrate that Think@** _ğ‘›_ **matches or exceeds standard**
