## Summary
- **Objective**: To analyze how pretraining data mixtures (e.g., code proportion, educational vs. general web data) impact model performance across diverse tasks, focusing on accuracy, calibration, and domain adaptability.  
- **Methodologies**: Correlation analysis between code proportion and task performance metrics (e.g., HellaSwag, ARC-C), comparison of data sources (FineWeb-Edu, RefinedWeb, DCLM), and evaluation across model scales (240M vs. 1B). Post-hoc calibration techniques (e.g., temperature scaling) were also tested.  
- **Results**:  
  - Code proportion harms commonsense tasks (e.g., HellaSwag) but benefits science tasks (e.g., ARC-C). NLI tasks show mixed effects, with some (e.g., RTE) slightly improving and others (e.g., QNLI) declining.  
  - Educational filtering (FineWeb-Edu) outperforms general web data for science tasks but underperforms for commonsense/NLI tasks. Model-based filtering (DCLM) matches or exceeds FineWeb-Edu in most categories.  
  - Scale-dependent effects are significant: task correlations with code proportion shift between model sizes (e.g., MNLI transitions from positive to negative).  
  - NLI calibration remains poorly aligned post-SFT, necessitating task-specific calibration techniques.  
- **Key Contributions**:  
  - Task- and scale-dependent impacts of data mixtures on model performance.  
  - Demonstration that model-based filtering (e.g., DCLM) can rival educational filtering without domain restrictions.  
  - Emphasis on multi-scale validation and task-specific calibration strategies to balance accuracy and calibration.  
- **Conclusions**: Pretraining data design is complex and task-specific; code proportion and data sources significantly influence performance but require tailored strategies. Calibration interventions are critical for tasks with weak alignment, and multi-scale validation is essential to ensure generalization across model sizes.  

## Title and Authors (Required)  
**Title**: *Task- and Scale-Dependent Impacts of Pretraining Data Mixtures on Model Performance*  
**Authors**: [Authors not provided in the extracted key parts]  
**Affiliations**: [Affiliations not provided in the extracted key parts]

===============

## 中文翻译

## 摘要
- **目标**：分析预训练数据混合（如代码比例、教育类与通用网络数据）对模型在多样化任务中的性能影响，重点关注准确性、校准性和领域适应性。  
- **方法**：分析代码比例与任务性能指标（如HellaSwag、ARC-C）的相关性，比较数据来源（FineWeb-Edu、RefinedWeb、DCLM），并在不同模型规模（240M vs. 1B）下进行评估。还测试了后置校准技术（如温度缩放）。  
- **结果**：  
  - 代码比例对常识任务（如HellaSwag）产生负面影响，但对科学任务（如ARC-C）有益。NLI任务表现复杂，部分任务（如RTE）略有提升，而其他任务（如QNLI）则下降。  
  - 教育过滤（FineWeb-Edu）在科学任务中优于通用网络数据，但在常识/NLI任务中表现较差。基于模型的过滤（DCLM）在多数类别中与FineWeb-Edu相当或更优。  
  - 规模依赖效应显著：任务与代码比例的相关性在不同模型规模间发生变化（如MNLI从正相关转为负相关）。  
  - NLI任务的校准在SFT后仍存在明显偏差，需采用任务特定校准技术。  
- **关键贡献**：  
  - 数据混合对模型性能的任务与规模依赖性影响。  
  - 证明基于模型的过滤（如DCLM）可在无领域限制的情况下与教育过滤相媲美。  
  - 强调多尺度验证和任务特定校准策略，以平衡准确性和校准性。  
- **结论**：预训练数据设计复杂且任务特定；代码比例和数据来源显著影响性能，但需定制化策略。校准干预对对齐性较弱的任务至关重要，多尺度验证是确保模型规模间泛化能力的关键。  

## 标题和作者（必填）  
**标题**：*预训练数据混合对模型性能的任务与规模依赖性影响*  
**作者**：[提取的关键部分中未提供作者信息]  
**所属机构**：[提取的关键部分中未提供所属机构信息]

#### Reference: 

Source file: 2602.11217v1.pdf

---

**Title**: The : _Magic Correlations_ Understanding Knowledge Transfer from Pretraining to Supervised Fine-Tuning
