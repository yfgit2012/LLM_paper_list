**Summary of Key Findings from the Paper:**

### **1. Impact of Code Data Proportion on Model Performance**  
- **Mixed Effects Across Tasks:**  
  - **Negative Impact:** Code data (e.g., programming or technical content) harms performance on **Commonsense reasoning tasks** like *HellaSwag* (correlation: -0.94 at 240M, -0.80 at 1B) and *PIQA* (-0.77 at 1B). This suggests code data may degrade grounded physical reasoning, as it lacks descriptions of real-world interactions.  
  - **Positive Impact:** Code data benefits **abstract logical reasoning** tasks like *SIQA* (0.24 at 1B) and *ARC-C* (science reasoning), possibly due to its structured logical patterns.  
  - **Scale Dependency:** Effects vary with model size. For example, *MNLI* shifts from a weak positive correlation (0.12 at 240M) to a strong negative (-0.68 at 1B) as models grow larger.  

### **2. Data Source Comparisons**  
- **FineWeb-Edu (Educational Filtering):**  
  - Slightly better at **Science tasks** (55.1% accuracy vs. RefinedWeb’s 54.3%) due to its focus on educational content.  
  - Underperforms in **Commonsense** and **NLI** compared to general web data (RefinedWeb and DCLM).  
- **RefinedWeb (General Web):**  
  - Performs comparably to DCLM across most categories, suggesting general web data is effective for diverse tasks.  
- **DCLM (Model-Based Filtering):**  
  - Matches or exceeds other sources in most categories, indicating that quality filtering via models can rival educational filtering without domain restrictions.  

### **3. Category-Specific Insights**  
- **Commonsense Tasks:**  
  - **HellaSwag** and *PIQA* are most harmed by code data, while *SIQA* and *BoolQ* benefit.  
- **Science Tasks:**  
  - Code data slightly improves performance on *ARC-C* (science reasoning), but general web data remains competitive.  
- **NLI Tasks:**  
  - Code data harms *QNLI* and *MNLI* (-0.53 to -0.68 correlation), but helps *RTE* (0.25), possibly due to simpler binary classification.  
- **Semantic Tasks:**  
  - Code data has minimal impact, with all sources performing similarly (e.g., *QQP*, *MRPC*).  

### **4. Key Takeaways and Implications**  
- **Scale Matters:** Small-scale experiments may not generalize to larger models. For example, code data’s negative impact on *HellaSwag* is more pronounced in 1B models than 240M.  
- **Data Curation Trade-offs:** Educational filtering (FineWeb-Edu) benefits science tasks but risks harming commonsense reasoning. Model-based filtering (DCLM) offers a flexible alternative.  
- **Task-Specific Design:** Tailoring data sources to task types (e.g., prioritizing code for logic tasks, general web for commonsense) could optimize performance.  

### **Conclusion**  
The paper highlights the nuanced relationship between data composition and model performance. While code data can enhance abstract reasoning, it risks degrading tasks requiring real-world knowledge. Careful curation, considering task type and model scale, is critical for effective training.

===============

## 中文翻译

**论文关键发现摘要：**

### **1. 代码数据比例对模型性能的影响**  
- **任务间的混合影响：**  
  - **负面影响：** 代码数据（如编程或技术内容）会损害**常识推理任务**的表现，例如*HellaSwag*（相关性：-0.94在240M模型，-0.80在1B模型）和*PIQA*（-0.77在1B模型）。这表明代码数据可能降低基于现实物理推理的能力，因为它缺乏对现实世界交互的描述。  
  - **正面影响：** 代码数据对**抽象逻辑推理任务**有益，例如*SIQA*（0.24在1B模型）和*ARC-C*（科学推理），可能由于其结构化的逻辑模式。  
  - **规模依赖性：** 效果随模型规模变化。例如，*MNLI*任务从240M模型的弱正相关（0.12）转变为1B模型的强负相关（-0.68）。  

### **2. 数据来源比较**  
- **FineWeb-Edu（教育过滤）：**  
  - 在**科学任务**中表现略优（准确率55.1% vs. RefinedWeb的54.3%），因其专注于教育内容。  
  - 相比通用网络数据（RefinedWeb和DCLM），在**常识**和**自然语言推理（NLI）**任务中表现较差。  
- **RefinedWeb（通用网络）：**  
  - 在大多数类别中与DCLM表现相当，表明通用网络数据对多样化任务有效。  
- **DCLM（模型过滤）：**  
  - 在大多数类别中与其它来源表现相当或更优，表明通过模型进行质量过滤可媲美教育过滤，无需领域限制。  

### **3. 分类特定见解**  
- **常识任务：**  
  - *HellaSwag*和*PIQA*受代码数据负面影响最大，而*SIQA*和*BoolQ*受益。  
- **科学任务：**  
  - 代码数据略微提升*ARC-C*（科学推理）表现，但通用网络数据仍具竞争力。  
- **NLI任务：**  
  - 代码数据损害*QNLI*和*MNLI*（相关性-0.53至-0.68），但有助于*RTE*（0.25），可能因二分类更简单。  
- **语义任务：**  
  - 代码数据影响较小，各来源表现相似（如*QQP*、*MRPC*）。  

### **4. 关键结论与启示**  
- **规模重要性：** 小规模实验可能无法推广至大模型。例如，代码数据对*HellaSwag*的负面影响在1B模型中比240M更显著。  
- **数据筛选权衡：** 教育过滤（FineWeb-Edu）利于科学任务，但可能损害常识推理；模型过滤（DCLM）提供更灵活的替代方案。  
- **任务特定设计：** 根据任务类型（如优先使用代码数据处理逻辑任务，通用网络数据处理常识任务）优化数据来源可提升性能。  

### **结论**  
论文强调了数据组成与模型性能之间的微妙关系。尽管代码数据可增强抽象推理能力，但可能损害需要现实世界知识的任务。根据任务类型和模型规模进行细致筛选，是有效训练的关键。

#### Reference: 

Source file: 2602.11217v1.pdf

---

**Title**: The : _Magic Correlations_ Understanding Knowledge Transfer from Pretraining to Supervised Fine-Tuning
