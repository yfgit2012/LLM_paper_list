## Summary
- **Objective**: To determine whether more shared parameters or fewer unique parameters yield better performance in parameter-efficient fine-tuning and to compare models like Qwen and Llama on the GSM8K benchmark.  
- **Methodologies**: Ablation studies were conducted by varying rank values ($ r $) and tied layers for models (e.g., Qwen-2.5, LLAMA-3.2) on GSM8K. Model comparisons focused on accuracy and parameter efficiency, with Qwen and Llama evaluated under matched parameter counts.  
- **Results**: Lower rank values (fewer parameters) achieved higher performance when total parameters were matched, indicating fewer unique parameters outperform shared ones. Qwen outperformed Llama significantly (74% vs. 60% accuracy) on GSM8K with the same parameter count, highlighting Qwen's superior efficiency.  
- **Key Contributions**:  
  1. Challenges the conventional wisdom of maximizing shared parameters in low-rank adaptation (LoRA) methods.  
  2. Demonstrates that targeted parameter updates (fewer unique parameters) yield better performance than uniform rank settings.  
  3. Highlights Qwen's task-specific optimization and efficiency gains for resource-constrained deployments.  
- **Conclusions**: Prioritizing fewer, unique parameters over shared ones in parameter-efficient training improves performance. This suggests that methods like LoRA, BitFit, and Adalora should focus on targeted updates rather than uniform rank settings, enabling efficiency without sacrificing accuracy.  

## Title and Authors (Required)  
**Title**: *Parameter-Efficient Fine-Tuning and Ablation Studies on Qwen and Llama for Mathematical Reasoning Tasks*  
**Authors**: [Authors not specified in the extracted text]  
**Affiliations**: [Affiliations not specified in the extracted text]

===============

## 中文翻译

## 摘要
- **目标**：确定在参数高效微调中，使用更多共享参数还是更少的独特参数能获得更好的性能，并在GSM8K基准上比较Qwen和Llama等模型的表现。  
- **方法**：在GSM8K上对模型（如Qwen-2.5、LLAMA-3.2）进行消融实验，通过调整秩值（r）和绑定层进行分析。模型比较重点关注准确率和参数效率，并在相同参数量下评估Qwen和Llama的表现。  
- **结果**：当总参数量相同时，较低的秩值（更少的参数）实现了更高的性能，表明更少的独特参数优于共享参数。在相同参数量下，Qwen在GSM8K上的准确率（74%）显著高于Llama（60%），凸显了Qwen的高效性。  
- **关键贡献**：  
  1. 质疑了低秩适应（LoRA）方法中最大化共享参数的常规智慧。  
  2. 证明了定向参数更新（更少的独特参数）的性能优于统一秩设置。  
  3. 强调了Qwen在任务特定优化及资源受限部署环境中的效率优势。  
- **结论**：在参数高效训练中，优先使用更少的独特参数而非共享参数可提升性能。这表明LoRA、BitFit和Adalora等方法应侧重于定向更新而非统一秩设置，从而在不牺牲准确率的前提下实现效率提升。  

## 标题与作者（必填）  
**标题**：*面向数学推理任务的Qwen与LLAMA参数高效微调及消融研究*  
**作者**：[提取文本中未指定作者]  
**所属机构**：[提取文本中未指定所属机构]

#### Reference: 

Source file: 2602.04118v1.pdf

---

**Title**: 1 Introduction
