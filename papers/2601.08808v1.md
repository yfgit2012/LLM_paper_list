The provided text is an appendix from a research paper discussing **Multiplex Thinking**, a method that enhances reasoning by using multiple parallel reasoning paths (controlled by a parameter $ K $) compared to traditional discrete reasoning (where $ K = 1 $). Below is a structured summary of the key points and findings from the appendix:

---

### **Key Findings and Analysis**
1. **Multiplex Width ($ K $) and Performance**:
   - **Baseline vs. Multiplex Thinking**: 
     - Discrete RL (with $ K = 1 $) shows a significant performance gap compared to multiplex variants ($ K \geq 2 $), as observed in **Figure 8**. This validates the hypothesis that multiplex thinking overcomes limitations of depth-first search.
   - **Diminishing Returns**: 
     - Increasing $ K $ beyond a certain point (e.g., $ K = 3 $ vs. $ K = 6 $) yields diminishing returns, as the performance curves for different $ K $ values remain closely clustered. This suggests optimal $ K $ values are likely small (e.g., $ K = 2 $ or $ K = 3 $).

2. **Training Dynamics**:
   - **Validation Metrics**: 
     - **Figure 9** shows training and validation scores over steps. Multiplex thinking variants (e.g., $ K = 2, 3, 6 $) outperform Discrete RL, with validation scores stabilizing at higher levels. This indicates that multiplex methods achieve better generalization.
   - **Pass@k Metrics**: 
     - The paper evaluates models using **Pass@k**, where higher $ k $ values (e.g., $ k = 1024 $) require the model to generate more reasoning steps. Multiplex thinking consistently outperforms Discrete RL across all $ k $, as shown in **Figure 7**.

3. **Implementation Details**:
   - **Hardware/Software**: The experiments use **Verifiable AI (verifiable.ai)** and **Verifiable AI (verifiable.ai)** frameworks, with training on **80 GPUs** and **1024 GPUs** for large-scale tasks.
   - **Training Setup**: 
     - The model is validated every **25 training steps**, and the **Pass@4** metric is used for validation on the **MATH-500** dataset.

4. **Qualitative Trajectory Examples**:
   - **Figure 10** (not explicitly detailed in the text) visualizes full reasoning trajectories for Multiplex Thinking, illustrating how multiple paths are explored during on-policy training.

---

### **Implications**
- **Efficiency vs. Complexity**: Multiplex thinking balances exploration (via multiple paths) and efficiency, avoiding the pitfalls of exhaustive search while improving performance over discrete methods.
- **Scalability**: The diminishing returns for larger $ K $ suggest that the method is scalable but requires careful tuning of $ K $ for optimal results.

---

### **Final Answer**
The appendix demonstrates that **Multiplex Thinking** (with $ K \geq 2 $) outperforms traditional discrete reasoning ( $ K = 1 $ ) across all evaluated metrics (e.g., Pass@k scores, validation accuracy). While increasing $ K $ initially improves performance, diminishing returns occur for larger values, indicating an optimal range for $ K $. The method's ability to explore multiple reasoning paths enhances generalization and robustness, as shown in training dynamics and trajectory examples. 

$$
\boxed{\text{Multiplex Thinking (with } K \geq 2 \text{) outperforms Discrete RL, with diminishing returns for larger } K \text{.}}
$$

===============

## 中文翻译

### **关键发现与分析**
1. **多路径宽度（K）与性能**：
   - **基线对比多路径思维**：
     - 离散强化学习（K=1）在**图8**中显示出与多路径变体（K≥2）相比显著的性能差距，验证了多路径思维克服深度优先搜索局限性的假设。
   - **边际效益递减**：
     - 当K超过一定阈值（例如K=3与K=6）时，性能提升趋于平缓，不同K值的性能曲线紧密聚集，表明最优K值可能较小（如K=2或K=3）。

2. **训练动态**：
   - **验证指标**：
     - **图9**展示了训练和验证得分随步骤的变化。多路径思维变体（如K=2、3、6）优于离散强化学习，验证得分在更高水平稳定，表明多路径方法具有更好的泛化能力。
   - **Pass@k指标**：
     - 论文通过**Pass@k**评估模型，其中更高k值（如k=1024）要求模型生成更多推理步骤。多路径思维在所有k值下均优于离散强化学习，如**图7**所示。

3. **实现细节**：
   - **硬件/软件**：实验使用**Verifiable AI (verifiable.ai)**框架，大规模任务在**80块GPU**和**1024块GPU**上训练。
   - **训练设置**：
     - 模型每**25个训练步骤**验证一次，**MATH-500**数据集的验证使用**Pass@4**指标。

4. **定性轨迹示例**：
   - **图10**（文本未详细说明）可视化了多路径思维的完整推理轨迹，展示了多路径思维在基于策略训练过程中如何探索多个路径。

---

### **启示**
- **效率与复杂度的平衡**：多路径思维通过多路径探索平衡了探索与效率，避免了穷举搜索的弊端，同时优于离散方法的性能。
- **可扩展性**：K值增大时的边际效益递减表明该方法具有可扩展性，但需谨慎调整K值以达到最佳效果。

---

### **最终答案**
附录表明，**多路径思维**（K≥2）在所有评估指标（如Pass@k得分、验证准确率）中均优于传统离散推理（K=1）。尽管K值增加初期性能提升显著，但更大K值会带来边际效益递减，表明K值存在最佳范围。该方法通过探索多路径推理增强了泛化能力与鲁棒性，如训练动态和轨迹示例所示。

$$
\boxed{\text{多路径思维（K≥2）优于离散强化学习，且更大K值带来边际效益递减。}}
$$

#### Reference: 

Source file: 2601.08808v1.pdf

---

**Title**: Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge

**Authors & Affiliations**: 1 _{_ tangyao,jgu32 _}_ @seas.upenn.edu baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code [and checkpoints are available at github.com/GMLR-Penn/Multiplex-Thinking.](https://github.com/GMLR-Penn/Multiplex-Thinking)
