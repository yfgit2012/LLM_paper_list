## Summary
- **Objective**: To investigate the ability of large language models (LLMs) like GPT-4, Claude, and Gemini to detect AI-generated text, highlighting their limitations under deceptive conditions and implications for academic integrity and education.  
- **Methodologies**: The study evaluated models under standard and deceptive conditions using manually curated prompts and generated text samples. Classification accuracy, confidence scores, and qualitative reasoning were analyzed. Deceptive prompts mimicked human writing to test vulnerabilities. Raw data and outputs were publicly shared via a GitHub repository.  
- **Results**: LLMs struggled to consistently detect AI-generated text, particularly with deceptive prompts. Accuracy varied across models, with some failing to identify synthetic content. Confidence scores were often inaccurate, reflecting overreliance on superficial patterns. Adversarial prompts exposed model vulnerabilities, suggesting susceptibility to sophisticated attacks. Educational implications emphasize the need for assessments resistant to AI misuse.  
- **Key Contributions**: Highlights the inadequacy of current LLM detection capabilities, recommends assessment design prioritizing critical thinking and originality, and proposes hybrid detection systems combining stylistic analysis with forensic markers. Provides reproducible data for future research.  
- **Conclusions**: LLMs are unreliable for detecting AI-generated text under adversarial conditions. Educational institutions should focus on creating assessments that prioritize human creativity and critical analysis. Future research must address model robustness and explore advanced detection strategies.  

## Title and Authors  
**Title**: "Self-Detection of AI-Generated Text by Large Language Models: A Study on Detection Capabilities and Vulnerabilities"  
**Authors**: Christopher Burger (affiliation not specified), [other authors if listed]  
**Affiliations**: [affiliations if provided]

===============

## 中文翻译

## 摘要
- **目标**：探讨大型语言模型（LLM）如GPT-4、Claude和Gemini检测AI生成文本的能力，突出其在欺骗性条件下的局限性及其对学术诚信和教育的影响。  
- **方法**：研究通过手动整理的提示和生成文本样本，在标准和欺骗性条件下评估模型。分析了分类准确率、置信度分数和定性推理。欺骗性提示模仿人类写作以测试模型漏洞。原始数据和输出通过GitHub仓库公开分享。  
- **结果**：LLM在检测AI生成文本方面表现不稳定，尤其在面对欺骗性提示时。不同模型的准确率差异较大，部分模型无法识别合成内容。置信度分数常不准确，反映出对表面模式的过度依赖。对抗性提示暴露了模型的脆弱性，表明其可能易受复杂攻击。教育意义强调需设计能抵御AI滥用的评估体系。  
- **关键贡献**：凸显当前LLM检测能力的不足，建议优先设计注重批判性思维和原创性的评估体系，并提出结合文体分析与法医学标记的混合检测系统。提供可复现的数据供未来研究使用。  
- **结论**：在对抗性条件下，LLM无法可靠检测AI生成文本。教育机构应注重设计优先考察人类创造力和批判性分析的评估体系。未来研究需解决模型鲁棒性问题，并探索更先进的检测策略。  

## 标题与作者  
**标题**："大型语言模型对AI生成文本的自我检测：关于检测能力与漏洞的研究"  
**作者**：Christopher Burger（未注明所属机构），[如有其他作者则列出]  
**所属机构**：[如有提供则注明]

#### Reference: 

Source file: 2512.23587v1.pdf

---

**Title**: Can AI Recognize Its Own Reflection? Self-Detection Performance of LLMs in Computing Education

**Authors & Affiliations**: cburger@olemiss.edu karmecet1@gmail.com cjtrotte@olemiss.edu
