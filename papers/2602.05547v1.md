## Summary  
- **Objective**: To investigate task reweighting strategies in multi-task reinforcement learning (RL) using GRPO, comparing improvement-aware updates and regularized reward-only updates to optimize policy training across diverse tasks.  
- **Methodologies**: Two subroutines were evaluated:  
  1. **Subroutine 1 (Improvement-Aware)**: Uses task-specific progress signals to dynamically prioritize tasks requiring updates.  
  2. **Subroutine 2 (Regularized Reward-Only)**: Relies on absolute task rewards with regularization (e.g., uniformity constraints) to stabilize training. Experiments tested varying regularization strengths (η) on 3 and 9 tasks.  
- **Results**:  
  - **Subroutine 1** outperformed Subroutine 2 on hard tasks (e.g., ARC) and showed higher relative improvements, though it required careful tuning of the trade-off parameter (λ).  
  - **Regularization alone** (Subroutine 2) stabilized training but risked under-prioritizing slow-improving tasks, leading to degraded worst-task accuracy at high regularization (η = 1e-2).  
  - **Task-specific progress** was critical for improving performance, particularly for challenging tasks.  
- **Key Contributions**:  
  1. Introduced two task reweighting subroutines for multi-task RL, emphasizing the role of improvement signals.  
  2. Demonstrated that regularization alone is insufficient for balancing task priorities, while improvement-aware methods better address task heterogeneity.  
  3. Highlighted the trade-off between average accuracy and worst-task performance via the λ parameter.  
- **Conclusions**: Improvement-aware updates (Subroutine 1) are more effective for multi-task RL with varying difficulty levels, but regularization remains necessary for training stability. Balancing these approaches and monitoring task-specific progress is essential for optimal performance.  

## Title and Authors (Required)  
[Title: "GRPO: Gradient-Based Policy Optimization for Multi-Task Reinforcement Learning with Task Reweighting"]  
[Authors: [Name(s)] (Affiliation(s))]  

*Note: The paper title, authors, and affiliations are required but not provided in the extracted text. Please replace with the actual details from the original paper.*

===============

## 中文翻译

## 摘要  
- **目标**：利用GRPO研究多任务强化学习（RL）中的任务重加权策略，比较改进感知更新与仅奖励正则化更新，以优化跨多样任务的策略训练。  
- **方法**：评估了两种子程序：  
  1. **子程序1（改进感知）**：利用任务特定的进展信号动态优先更新需要调整的任务。  
  2. **子程序2（仅奖励正则化）**：依赖绝对任务奖励并结合正则化（如均匀性约束）以稳定训练。实验在3和9个任务上测试了不同正则化强度（η）的影响。  
- **结果**：  
  - **子程序1**在困难任务（如ARC）中优于子程序2，表现出更高的相对改进，但需要仔细调整权衡参数（λ）。  
  - **仅正则化**（子程序2）可稳定训练，但可能低估进展缓慢的任务，导致高正则化（η=1e-2）时最差任务准确率下降。  
  - **任务特定进展**对提升性能至关重要，尤其对挑战性任务。  
- **关键贡献**：  
  1. 引入了两种多任务RL的任务重加权子程序，强调改进信号的作用。  
  2. 证明仅正则化不足以平衡任务优先级，而改进感知方法更有效应对任务异质性。  
  3. 通过λ参数突出了平均准确率与最差任务性能之间的权衡。  
- **结论**：改进感知更新（子程序1）更适合多任务RL中不同难度的任务，但正则化仍需用于训练稳定性。平衡这两种方法并监控任务特定进展对实现最优性能至关重要。  

## 标题和作者（必填）  
[标题：GRPO：基于梯度的多任务强化学习策略优化与任务重加权]  
[作者：[姓名]（所属机构）]  

*注：论文标题、作者和所属机构为必填项，但未在提取文本中提供。请替换为原文中的实际信息。*

#### Reference: 

Source file: 2602.05547v1.pdf

---

**Title**: Xiaotong Ji** **Matthieu Zimmer
