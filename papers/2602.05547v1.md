The document discusses **Multi-Task GRPO** (Gradient Policy Optimization), a reinforcement learning method designed to handle multiple tasks simultaneously. Here's a structured summary of the key points and insights:

---

### **Core Algorithm (Pseudocode)**
- **Objective**: Optimize policies across multiple tasks by balancing gradients from all tasks.
- **Key Components**:
  - **Sampling**: Collect data from all tasks.
  - **Gradient Calculation**: Compute gradients for each task, possibly using a shared policy or task-specific parameters.
  - **Parameter Updates**: Adjust policy parameters to minimize combined gradients, with a focus on balancing task performance.

---

### **Experiments and Findings**
#### **1. Regularization vs. Improvement-Aware Reweighting**
- **Regularized Reward-Only Reweighting (Subroutine 2)**:
  - **Pros**: Prevents weight collapse by driving weights toward uniformity (average reward maximization).
  - **Cons**: Underprioritizes underperforming or slowly improving tasks, leading to suboptimal results on harder tasks.
  - **Example**: In Experiment 1 (3 tasks), regularization (η=1e-2) improved average accuracy but failed to enhance performance on the harder task (ARC), which had high zero-gradient rates (indicating poor learning).

- **Improvement-Aware Reweighting (Subroutine 1)**:
  - **Pros**: Dynamically adjusts task priorities based on how much each task benefits from policy updates. Better at allocating resources to underperforming tasks.
  - **Cons**: Requires tracking task-specific improvement signals, which adds complexity.
  - **Example**: In Experiment 1, improvement-aware updates maintained stronger performance on ARC, even with high zero-gradient rates. In Experiment 2 (9 tasks), they outperformed regularization on both worst-task accuracy and average relative change, especially for hard tasks.

---

### **Key Results**
- **Regularization Limitations**:
  - Prevents weight collapse but fails to prioritize under-optimized tasks.
  - In Experiment 2, stronger regularization (η=1e-2) degraded worst-task accuracy below DAPO (a baseline method), though average accuracy improved slightly due to easier tasks.

- **Improvement-Aware Success**:
  - **Trade-off Control**: Smaller trade-off parameters (e.g., λ=0.1–0.3) prioritize average accuracy and progress on hard tasks, while larger λ values focus on worst-task performance.
  - **Task-Specific Benefits**: Handles tasks with high zero-gradient rates (like ARC) better by focusing on their improvement signals.

---

### **Visual Insights**
- **Figure 10**: Compares Subroutine 1 (improvement-aware) and Subroutine 2 (regularized). Regularized updates achieve similar worst-task accuracy but lower average accuracy, while improvement-aware updates excel on harder tasks.
- **Tables**: Show metrics like **average relative change** and **worst-task accuracy**, highlighting that improvement-aware methods outperform regularization on challenging tasks.

---

### **Conclusion**
- **Regularization** is useful for preventing weight collapse but insufficient for prioritizing underperforming tasks.
- **Improvement-Aware Reweighting** is more effective for multi-task learning, as it dynamically allocates resources to tasks needing attention, balancing average and worst-task performance.
- **Implications**: This approach is critical for scenarios where tasks vary in difficulty, ensuring that all tasks receive adequate training focus.

This method underscores the importance of **task-specific signals** in multi-task reinforcement learning, moving beyond simple regularization to address complex, real-world scenarios.

===============

## 中文翻译

### **核心算法（伪代码）**
- **目标**：通过平衡所有任务的梯度来优化多个任务的策略。
- **关键组件**：
  - **采样**：从所有任务中收集数据。
  - **梯度计算**：为每个任务计算梯度，可能使用共享策略或任务特定参数。
  - **参数更新**：调整策略参数以最小化综合梯度，重点在于平衡任务表现。

---

### **实验与发现**
#### **1. 正则化 vs 改进感知重加权**
- **正则化奖励仅重加权（子程序2）**：
  - **优点**：通过驱动权重趋于均匀（平均奖励最大化）防止权重崩溃。
  - **缺点**：低估表现较差或改进缓慢的任务，导致在较难任务上表现不佳。
  - **示例**：在实验1（3个任务）中，正则化（η=1e-2）提高了平均准确率，但未能提升较难任务（ARC）的表现，该任务具有较高的零梯度率（表明学习效果差）。

- **改进感知重加权（子程序1）**：
  - **优点**：根据每个任务从策略更新中获益程度动态调整任务优先级，更擅长将资源分配给表现较差的任务。
  - **缺点**：需要跟踪任务特定的改进信号，增加了复杂性。
  - **示例**：在实验1中，改进感知更新在ARC任务上保持了更强的表现，即使存在高零梯度率。在实验2（9个任务）中，其在最差任务准确率和平均相对变化方面均优于正则化，尤其在困难任务中表现更优。

---

### **关键结果**
- **正则化的局限性**：
  - 防止权重崩溃，但无法优先处理未优化的任务。
  - 在实验2中，更强的正则化（η=1e-2）使最差任务准确率低于DAPO（基线方法），尽管由于较易任务的平均准确率略有提升。

- **改进感知重加权的成功**：
  - **权衡控制**：较小的权衡参数（如λ=0.1–0.3）优先考虑平均准确率和困难任务的进展，而较大的λ值则聚焦于最差任务表现。
  - **任务特定优势**：通过关注任务的改进信号，更好地处理零梯度率高的任务（如ARC）。

---

### **视觉洞察**
- **图10**：比较子程序1（改进感知）和子程序2（正则化）。正则化更新在最差任务准确率上与改进感知更新相似，但平均准确率较低，而改进感知更新在较难任务上表现更优。
- **表格**：展示如**平均相对变化**和**最差任务准确率**等指标，突出改进感知方法在挑战性任务上优于正则化。

---

### **结论**
- **正则化**有助于防止权重崩溃，但不足以优先处理表现较差的任务。
- **改进感知重加权**在多任务学习中更有效，因为它动态分配资源至需要关注的任务，平衡平均与最差任务表现。
- **启示**：该方法对于任务难度差异较大的场景至关重要，确保所有任务获得充分的训练关注。

该方法强调了**任务特定信号**在多任务强化学习中的重要性，超越了简单的正则化，以应对复杂的现实场景。

#### Reference: 

Source file: 2602.05547v1.pdf

---

**Title**: Xiaotong Ji** **Matthieu Zimmer
