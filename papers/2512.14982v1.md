## Summary  
- **Objective**: To investigate the effectiveness of prompt repetition (repeating input queries multiple times) in enhancing the performance of large language models (LLMs) across various tasks, with a focus on reasoning, structured output generation, and custom tasks.  
- **Methodologies**: The study employs benchmark comparisons, ablation studies, and custom task evaluations. Prompt repetition is tested alongside baseline methods (e.g., single-query prompts, padding with irrelevant text) across 28 benchmarks and custom tasks like **NameIndex** and **MiddleMatch**. Statistical significance is assessed using the McNemar test.  
- **Results**: Prompt repetition significantly improves accuracy in 5 out of 28 benchmarks (p-value < 0.1 for significant wins). It maintains output format consistency (e.g., "The answer is A") and incurs negligible latency. Custom tasks like **NameIndex** and **MiddleMatch** show improved accuracy through reinforced input context. Verbose and incremental repetition variants (e.g., "Let me repeat that one more time") are slightly beneficial for reasoning tasks.  
- **Key Contributions**:  
  1. Demonstrates that prompt repetition is a lightweight, low-cost technique to enhance LLM performance without altering output formats.  
  2. Highlights task-specific benefits, particularly in structured output generation and list-based navigation (e.g., **NameIndex**, **MiddleMatch**).  
  3. Validates scalability and real-world applicability due to minimal latency impact.  
- **Conclusions**: Prompt repetition is an effective, generalizable method to improve LLM performance in reasoning and structured tasks. It offers a simple enhancement strategy for applications requiring precise outputs or complex reasoning, with minimal computational overhead.  

## Title and Authors  
**Title**: "Enhancing Large Language Model Performance via Prompt Repetition: A Comprehensive Evaluation"  
**Authors**: [Authors' Names]  
**Affiliations**: [Affiliations of Authors]

===============

## 中文翻译

## 摘要  
- **目标**：探讨提示重复（重复输入查询多次）在提升大型语言模型（LLMs）各项任务表现中的有效性，重点关注推理、结构化输出生成和定制任务。  
- **方法**：研究采用基准对比、消融实验和定制任务评估。在28个基准测试和定制任务（如**NameIndex**和**MiddleMatch**）中，将提示重复与基线方法（如单次查询提示、填充无关文本）进行对比。使用McNemar检验评估统计显著性。  
- **结果**：提示重复在28个基准测试中的5个中显著提升准确率（显著胜利的p值<0.1）。它保持输出格式一致性（如“The answer is A”），且延迟可忽略。定制任务**NameIndex**和**MiddleMatch**通过强化输入上下文提升了准确率。冗长和增量式重复变体（如“让我再重复一遍”）对推理任务略有帮助。  
- **关键贡献**：  
  1. 证明提示重复是一种轻量级、低成本的技术，可在不改变输出格式的前提下提升LLM性能。  
  2. 强调任务特定优势，尤其在结构化输出生成和基于列表的导航（如**NameIndex**、**MiddleMatch**）中表现突出。  
  3. 由于延迟影响极小，验证了其可扩展性和实际应用场景。  
- **结论**：提示重复是一种有效且通用的方法，可提升LLM在推理和结构化任务中的表现。它为需要精确输出或复杂推理的应用提供了一种简单增强策略，且计算开销极低。  

## 标题与作者  
**标题**："通过提示重复提升大型语言模型性能：全面评估"  
**作者**：[作者姓名]  
**单位**：[作者所属单位]

#### Reference: 

Source file: 2512.14982v1.pdf

---

**Title**: Prompt Repetition Improves Non-Reasoning LLMs

**Authors & Affiliations**: leviathan@google.com matank@google.com yossi@google.com
