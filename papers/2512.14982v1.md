The document presents a study on the impact of **prompt repetition** on the performance of **large language models (LLMs)**, particularly in tasks requiring **reasoning**. Here's a structured summary of the key points:

---

### **1. Core Focus**
- **Prompt Repetition**: Repeating the same question or instruction multiple times to improve model accuracy, especially in reasoning tasks.
- **Key Question**: Does repeating prompts enhance LLMs' ability to reason, and if so, under what conditions?

---

### **2. Methodology**
- **Experiments**: Conducted on **standard benchmarks** (e.g., multiple-choice questions) and **custom tasks** (e.g., "NameIndex," "MiddleMatch").
- **Variations**:
  - **Baseline**: Single prompt.
  - **Prompt Repetition**: Repeating the prompt 2x, 3x, or with phrasing like "Let me repeat that."
  - **Padding**: Using irrelevant text to simulate repetition (e.g., periods) to compare effects.
- **Statistical Tests**: McNemar test to determine significant improvements (p-value < 0.1).

---

### **3. Key Findings**
- **Accuracy Improvements**: Prompt repetition showed **neutral to slightly positive** effects in reasoning tasks (5 wins, 1 loss, 22 ties in 28 tests).
- **Statistical Significance**: 5 out of 28 tests showed significant gains, suggesting repetition helps in specific cases.
- **Custom Tasks**:
  - **NameIndex**: Repeating prompts improved accuracy in retrieving the 25th name from a list.
  - **MiddleMatch**: Repeating helped identify the name between two given entries in a list with repetitions.
- **Trade-offs**:
  - **Latency**: Increased response time due to longer prompts.
  - **Redundancy**: Potential for unnecessary repetition in simple tasks.

---

### **4. Technical Insights**
- **Reasoning Process**: Models may use repeated prompts to reinforce understanding or handle ambiguous questions.
- **Prompt Structure**:
  - **Verbose Repetition**: Phrases like "Let me repeat that" improved clarity.
  - **Padding**: Irrelevant text (e.g., periods) had mixed results, suggesting structural repetition is more effective.

---

### **5. Implications and Future Work**
- **Practical Applications**: Useful for tasks requiring precision (e.g., data retrieval, structured problem-solving).
- **Limitations**: Not universally beneficial; depends on task complexity and model design.
- **Future Directions**:
  - Explore **dynamic repetition strategies** (e.g., adaptive repetition based on task difficulty).
  - Integrate with **retrieval-augmented generation** for complex reasoning.
  - Apply to **multi-step tasks** and **real-world scenarios** (e.g., customer support, diagnostics).

---

### **6. Conclusion**
Prompt repetition can **enhance accuracy** in LLMs for reasoning tasks, particularly in structured or ambiguous scenarios. However, it requires balancing **efficiency** (latency) and **effectiveness**. The study highlights the potential of repetition as a technique to improve model performance, though further research is needed to optimize its use.

---

**Key Takeaway**: Repeating prompts can be a **valuable heuristic** for improving LLM reasoning, but its success depends on task design, model architecture, and the trade-off between accuracy and computational cost.

===============

## 中文翻译

### **1. 核心关注点**
- **提示重复**：重复相同的问题或指令多次以提高模型准确性，尤其是在推理任务中。
- **关键问题**：重复提示是否能增强大语言模型（LLMs）的推理能力，如果能，是在什么条件下？

---

### **2. 方法论**
- **实验**：在**标准基准测试**（如多项选择题）和**定制任务**（如“NameIndex”、“MiddleMatch”）上进行。
- **变体**：
  - **基线**：单次提示。
  - **提示重复**：重复提示2次、3次，或使用“让我再重复一遍”等措辞。
  - **填充**：使用无关文本（如句号）模拟重复以比较效果。
- **统计检验**：采用McNemar检验确定显著性提升（p值 < 0.1）。

---

### **3. 关键发现**
- **准确性提升**：提示重复在推理任务中表现出**中性到轻微正面**效果（28次测试中5次提升，1次下降，22次平局）。
- **统计显著性**：28次测试中有5次显示显著提升，表明重复在特定情况下有助于提升表现。
- **定制任务**：
  - **NameIndex**：重复提示提高了从列表中检索第25个名称的准确性。
  - **MiddleMatch**：重复提示帮助在列表中识别两个给定条目之间的名称。
- **权衡**：
  - **延迟**：由于提示变长导致响应时间增加。
  - **冗余**：简单任务中可能存在不必要的重复。

---

### **4. 技术洞察**
- **推理过程**：模型可能通过重复提示强化理解或处理歧义问题。
- **提示结构**：
  - **详细重复**：如“让我再重复一遍”等措辞提升了清晰度。
  - **填充**：无关文本（如句号）效果参差不齐，表明结构性重复更有效。

---

### **5. 启示与未来工作**
- **实际应用**：适用于需要精确性的任务（如数据检索、结构化问题解决）。
- **局限性**：并非普遍有益，取决于任务复杂性和模型设计。
- **未来方向**：
  - 探索**动态重复策略**（如基于任务难度的自适应重复）。
  - 与**检索增强生成**结合以提升复杂推理能力。
  - 应用于**多步骤任务**和**现实场景**（如客户支持、诊断）。

---

### **6. 结论**
提示重复可**提升大语言模型在推理任务中的准确性**，特别是在结构化或歧义场景中。然而，需在**效率**（延迟）与**有效性**之间取得平衡。研究强调了重复作为提升模型性能的技术潜力，但仍需进一步研究以优化其应用。

---

**核心要点**：重复提示可作为**提升大语言模型推理能力的有价值启发式方法**，但其成功取决于任务设计、模型架构以及准确性与计算成本之间的权衡。

#### Reference: 

Source file: 2512.14982v1.pdf

---

**Title**: Prompt Repetition Improves Non-Reasoning LLMs

**Authors & Affiliations**: leviathan@google.com matank@google.com yossi@google.com
