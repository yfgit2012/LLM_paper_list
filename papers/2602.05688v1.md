The provided document outlines a research effort focused on **designing activation functions** for machine learning models to improve **out-of-distribution (OOD) generalization**. Here's a structured breakdown of the key components and findings:

---

### **A. Core Contributions: Activation Function Design**
1. **Symmetric Phase-Flipped State-Switching**:
   - **Mechanism**: Combines two states (positive and negative) using a non-monotonic switching phase (via sinusoidal disruption) to create a **fractal-like boundary**. This makes the activation function harder to exploit with gradient-based methods.
   - **Intrinsic Collapse**: For far-OOD inputs, an exponential gate (`g_x`) attenuates the base signal and decays chaotic amplitude, causing both states to smoothly converge to zero. This avoids external multiplicative gates and ensures gradient-friendly collapse.

2. **GELUSine Activation**:
   - **Form**: `GELU(x) + α * sin(x)`, where `α` is a tunable hyperparameter.
   - **Key Finding**: Empirical analysis shows that `α = 0.1` (as discovered by AlphaEvolve) balances the GELU and sine components, minimizing test loss on synthetic datasets. This aligns with empirical evaluations of random `α` choices.

3. **Other Discovered Functions**:
   - **GMTU (Gaussian-Modulated Tangent Unit)**: Used for sin product datasets.
   - **FISG (Fourier-Informed Spectral Gating)**: Applied to Feynman equations with tensor outputs.
   - **Quaternion-Inspired and Turbulent Activation Functions**: Tailored for Feynman equations and polynomials, respectively.

---

### **B. AlphaEvolve Meta-Prompt**
- **Role**: A reinforcement learning framework to iteratively improve OOD evaluation metrics by modifying activation functions.
- **Modification**: The prompt is adjusted to prioritize **generalizable functions** by explicitly requiring theoretical justifications for each proposal (e.g., mathematical support for OOD robustness).

---

### **C. Synthetic Datasets and Evaluation**
- **Datasets Used**:
  - **Feynman Equations**: 100 symbolic equations from physics.
  - **Random Polynomials**: 1D and 20D polynomials with random coefficients.
  - **Spherical Harmonics**: Generalized Fourier basis on spheres.
  - **Random Sin Products**: Functions of the form `sin(θ·x)·sin(ϕ·x)·sin(ψ·x)`.
- **Empirical Results**:
  - **Figure 8**: Compares test loss across different `α` values for GELUSine on synthetic datasets. Lower `α` (e.g., `α=0.1`) shows better performance.
  - **Figure 9**: Visualizes GELUSine activation for varying `α`, highlighting how sine perturbations enhance robustness.

---

### **D. Real-World Datasets**
- **CIFAR-10**: 60k 32x32 color images in 10 classes.
- **CLRS-30**: Algorithmic reasoning benchmark (e.g., quicksort, BFS).
- **ImageNet-1K**: 1.2M images for image classification.
- **ogbg-molhiv**: Graph-based molecular property prediction (HIV inhibition).

---

### **E. Architectures and Hyperparameters**
- **MLP for Synthetic Datasets**:
  - 3 layers, 64 features, MSE loss, learning rate `1e-3`, batch size `128`.
- **CIFAR-10**: VGG-16 (without maxpool).
- **ImageNet-1K**: ResNet-50.
- **ogbg-molhiv**: 5-layer GCN with 256 hidden units.

---

### **F. Key Takeaways**
1. **OOD Robustness**: The symmetric phase-flipped mechanism and GELUSine design reduce overfitting to in-distribution data by introducing non-monotonicity and chaotic decay.
2. **Hyperparameter Tuning**: The optimal `α=0.1` for GELUSine balances smoothness and perturbation, avoiding overfitting.
3. **Domain-Specific Adaptation**: Different activation functions (e.g., FISG for Feynman equations, GMTU for sin products) are tailored to dataset characteristics.
4. **AlphaEvolve's Role**: The meta-prompt framework enables systematic exploration of activation functions to improve OOD generalization.

---

### **Potential Questions & Answers**
- **Q**: Why use a sinusoidal disruption in the switching phase?  
  **A**: It introduces non-monotonicity, fractalizing the switching boundary to resist gradient-based exploitation, enhancing OOD robustness.

- **Q**: How does the exponential gate (`g_x`) contribute to intrinsic collapse?  
  **A**: It attenuates the base signal for far-OOD inputs, ensuring both states decay smoothly to zero, avoiding abrupt saturation.

- **Q**: Why is `α=0.1` optimal for GELUSine?  
  **A**: Empirical analysis shows it balances the smoothness of GELU with the perturbation of sine, minimizing test loss on synthetic datasets.

---

This work highlights the importance of **activation function design** in achieving robustness to distribution shifts, with implications for both synthetic and real-world machine learning tasks.

===============

## 中文翻译

该文档概述了一项研究工作，旨在**设计激活函数**以改进机器学习模型的**分布外（OOD）泛化能力**。以下是关键内容和发现的结构化总结：

---

### **A. 核心贡献：激活函数设计**
1. **对称相翻转状态切换**：
   - **机制**：通过非单调切换相（利用正弦扰动）结合正负两种状态，形成**分形边界**，使激活函数更难以被基于梯度的方法利用。
   - **内在坍缩**：对于分布外输入，指数门控（`g_x`）会衰减基础信号并降低混沌振幅，使两种状态平滑收敛至零。这避免了外部乘法门控，确保梯度友好的坍缩。

2. **GELUSine 激活函数**：
   - **形式**：`GELU(x) + α * sin(x)`，其中 `α` 是可调超参数。
   - **关键发现**：实证分析表明，`α = 0.1`（由 AlphaEvolve 发现）可平衡 GELU 与正弦分量，使合成数据集的测试损失最小化。这与随机 `α` 选择的实证评估结果一致。

3. **其他发现的函数**：
   - **GMTU（高斯调制切线单元）**：用于正弦乘积数据集。
   - **FISG（傅里叶信息谱门控）**：应用于具有张量输出的费曼方程。
   - **四元数启发式与湍流激活函数**：分别针对费曼方程和多项式。

---

### **B. AlphaEvolve 元提示**
- **作用**：一种强化学习框架，通过修改激活函数来迭代改进分布外评估指标。
- **修改方式**：通过明确要求每个提案的理论依据（如分布外鲁棒性的数学支持），调整提示以优先选择**可泛化函数**。

---

### **C. 合成数据集与评估**
- **使用数据集**：
  - **费曼方程**：100 个物理符号方程。
  - **随机多项式**：1D 和 20D 多项式，随机系数。
  - **球面调和函数**：球面上的广义傅里叶基。
  - **随机正弦乘积**：形式为 `sin(θ·x)·sin(ϕ·x)·sin(ψ·x)` 的函数。
- **实证结果**：
  - **图 8**：比较 GELUSine 在合成数据集上不同 `α` 值的测试损失。较小的 `α`（如 `α=0.1`）表现更优。
  - **图 9**：可视化不同 `α` 值下的 GELUSine 激活函数，突出正弦扰动如何增强鲁棒性。

---

### **D. 实际数据集**
- **CIFAR-10**：10 类中 60,000 张 32x32 色图像。
- **CLRS-30**：算法推理基准（如快速排序、广度优先搜索）。
- **ImageNet-1K**：120 万张图像用于图像分类。
- **ogbg-molhiv**：基于图的分子属性预测（HIV 抑制）。

---

### **E. 架构与超参数**
- **合成数据集的 MLP**：
  - 3 层，64 个特征，均方误差损失，学习率 `1e-3`，批量大小 `128`。
- **CIFAR-10**：VGG-16（无最大池化）。
- **ImageNet-1K**：ResNet-50。
- **ogbg-molhiv**：5 层 GCN，256 个隐藏单元。

---

### **F. 关键结论**
1. **分布外鲁棒性**：对称相翻转机制与 GELUSine 设计通过引入非单调性和混沌衰减，减少对分布内数据的过拟合。
2. **超参数调优**：GELUSine 的最优 `α=0.1` 在平滑性与扰动之间取得平衡，避免过拟合。
3. **领域特定适配**：不同激活函数（如 FISG 用于费曼方程，GMTU 用于正弦乘积）针对数据集特性进行定制。
4. **AlphaEvolve 的作用**：元提示框架可系统探索激活函数，提升分布外泛化能力。

---

### **潜在问题与解答**
- **问**：为何在切换相中使用正弦扰动？  
  **答**：它引入非单调性，使切换边界分形化，抵抗基于梯度的利用，增强分布外鲁棒性。

- **问**：指数门控（`g_x`）如何促进内在坍缩？  
  **答**：它对远分布外输入衰减基础信号，确保两种状态平滑衰减至零，避免突变饱和。

- **问**：为何 `α=0.1` 是 GELUSine 的最优值？  
  **答**：实证分析表明，它在 GELU 的平滑性与正弦扰动之间取得平衡，使合成数据集的测试损失最小化。

---

本工作突显了**激活函数设计**在实现分布偏移鲁棒性中的重要性，对合成与实际机器学习任务均具有重要意义。

#### Reference: 

Source file: 2602.05688v1.pdf

---

**Title**: Mining Generalizable Activation Functions
