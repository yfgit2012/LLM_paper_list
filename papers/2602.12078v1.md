## Summary  
- **Objective**: To advance recursive reasoning in machine learning by integrating Mamba (a linear-time sequence model) with Transformers, enabling efficient and scalable solutions for complex tasks like multi-step reasoning and puzzle-solving.  
- **Methodologies**: The paper combines Mamba's selective state spaces (for linear-time sequence modeling) with Transformers' parallelization capabilities. It introduces **post-norm** (layer normalization after residual connections) to stabilize recursive computation and evaluates performance on tasks like the **ARC Prize** challenges. Difficulty stratification is used to analyze how different operators (Mamba vs. Transformers) solve disjoint subsets of problems.  
- **Results**: Hybrid models (e.g., Jamba) demonstrate complementary strengths: Mamba excels in sequential, memory-efficient tasks, while Transformers handle parallel, context-rich reasoning. Post-norm ensures numerical stability in recursive loops, and difficulty stratification highlights the effectiveness of mixing strategies for complex puzzles.  
- **Key Contributions**:  
  1. **Hybrid Mamba-Transformer architecture** for recursive reasoning, balancing efficiency and scalability.  
  2. **Post-norm technique** to stabilize deep recursive computation.  
  3. **Difficulty stratification** analysis showing how Mamba and Transformers solve partially disjoint subsets of tasks.  
  4. **Compute-normalized evaluation** framework to fairly compare model efficiency and accuracy.  
- **Conclusions**: The integration of Mamba and Transformers enables robust, scalable recursive reasoning for tasks like scientific reasoning and program synthesis. Future work includes embedding recursion into SSM state updates and exploring hybrid architectures further.  

## Title and Authors  
**Title**: "Recursive Reasoning with Mamba and Transformers: Bridging Linear-Time Models and Parallel Reasoning"  
**Authors**: [Author Names] (Affiliations: e.g., Google Research, MIT, etc.)

===============

## 中文翻译

## 摘要  
- **目标**：通过将Mamba（线性时间序列模型）与Transformer结合，推动机器学习中的递归推理，实现复杂任务（如多步推理和解谜）的高效可扩展解决方案。  
- **方法**：论文将Mamba的**选择性状态空间**（用于线性时间序列建模）与Transformer的并行化能力结合，并引入**后归一化**（残差连接后的层归一化）以稳定递归计算，同时在**ARC Prize**等任务中评估性能。通过难度分层分析不同操作符（Mamba vs. Transformer）如何解决问题的不相交子集。  
- **结果**：混合模型（如Jamba）展现出互补优势：Mamba在顺序、内存高效的任务中表现优异，而Transformer擅长并行、上下文丰富的推理。后归一化确保递归循环的数值稳定性，难度分层分析凸显混合策略对复杂谜题的有效性。  
- **关键贡献**：  
  1. **混合Mamba-Transformer架构**，实现递归推理的效率与可扩展性平衡。  
  2. **后归一化技术**，稳定深层递归计算。  
  3. **难度分层分析**，展示Mamba与Transformer如何解决任务的部分不相交子集。  
  4. **计算归一化评估框架**，公平比较模型的效率与准确性。  
- **结论**：Mamba与Transformer的结合使科学推理和程序综合等任务的递归推理具备鲁棒性和可扩展性。未来工作包括将递归嵌入SSM状态更新中，并进一步探索混合架构。  

## 标题与作者  
**标题**："Mamba与Transformer的递归推理：连接线性时间模型与并行推理"  
**作者**：[作者姓名]（单位：如Google研究、MIT等）

#### Reference: 

Source file: 2602.12078v1.pdf

---

**Title**: Wenlong Wang & Fergal Reid

**Authors & Affiliations**: _{_ wenlong.wang,fergal.reid _}_ @intercom.io
