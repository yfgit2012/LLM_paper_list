## Summary
- **Objective**: To introduce **ScImage**, a benchmark for evaluating multimodal large language models (LLMs) in generating scientific images from textual descriptions, addressing the gap in scientific visualization capabilities and improving accuracy in spatial, numeric, and attribute understanding.  
- **Methodologies**: The study evaluates five models (GPT-4o, Llama, AutomaTikZ, Dall-E, StableDiffusion) across two output modes (direct image generation and text-to-code-to-image generation using Python/TikZ) and four languages. A dataset with 101 query templates and 404 generation queries is constructed, covering diverse scientific objects. Evaluation criteria include correctness, relevance, and scientific style, with manual and automated assessments.  
- **Results**: GPT-4o (text-code-image) outperforms others in correctness (3.50–3.93), relevance (3.40–3.67), and scientific style (3.59–3.93). Code-based models (e.g., Python) show higher scientific accuracy than direct image generation (e.g., Python: 3.51 vs. Dall-E: 1.55). Multilingual performance varies, with English yielding the highest scores and Farsi the lowest. Spatial understanding remains the most challenging task, and code generation models face compilation errors (e.g., AutomaTikZ: 113–116 errors).  
- **Key Contributions**: Introduction of **ScImage** as a novel benchmark for scientific image generation, evaluation of multimodal models across languages and modes, and identification of critical challenges (e.g., spatial understanding, complex object interactions). The dataset and evaluation criteria provide a foundation for improving scientific visualization tools.  
- **Conclusions**: While GPT-4o performs best, complex scientific tasks remain challenging. Code-based models excel in scientific accuracy, underscoring the need for better training data and metrics. Future work includes expanding **ScImage**, developing automated quality metrics, and enhancing LLMs for scientific tasks.  

## Title and Authors (Required)  
**Title**: ScImage: A Benchmark for Evaluating Multimodal Large Language Models in Scientific Image Generation  
**Authors**: Not provided in the extracted content.

===============

## 中文翻译

## 摘要
- **目标**：介绍**ScImage**，一个用于评估多模态大语言模型（LLMs）在从文本描述生成科学图像方面的基准，解决科学可视化能力的不足，并提高空间、数值和属性理解的准确性。  
- **方法**：研究在两种输出模式（直接图像生成和使用Python/TikZ的文本-代码-图像生成）和四种语言下评估了五个模型（GPT-4o、Llama、AutomaTikZ、Dall-E、StableDiffusion）。构建了一个包含101个查询模板和404个生成查询的数据集，涵盖多样化的科学对象。评估标准包括正确性、相关性和科学风格，采用人工和自动化评估。  
- **结果**：GPT-4o（文本-代码-图像生成）在正确性（3.50–3.93）、相关性（3.40–3.67）和科学风格（3.59–3.93）方面优于其他模型。基于代码的模型（如Python）在科学准确性上高于直接图像生成（如Python：3.51 vs. Dall-E：1.55）。多语言性能存在差异，英语得分最高，法语最低。空间理解仍是最大挑战，代码生成模型面临编译错误（如AutomaTikZ：113–116个错误）。  
- **关键贡献**：引入**ScImage**作为科学图像生成的新基准，评估多模态模型在多语言和模式下的表现，并识别关键挑战（如空间理解、复杂对象交互）。数据集和评估标准为改进科学可视化工具提供了基础。  
- **结论**：尽管GPT-4o表现最佳，复杂科学任务仍具挑战性。基于代码的模型在科学准确性上表现突出，凸显了对更优训练数据和评估指标的需求。未来工作包括扩展**ScImage**、开发自动化质量指标以及增强LLMs在科学任务中的能力。  

## 标题和作者（必填）  
**标题**：ScImage：用于评估多模态大语言模型在科学图像生成中的基准  
**作者**：提取内容中未提供。

#### Reference: 

Source file: 2412.02368v1.pdf

---

**Title**: ScImage : HOW GOOD ARE MULTIMODAL LARGE LANGUAGE MODELS AT SCIENTIFIC TEXT-TO-IMAGE GENERATION?

**Authors & Affiliations**: 1University of Twente, `l.zhang-5@utwente.nl` 2University of Technology Nuremberg (UTN), `steffen.eger@utn.de` 4University of Sheffield, `{ycheng80,zhixue.zhao}@sheffield.ac.uk`
