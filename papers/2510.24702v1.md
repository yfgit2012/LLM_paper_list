## Summary
- **Objective**: To enhance writing quality using Large Language Models (LLMs) and develop balanced data sampling strategies for training AI agents across diverse domains, ensuring equitable domain representation and task-specific optimization.  
- **Methodologies**:  
  - **LLM Integration**: Leveraged LLMs for refining writing style, clarity, and formatting (e.g., LaTeX tables/figures).  
  - **Resampling Techniques**: Adjusted dataset sizes using a multiplier `wd` to balance domain representation (upsampling for underrepresented domains, downsampling for overrepresented ones).  
  - **Domain Filtering**: Excluded irrelevant data based on agent-specific evaluation tasks (e.g., non-web data for coding agents, web-focused data for navigation agents).  
- **Results**:  
  - Streamlined academic/technical writing with improved clarity and formatting.  
  - Resampling maintained consistent epoch sizes while balancing domain proportions.  
  - Domain-specific filtering improved model performance by aligning training data with evaluation tasks (e.g., coding vs. web browsing).  
- **Key Contributions**:  
  - Novel integration of LLMs for writing assistance and formatting efficiency.  
  - Resampling strategies for equitable domain representation in training datasets.  
  - Domain-adaptive filtering to prioritize task-relevant data, enhancing model focus and performance.  
- **Conclusions**: Combining LLM-driven writing refinement, balanced data sampling, and domain-specific filtering is critical for training robust AI agents capable of handling diverse tasks while maintaining focus on relevant data.  

## Title and Authors (Required)  
**Title**: *Enhancing AI Agent Training via LLM-Driven Writing Assistance and Domain-Specific Data Sampling*  
**Authors**: [Authors not provided in the extracted content]  
**Affiliations**: [Affiliations not provided in the extracted content]

===============

## 中文翻译

## 摘要
- **目标**：利用大语言模型（LLMs）提升写作质量，开发跨多领域训练AI代理的平衡数据采样策略，确保各领域表示的均衡性及任务特定优化。
- **方法**：
  - **LLM整合**：利用LLMs优化写作风格、清晰度和格式（如LaTeX表格/图表）。
  - **重采样技术**：通过乘数`wd`调整数据集大小，平衡领域表示（对欠代表领域进行过采样，对过代表领域进行欠采样）。
  - **领域过滤**：基于代理特定的评估任务排除无关数据（如代码代理排除非网页数据，导航代理聚焦网页数据）。
- **结果**：
  - 优化了学术/技术写作的清晰度和格式。
  - 重采样保持了每个训练周期的数据量一致性，同时平衡了领域比例。
  - 领域特定过滤通过使训练数据与评估任务对齐，提升了模型性能（如代码与网页浏览任务）。
- **关键贡献**：
  - 提出LLM在写作辅助和格式优化中的创新整合。
  - 开发了训练数据集中领域均衡表示的重采样策略。
  - 领域自适应过滤优先选择任务相关数据，增强模型专注度与性能。
- **结论**：结合LLM驱动的写作优化、平衡数据采样和领域特定过滤，是训练能够处理多样化任务并保持相关数据聚焦的稳健AI代理的关键。

## 标题与作者（必填）
**标题**：*通过LLM驱动的写作辅助与领域特定数据采样增强AI代理训练*  
**作者**：[提取内容中未提供作者信息]  
**所属机构**：[提取内容中未提供所属机构信息]

#### Reference: 

Source file: 2510.24702v1.pdf

---

**Title**: Yueqi Song** [1] **, Ketan Ramaneti** [1] **, Zaid Sheikh** [1] **, Ziru Chen** [2] **, Boyu Gou** [2] **, Tianbao Xie** [3] **,

**Authors & Affiliations**: _{_ yueqis,gneubig _}_ @cs.cmu.edu
